<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GenAI OWASP Top 10 Quiz | AWS Security Specialty (SCS-C03)</title>
    <style>
        :root { --bg-primary: #0a0e17; --bg-secondary: #141b2d; --bg-tertiary: #1a2236; --text-primary: #e2e8f0; --text-secondary: #94a3b8; --accent-primary: #ec4899; --accent-secondary: #f472b6; --success: #22c55e; --error: #ef4444; --warning: #eab308; --info: #3b82f6; --border: #2d3a52; }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', system-ui, sans-serif; background: var(--bg-primary); color: var(--text-primary); line-height: 1.6; min-height: 100vh; }
        .container { max-width: 900px; margin: 0 auto; padding: 2rem; }
        header { text-align: center; margin-bottom: 2rem; padding: 2rem; background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-tertiary) 100%); border-radius: 16px; border: 1px solid var(--border); }
        .badge { display: inline-block; background: var(--accent-primary); color: white; padding: 0.25rem 0.75rem; border-radius: 20px; font-size: 0.75rem; font-weight: 600; margin-bottom: 1rem; text-transform: uppercase; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
        .subtitle { color: var(--text-secondary); font-size: 1rem; }
        .stats-bar { display: flex; justify-content: center; gap: 2rem; margin-top: 1.5rem; padding-top: 1.5rem; border-top: 1px solid var(--border); }
        .stat { text-align: center; } .stat-value { font-size: 1.5rem; font-weight: 700; color: var(--accent-primary); } .stat-label { font-size: 0.75rem; color: var(--text-secondary); text-transform: uppercase; }
        .mode-selector { display: flex; gap: 1rem; justify-content: center; margin-bottom: 2rem; }
        .mode-btn { padding: 0.75rem 2rem; border: 2px solid var(--border); background: var(--bg-secondary); color: var(--text-primary); border-radius: 8px; cursor: pointer; font-size: 1rem; transition: all 0.3s ease; }
        .mode-btn:hover { border-color: var(--accent-primary); background: var(--bg-tertiary); }
        .progress-container { background: var(--bg-secondary); border-radius: 12px; padding: 1rem 1.5rem; margin-bottom: 1.5rem; border: 1px solid var(--border); }
        .progress-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 0.75rem; }
        .progress-text { font-size: 0.875rem; color: var(--text-secondary); }
        .timer { font-size: 1.25rem; font-weight: 600; color: var(--accent-primary); font-family: 'Consolas', monospace; }
        .timer.warning { color: var(--warning); animation: pulse 1s infinite; } .timer.danger { color: var(--error); animation: pulse 0.5s infinite; }
        @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
        .progress-bar { height: 8px; background: var(--bg-tertiary); border-radius: 4px; overflow: hidden; }
        .progress-fill { height: 100%; background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary)); border-radius: 4px; transition: width 0.3s ease; }
        .question-card { background: var(--bg-secondary); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; border: 1px solid var(--border); }
        .question-header { display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 1rem; }
        .question-number { background: var(--bg-tertiary); color: var(--accent-primary); padding: 0.25rem 0.75rem; border-radius: 6px; font-size: 0.875rem; font-weight: 600; }
        .question-domain { font-size: 0.75rem; color: var(--text-secondary); background: var(--bg-tertiary); padding: 0.25rem 0.5rem; border-radius: 4px; }
        .question-text { font-size: 1.125rem; margin-bottom: 1.5rem; line-height: 1.7; }
        .options { display: flex; flex-direction: column; gap: 0.75rem; }
        .option { display: flex; align-items: flex-start; gap: 1rem; padding: 1rem; background: var(--bg-tertiary); border: 2px solid var(--border); border-radius: 10px; cursor: pointer; transition: all 0.2s ease; }
        .option:hover:not(.disabled) { border-color: var(--accent-primary); background: rgba(236, 72, 153, 0.1); }
        .option.selected { border-color: var(--accent-primary); border-width: 3px; background: rgba(236, 72, 153, 0.2); box-shadow: 0 0 0 3px rgba(236, 72, 153, 0.25), inset 0 0 12px rgba(236, 72, 153, 0.1); }
        .option.correct { border-color: var(--success); background: rgba(34, 197, 94, 0.15); }
        .option.incorrect { border-color: var(--error); background: rgba(239, 68, 68, 0.15); }
        .option.disabled { cursor: default; }
        .option-marker { width: 28px; height: 28px; border-radius: 50%; border: 2px solid var(--border); display: flex; align-items: center; justify-content: center; font-weight: 600; font-size: 0.875rem; flex-shrink: 0; }
        .option.selected .option-marker { border-color: var(--accent-primary); background: var(--accent-primary); color: white; }
        .option.correct .option-marker { border-color: var(--success); background: var(--success); color: white; }
        .option.incorrect .option-marker { border-color: var(--error); background: var(--error); color: white; }
        .option-text { flex: 1; padding-top: 2px; }
        .explanation { margin-top: 1.5rem; padding: 1.5rem; background: var(--bg-tertiary); border-radius: 10px; border-left: 4px solid var(--info); display: none; }
        .explanation.show { display: block; animation: fadeIn 0.3s ease; }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(-10px); } to { opacity: 1; transform: translateY(0); } }
        .explanation-title { display: flex; align-items: center; gap: 0.5rem; font-weight: 600; margin-bottom: 0.75rem; color: var(--info); }
        .answer-header { font-weight: 600; color: var(--success); margin-bottom: 0.5rem; }
        .explanation-text { color: var(--text-secondary); margin-bottom: 1rem; }
        .doc-link { display: inline-flex; align-items: center; gap: 0.5rem; color: var(--accent-primary); text-decoration: none; font-size: 0.875rem; padding: 0.5rem 1rem; background: rgba(236, 72, 153, 0.1); border-radius: 6px; }
        .doc-link:hover { background: rgba(236, 72, 153, 0.2); }
        .btn-container { display: flex; justify-content: space-between; gap: 1rem; margin-top: 1rem; }
        .btn { padding: 0.75rem 1.5rem; border: none; border-radius: 8px; font-size: 1rem; font-weight: 600; cursor: pointer; transition: all 0.2s ease; }
        .btn-primary { background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary)); color: white; }
        .btn-primary:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(236, 72, 153, 0.4); }
        .btn-secondary { background: var(--bg-tertiary); color: var(--text-primary); border: 1px solid var(--border); }
        .btn-secondary:hover { border-color: var(--accent-primary); }
        .btn:disabled { opacity: 0.5; cursor: not-allowed; }
        .results-card { background: var(--bg-secondary); border-radius: 16px; padding: 2rem; text-align: center; border: 1px solid var(--border); display: none; }
        .results-card.show { display: block; animation: fadeIn 0.5s ease; }
        .results-score { font-size: 4rem; font-weight: 700; margin: 1rem 0; }
        .results-score.pass { color: var(--success); } .results-score.fail { color: var(--error); }
        .results-status { font-size: 1.5rem; margin-bottom: 1rem; }
        .results-status.pass { color: var(--success); } .results-status.fail { color: var(--error); }
        .results-details { color: var(--text-secondary); margin-bottom: 2rem; }
        .results-breakdown { display: flex; justify-content: center; gap: 3rem; margin-bottom: 2rem; padding: 1.5rem; background: var(--bg-tertiary); border-radius: 10px; }
        .breakdown-item { text-align: center; } .breakdown-value { font-size: 2rem; font-weight: 700; } .breakdown-value.correct { color: var(--success); } .breakdown-value.incorrect { color: var(--error); } .breakdown-label { font-size: 0.875rem; color: var(--text-secondary); }
        .review-section { background: var(--bg-secondary); border-radius: 16px; padding: 2rem; border: 1px solid var(--border); display: none; }
        .review-section.show { display: block; animation: fadeIn 0.5s ease; }
        .review-header { text-align: center; margin-bottom: 2rem; }
        .review-table { width: 100%; border-collapse: collapse; margin-bottom: 2rem; }
        .review-table th { background: var(--bg-tertiary); color: var(--text-secondary); font-size: 0.875rem; font-weight: 600; text-transform: uppercase; padding: 1rem; text-align: left; border-bottom: 2px solid var(--border); }
        .review-table td { padding: 1rem; border-bottom: 1px solid var(--border); vertical-align: top; }
        .review-table tbody tr { cursor: pointer; transition: all 0.2s ease; }
        .review-table tbody tr:hover { background: rgba(236, 72, 153, 0.1); transform: translateX(4px); }
        .review-table tbody tr.reviewed { background: rgba(236, 72, 153, 0.25); border-left: 6px solid var(--accent-primary); box-shadow: inset 0 0 20px rgba(236, 72, 153, 0.15); }
        .review-table tbody tr.reviewed:hover { background: rgba(236, 72, 153, 0.3); }
        .review-table tbody tr.reviewed .review-q-num { color: var(--accent-secondary); font-weight: 800; }
        .review-table tbody tr.reviewed .review-q-num::before { content: "‚úì "; color: var(--accent-primary); font-size: 1rem; }
        .review-q-num { font-weight: 700; color: var(--accent-primary); white-space: nowrap; }
        .review-q-text { color: var(--text-primary); line-height: 1.6; }
        .review-answer { font-weight: 600; }
        .review-answer.correct { color: var(--success); }
        .review-answer.incorrect { color: var(--error); }
        .review-status { text-align: center; font-size: 1.25rem; }
        .review-hint { text-align: center; color: var(--text-secondary); font-size: 0.875rem; margin-bottom: 1rem; }
        .btn-group { display: flex; gap: 1rem; justify-content: center; }
        .hidden { display: none !important; }
        .shuffle-notice { text-align: center; padding: 0.75rem; background: rgba(236, 72, 153, 0.1); border-radius: 8px; margin-bottom: 1.5rem; color: var(--accent-secondary); font-size: 0.875rem; }
        @media (max-width: 640px) { .container { padding: 1rem; } h1 { font-size: 1.5rem; } .stats-bar { gap: 1rem; } .mode-selector, .btn-container, .results-breakdown { flex-direction: column; } }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <span class="badge">Domain 3: Infrastructure Security (26%)</span>
            <h1>ü§ñ GenAI OWASP Top 10 Security</h1>
            <p class="subtitle">Comprehensive Quiz for AWS Security Specialty (SCS-C03)</p>
            <div class="stats-bar">
                <div class="stat"><div class="stat-value">25</div><div class="stat-label">Questions</div></div>
                <div class="stat"><div class="stat-value">80%</div><div class="stat-label">Pass Score</div></div>
                <div class="stat"><div class="stat-value">62:30</div><div class="stat-label">Exam Time</div></div>
            </div>
        </header>
        <div class="mode-selector" id="modeSelector">
            <button class="mode-btn" onclick="startQuiz('learn')">üìö Learn Mode</button>
            <button class="mode-btn" onclick="startQuiz('exam')">‚è±Ô∏è Exam Mode</button>
        </div>
        <div id="quizContent" class="hidden">
            <div class="progress-container">
                <div class="progress-header">
                    <span class="progress-text">Question <span id="currentQ">1</span> of <span id="totalQ">25</span></span>
                    <span class="timer" id="timer">62:30</span>
                </div>
                <div class="progress-bar"><div class="progress-fill" id="progressFill" style="width: 0%"></div></div>
            </div>
            <div class="shuffle-notice" id="shuffleNotice">üîÄ Questions and options have been randomized</div>
            <div id="questionsContainer"></div>
            <div class="btn-container">
                <button class="btn btn-secondary" id="prevBtn" onclick="navigate(-1)" disabled>‚Üê Previous</button>
                <button class="btn btn-secondary" id="backToReviewBtn" onclick="backToReview()" style="display: none;">‚Üê Back to Review</button>
                <button class="btn btn-secondary" id="shuffleBtn" onclick="shuffleQuiz()">üîÄ Shuffle</button>
                <button class="btn btn-primary" id="nextBtn" onclick="navigate(1)">Next ‚Üí</button>
            </div>
        </div>
        <div class="results-card" id="resultsCard">
            <h2>Quiz Complete!</h2>
            <div class="results-score" id="scorePercent">0%</div>
            <div class="results-status" id="resultStatus">-</div>
            <p class="results-details" id="resultDetails"></p>
            <div class="results-breakdown">
                <div class="breakdown-item"><div class="breakdown-value correct" id="correctCount">0</div><div class="breakdown-label">Correct</div></div>
                <div class="breakdown-item"><div class="breakdown-value incorrect" id="incorrectCount">0</div><div class="breakdown-label">Incorrect</div></div>
            </div>
            <div class="btn-group">
                <button class="btn btn-primary" onclick="showReview()">üìä Review Answers</button>
                <button class="btn btn-secondary" onclick="restartQuiz()">üîÑ Restart Quiz</button>
            </div>
        </div>
        <div class="review-section" id="reviewSection">
            <div class="review-header">
                <h2>Answer Review</h2>
                <p class="results-details">Click any question to review the full answer and explanation</p>
            </div>
            <p class="review-hint">üí° Tip: Click on any row to see the full question and explanation in Learn mode</p>
            <table class="review-table">
                <thead>
                    <tr>
                        <th style="width: 5%">#</th>
                        <th style="width: 50%">Question</th>
                        <th style="width: 15%">Your Answer</th>
                        <th style="width: 15%">Correct Answer</th>
                        <th style="width: 10%">Status</th>
                    </tr>
                </thead>
                <tbody id="reviewTableBody"></tbody>
            </table>
            <div class="btn-group">
                <button class="btn btn-secondary" onclick="backToResults()">‚Üê Back to Results</button>
                <button class="btn btn-primary" onclick="restartQuiz()">üîÑ Restart Quiz</button>
            </div>
        </div>
    </div>
    <script>
        const questions = [
            { id: 1, question: "What is LLM01:2025 Prompt Injection and what makes it the top risk for LLM applications?", options: ["A vulnerability where SQL injection strings embedded in prompts corrupt the LLM's connected backend database", "A vulnerability where malicious actors manipulate LLM prompts to override instructions, bypass guardrails, or access restricted functions", "A performance bottleneck where concurrent prompt submission overwhelms LLM capacity and causes degraded response times", "An authentication flaw where forged tokens allow unauthorized callers to bypass API gateway controls and reach the LLM"], correct: 1, explanation: "Prompt Injection (LLM01:2025) occurs when user prompts alter the LLM's behavior in unintended ways, overriding system instructions or bypassing security guardrails. Attackers can craft prompts that manipulate the LLM to reveal sensitive information, execute unauthorized actions, or access restricted functionality. This is the top risk because it's fundamental to how LLMs process input and difficult to completely prevent.", docLink: "https://genai.owasp.org/llmrisk/llm01-prompt-injection/" },
            { id: 2, question: "Which AWS service provides mitigation for LLM01:2025 Prompt Injection by filtering harmful prompts before they reach the LLM?", options: ["AWS WAF with custom rule groups configured to inspect and block manipulative LLM traffic patterns at the API layer", "Amazon Bedrock Guardrails with content filters and prompt attack detection", "Amazon Inspector with continuous vulnerability scanning enabled across all LLM-serving compute infrastructure", "AWS Shield Advanced with behavioral detection to identify and absorb volumetric attacks targeting LLM API endpoints"], correct: 1, explanation: "Amazon Bedrock Guardrails provides protection against prompt injection attacks by detecting and filtering harmful or manipulative prompts before they reach the LLM. Guardrails can be configured with content filters, denied topics, word filters, and sensitive information filters. Additionally, implementing input validation in the application controller layer (Lambda function) with syntactic and semantic validation adds another layer of defense.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 3, question: "What is LLM02:2025 Sensitive Information Disclosure and how does it manifest in LLM applications?", options: ["When the LLM's training dataset files are stored in a misconfigured public S3 bucket accessible to external parties", "When LLMs unintentionally leak confidential data, PII, or proprietary information during interactions or in responses", "When hardcoded API keys or credentials in application source code grant attackers direct access to the LLM service", "When attackers reverse-engineer LLM model weights to reconstruct proprietary training data or recover internal logic"], correct: 1, explanation: "Sensitive Information Disclosure (LLM02:2025) occurs when LLMs inadvertently reveal confidential information in their responses. This can include PII, proprietary business data, credentials, or training data. The risk exists because LLMs may memorize training data, and malicious prompt engineering can cause them to reveal unintended information. This affects both the LLM itself and the application's data handling.", docLink: "https://genai.owasp.org/llmrisk/llm022025-sensitive-information-disclosure/" },
            { id: 4, question: "Which combination of AWS services best mitigates LLM02:2025 Sensitive Information Disclosure?", options: ["Amazon GuardDuty for threat detection combined with AWS Security Hub to aggregate and centralize all security findings", "Amazon Bedrock Guardrails for content filtering, user-level access control via user_id, and CloudWatch Log data protection for masking sensitive information in logs", "AWS KMS customer-managed keys encrypting all LLM response payloads with S3 versioning for immutable output retention", "Amazon Macie scanning all LLM-generated output for sensitive data patterns combined with ACM-enforced TLS on all endpoints"], correct: 1, explanation: "Mitigating sensitive information disclosure requires multiple layers: Amazon Bedrock Guardrails filters PII and sensitive content from both inputs and outputs using configurable thresholds and regex patterns. Implementing user-level access control by passing user_id through the application ensures data plane restrictions. CloudWatch Log data protection masks sensitive information in logs if model invocation logging is enabled. This defense-in-depth approach addresses multiple vectors of information leakage.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 5, question: "What characterizes LLM03:2025 Supply Chain vulnerabilities in LLM applications?", options: ["Network latency delays that slow LLM model artifact delivery across regions during CI/CD deployment pipelines", "Vulnerabilities in the development, deployment, and plugin ecosystems including compromised models, libraries, or data sources", "Weaknesses in physical data center hardware hosting LLM GPU compute leading to availability and reliability failures", "Unexpected cost spikes from LLM API usage that exceed budget allocations and disrupt financial operations planning"], correct: 1, explanation: "Supply Chain vulnerabilities (LLM03:2025) encompass risks throughout the LLM application lifecycle including compromised pre-trained models, poisoned training data, vulnerable third-party plugins, outdated dependencies, and insecure integrations. The complex ecosystem of models, libraries, plugins, and data sources creates multiple attack vectors. This includes risks from model repositories, plugin marketplaces, and external data sources used in RAG systems.", docLink: "https://genai.owasp.org/llmrisk/llm032025-supply-chain/" },
            { id: 6, question: "What is LLM04:2025 Data and Model Poisoning?", options: ["When the LLM's inference pipeline produces toxic, hateful, or offensive content despite content moderation controls", "When training data, fine-tuning data, or embeddings are manipulated to compromise model behavior, introduce backdoors, or bias outputs", "When excessive training data volume causes overfitting, reducing generalization and increasing the rate of hallucinations", "When model weights are silently corrupted during storage or network transfer, causing unpredictable inference failures"], correct: 1, explanation: "Data and Model Poisoning (LLM04:2025) occurs when attackers manipulate training data, fine-tuning data, or embedding data to compromise the LLM's behavior. This can introduce backdoors, biases, or vulnerabilities that persist in the model. Poisoning can happen during pre-training, fine-tuning, or through compromised RAG data stores. The impact can range from biased outputs to security vulnerabilities that activate under specific conditions.", docLink: "https://genai.owasp.org/llmrisk/llm042025-data-and-model-poisoning/" },
            { id: 7, question: "What is LLM05:2025 Improper Output Handling and what security risks does it pose?", options: ["When LLM responses use unexpected formatting structures causing rendering failures and broken display in the UI layer", "When insufficient validation, sanitization, or encoding of LLM outputs enables XSS, SSRF, remote code execution, or privilege escalation", "When LLM-generated responses exceed maximum token or character limits and are silently truncated before delivery", "When the LLM produces outputs in an unintended language, requiring costly translation before the content can be consumed"], correct: 1, explanation: "Improper Output Handling (LLM05:2025) occurs when LLM-generated content is used downstream without proper validation or sanitization. Since LLMs can generate executable code (JavaScript, Markdown, shell commands), improper handling can lead to XSS attacks, SSRF, remote code execution, or privilege escalation. The vulnerability exists because LLM outputs are often treated as trusted data when they should be validated like any user input.", docLink: "https://genai.owasp.org/llmrisk/llm052025-improper-output-handling/" },
            { id: 8, question: "How does the AWS blog recommend mitigating LLM05:2025 Improper Output Handling?", options: ["Select only LLMs with built-in output sanitization pipelines that automatically strip all executable content from responses", "Treat the LLM as an untrusted user, apply output encoding (HTML entity encoding, JavaScript escaping), and use Amazon Bedrock Agents to parse and control output processing at each step", "Disable all code and executable content generation in the LLM model configuration to prevent injection at the source", "Route all LLM-generated output through AWS WAF custom inspection rules before rendering it in any downstream system"], correct: 1, explanation: "The recommended mitigation is to treat the LLM as an untrusted user and apply rigorous output validation. This includes output encoding techniques (HTML entity encoding, JavaScript escaping) to neutralize potentially harmful content. Amazon Bedrock Agents can parse LLM outputs at each step and implement custom logic to control processing. The action group Lambda function encodes all output text making it non-executable, and preprocessing/postprocessing templates can strip unexpected information.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 9, question: "What is LLM06:2025 Excessive Agency?", options: ["When the LLM produces excessively long or verbose responses that consume disproportionate API quota and user bandwidth", "When LLM-based systems are granted excessive permissions or autonomy, enabling them to perform unauthorized actions beyond intended scope", "When multiple LLM agents simultaneously compete for shared API rate limits, causing resource contention and request failures", "When the number of LLM platform administrators exceeds governance thresholds, creating audit and access control risks"], correct: 1, explanation: "Excessive Agency (LLM06:2025) occurs when LLM-based systems are granted too much autonomy, permissions, or access to perform actions. This includes excessive permissions to APIs, databases, or external systems; overly broad functionality in plugins; or insufficient validation of LLM-initiated actions. The risk is that the LLM might be manipulated (via prompt injection) to perform unauthorized operations if it has excessive permissions or agency.", docLink: "https://genai.owasp.org/llmrisk/llm062025-excessive-agency/" },
            { id: 10, question: "Which AWS security practice best mitigates LLM06:2025 Excessive Agency?", options: ["Disabling all external plugin integrations entirely to eliminate any surface area for LLM-initiated unauthorized actions", "Applying least privilege IAM roles to action group Lambda functions, implementing user-level access control via user_id, and using human-in-the-loop processes for sensitive operations", "Using Amazon Cognito user pools to centralize LLM service authentication and enforce short-lived session token expiration", "Encrypting all LLM API calls with AWS KMS customer-managed keys and enforcing TLS 1.3 across every integration endpoint"], correct: 1, explanation: "Mitigating Excessive Agency requires implementing least privilege access at multiple layers. Action group Lambda functions should have IAM roles with minimal required permissions. User-level access control using user_id ensures proper authorization for third-party plugins and APIs. Human-in-the-loop processes should be implemented for sensitive operations. The combination of IAM-based infrastructure access control and application-level user authorization wraps deterministic controls around non-deterministic LLM behavior.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 11, question: "What is LLM07:2025 System Prompt Leakage and why is it a security concern?", options: ["When application or infrastructure logs containing LLM request details are inadvertently exposed to unauthorized users", "When the system prompt (containing instructions, guidelines, or sensitive context for the LLM) is exposed to attackers through manipulation or extraction", "When the LLM erroneously includes internal error messages or stack traces within its user-facing output responses", "When the prompt templates used during LLM fine-tuning are published in public model cards or open research documentation"], correct: 1, explanation: "System Prompt Leakage (LLM07:2025) is a new risk category in 2025 where attackers extract the system prompt that guides the LLM's behavior. System prompts often contain sensitive information like internal policies, business logic, security controls, or API access patterns. Leaking this information can help attackers craft more effective attacks, understand system limitations, or bypass security controls. This is particularly concerning because system prompts are fundamental to LLM application behavior.", docLink: "https://genai.owasp.org/llmrisk/llm072025-system-prompt-leakage/" },
            { id: 12, question: "What defines LLM08:2025 Vector and Embedding Weaknesses?", options: ["Poor network throughput and high latency between application servers and vector database nodes causing query timeouts", "Security vulnerabilities in RAG systems including poisoned embeddings, insecure vector databases, or unauthorized access to embedding stores", "Rendering failures caused by incompatible vector graphic formats in the LLM application's frontend interface layer", "Consistency problems in distributed LLM deployments arising from vector clock drift across geographically separated nodes"], correct: 1, explanation: "Vector and Embedding Weaknesses (LLM08:2025) represent security issues in Retrieval-Augmented Generation (RAG) systems and embedding-based architectures. Vulnerabilities include poisoned embeddings that influence retrieval results, insecure vector database access, compromised similarity search, or unauthorized manipulation of embedding stores. Since many LLM applications use RAG to ground responses in proprietary data, compromising the vector store or embeddings can subvert the entire system's knowledge base.", docLink: "https://genai.owasp.org/llmrisk/llm082025-vector-and-embedding-weaknesses/" },
            { id: 13, question: "How does Amazon Bedrock Knowledge Bases help mitigate LLM08:2025 Vector and Embedding Weaknesses?", options: ["By automatically scanning and patching known CVEs across all vector database engine dependencies without manual intervention", "By providing encryption at rest and in transit, TLS for third-party vector stores, and metadata filtering for user-level access control", "By replacing all vector embeddings with blockchain-anchored cryptographic tokens that provide tamper-evident storage", "By storing all vectors in plaintext within isolated VPC subnets to simplify inspection and accelerate similarity search"], correct: 1, explanation: "Amazon Bedrock Knowledge Bases mitigates vector and embedding risks through multiple mechanisms: encryption at rest using AWS KMS (customer or AWS-managed keys), TLS encryption in transit for third-party vector stores, encrypted ingestion jobs, and metadata filtering for implementing user-level access controls. The metadata filtering feature enables building segmented access solutions where users can only retrieve embeddings they're authorized to access, and periodic resyncing maintains accurate access controls.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 14, question: "What is LLM09:2025 Misinformation?", options: ["Intentional false statements injected by adversarial users through crafted prompts designed to manipulate downstream decisions", "When LLMs generate false, misleading, or fabricated information (hallucinations) that appears credible but is factually incorrect", "Spam, junk, or low-quality repetitive content generated by the LLM due to misconfigured sampling temperature settings", "Coordinated disinformation patterns detected and flagged by the LLM model during input content moderation processing"], correct: 1, explanation: "Misinformation (LLM09:2025) refers to LLMs generating false or misleading information that appears credible - commonly called 'hallucinations'. This poses a core vulnerability for applications relying on LLMs for factual information, as the model may confidently present fabricated data, incorrect facts, or non-existent sources. The risk is particularly high in decision-making contexts, customer support, or any application where users trust the LLM's factual accuracy. Misinformation can be unintentional (model limitations) or exploited by attackers.", docLink: "https://genai.owasp.org/llmrisk/llm092025-misinformation/" },
            { id: 15, question: "What characterizes LLM10:2025 Unbounded Consumption?", options: ["Users generating and consuming disproportionate LLM content volumes, overwhelming downstream moderation and logging pipelines", "Uncontrolled LLM resource usage leading to denial of service, excessive costs, model extraction, or system degradation", "Excessive GPU and power consumption by LLM inference workloads that breaches sustainability and carbon budget targets", "Conversation history and session logs exceeding configured storage quotas, causing data loss and retrieval failures"], correct: 1, explanation: "Unbounded Consumption (LLM10:2025) occurs when LLM operations consume excessive resources without proper controls, leading to denial of service, unexpected costs, model extraction through repeated queries, or system degradation. This includes resource-intensive prompts, excessive API calls, context window exhaustion, or lack of rate limiting. Attackers can exploit this to degrade service quality for others, incur significant costs for the provider, or extract model behavior through systematic querying.", docLink: "https://genai.owasp.org/llmrisk/llm102025-unbounded-consumption/" },
            { id: 16, question: "Which AWS services and configurations help mitigate LLM10:2025 Unbounded Consumption?", options: ["Restricting deployments exclusively to the smallest available foundation model variants to minimize per-request compute costs", "Implementing AWS WAF rate limiting, setting Amazon Bedrock request parameter limits (max token length), and configuring hard limits on Amazon Bedrock agent actions", "Disabling parallel processing and enforcing strict serialization of all LLM inference requests to prevent queue buildup", "Using Amazon CloudFront to cache frequently repeated LLM responses and reduce repeated downstream inference invocations"], correct: 1, explanation: "Mitigating Unbounded Consumption requires multiple controls: AWS WAF Bot Control provides rate limiting and blocks unintended requests. Amazon Bedrock allows setting request parameters like maximum input length to limit resource-intensive requests. Amazon Bedrock Agents have hard limits on the maximum number of queued and total actions to fulfill requests, preventing infinite loops. These controls prevent resource exhaustion, control costs, and maintain service availability during attack attempts.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 17, question: "Which layer of a generative AI assistant architecture is primarily responsible for mitigating authentication-layer attacks and LLM10:2025 Unbounded Consumption?", options: ["The LLM and LLM agent layer applying model-level output filters, sampling temperature controls, and fine-tuned safety guardrails", "The authentication layer using Amazon Cognito, AWS WAF, and AWS Shield Advanced", "The RAG data store layer enforcing encryption at rest, metadata access controls, and periodic embedding resyncing schedules", "The application controller layer validating and sanitizing all inbound requests before forwarding them to the LLM"], correct: 1, explanation: "The authentication layer, implemented with Amazon Cognito, AWS WAF, and AWS Shield Advanced, provides the first line of defense against authentication attacks and unbounded consumption. Amazon Cognito handles MFA, OAuth 2.0, secure session management, and risk-based adaptive authentication. AWS WAF with Bot Control filters malicious traffic and implements rate limiting. AWS Shield Advanced provides DDoS protection. These services work together to prevent unauthorized access and resource exhaustion before requests reach the application.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 18, question: "What is the primary security function of the application controller layer (LLM orchestrator Lambda) in the AWS reference architecture?", options: ["To continuously fine-tune and retrain the LLM models based on user interaction data and quality feedback signals", "To perform strict input validation, sanitization, and pass user_id downstream to enable access controls", "To persist, retrieve, and manage multi-turn conversation history in DynamoDB for dialogue context continuity", "To enforce transport-layer security for all API communications using TLS certificates provisioned through ACM"], correct: 1, explanation: "The application controller (LLM orchestrator Lambda) performs critical security functions: extracting and validating event payloads from API Gateway, conducting syntactic and semantic validation, sanitizing inputs through allowlisting/denylisting and format validation, and passing user_id downstream to enable user-level access controls. This layer protects against LLM01:2025 Prompt Injection and LLM02:2025 Sensitive Information Disclosure by implementing defense before requests reach the LLM.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 19, question: "How does Amazon Bedrock Guardrails protect against both input and output security risks?", options: ["By maintaining a curated allowlist of pre-approved prompt templates and blocking any user input that deviates from them", "By filtering harmful content in inputs and outputs using content filters, denied topics, word filters, PII detection, and sensitive information redaction", "By intercepting all user inputs and substituting them with safe pre-validated default prompts prior to LLM processing", "By encrypting all prompts and model responses with AWS KMS customer-managed keys to prevent unauthorized interception"], correct: 1, explanation: "Amazon Bedrock Guardrails provides comprehensive protection for both inputs and outputs through multiple filter types: content filters with configurable thresholds for harmful categories, denied topics to block specific subjects, word filters to block offensive terms, contextual grounding checks, PII detection and redaction, and sensitive information filtering using regex patterns. Guardrails automatically evaluates both user inputs and model responses, providing defense against LLM01:2025 Prompt Injection, LLM02:2025 Sensitive Information Disclosure, and LLM05:2025 Improper Output Handling.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 20, question: "What security principle should guide the treatment of LLM outputs according to AWS best practices?", options: ["LLM outputs should be implicitly trusted because they originate from a controlled, vendor-managed AI inference service", "LLM outputs should be treated as untrusted user input requiring validation, sanitization, and encoding before use", "LLM outputs should be encrypted with customer-managed KMS keys before persisting them to any downstream storage layer", "LLM outputs should be held in a review queue and only released after a designated human reviewer approves each response"], correct: 1, explanation: "The critical security principle is to treat the LLM as an untrusted user. LLM-generated outputs should undergo the same rigorous validation, sanitization, and encoding as any user-provided input. This means applying output encoding (HTML entity encoding, JavaScript escaping), validating against expected formats, and sanitizing before use in downstream systems. This principle addresses LLM05:2025 Improper Output Handling and prevents XSS, SSRF, code injection, and privilege escalation vulnerabilities.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 21, question: "How does the agent plugin controller layer mitigate LLM06:2025 Excessive Agency and LLM08:2025 Vector & Embedding Weaknesses?", options: ["By caching all third-party plugin responses in ElastiCache to reduce repeated external API calls and attack surface area", "By applying least privilege IAM roles and implementing custom user-level authorization logic using user_id for third-party plugins and data sources", "By capping the total number of registered action group plugins at five per application to limit the potential attack surface", "By requiring all plugins and action groups to be developed, reviewed, and certified exclusively by approved AWS teams"], correct: 1, explanation: "The agent plugin controller (action group Lambda) mitigates excessive agency through two key mechanisms: (1) Least privilege IAM roles that limit infrastructure-level access to only required AWS services, and (2) Custom authorization logic that uses the user_id parameter to enforce user-level access control for third-party plugins and APIs. This dual-layer approach - IAM for infrastructure and user_id for data plane - wraps deterministic access controls around non-deterministic LLM behavior, preventing unauthorized plugin execution or data access.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 22, question: "What role does Amazon Bedrock prompt management and versioning play in security?", options: ["It creates an immutable audit trail linking specific prompt versions to security incidents to support forensic investigations", "It enables continuous improvement of user experience while maintaining security through careful management of prompt changes and handling", "It automatically generates security-hardened prompt templates using static analysis that prevents exploitation by attackers", "It manages versioning of the underlying LLM foundation models themselves, enabling safe rollback after a problematic update"], correct: 1, explanation: "Amazon Bedrock prompt management and versioning enables security-conscious evolution of LLM applications. By carefully managing changes to prompts and their handling, organizations can enhance functionality without introducing new vulnerabilities. Versioning allows testing security implications of prompt changes, rolling back problematic modifications, and maintaining audit trails. This capability helps mitigate LLM01:2025 Prompt Injection attacks by enabling controlled, tested updates to system prompts and guardrail configurations.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 23, question: "According to the AWS Generative AI Security Scoping Matrix, where does a custom-built generative AI assistant (Scope 3) require the most security controls?", options: ["Only at the model training and fine-tuning layer, since all other infrastructure components are fully managed and secured by AWS", "Across the full stack from frontend through LLM integration, since you build and secure the application while using managed LLM services", "Only at the API authentication gateway layer, since the managed LLM service handles all other downstream security controls", "Only at the data storage and conversation history layer, since application logic security is handled by the LLM provider"], correct: 1, explanation: "Scope 3 applications (custom-built assistants using managed LLMs like Amazon Bedrock) require comprehensive security controls across the full stack. Your security responsibility includes the frontend, authentication, application logic, prompt engineering, output handling, plugin integrations, and RAG implementation - essentially everything except the base LLM model itself. This is broader than Scope 1/2 (consumer apps) but narrower than Scope 4/5 (custom model training). The security controls span from edge protection to data access controls.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" },
            { id: 24, question: "What is a key difference between the OWASP Top 10 for LLMs 2025 and the 2023/2024 version?", options: ["The 2025 version completely replaces the prior risk taxonomy with ten entirely new categories not present in earlier editions", "The 2025 version includes System Prompt Leakage as a new risk category and emphasizes system-wide security beyond individual prompts", "The 2025 version narrows its scope to apply exclusively to closed-source commercial LLMs used in enterprise environments", "The 2025 version pivots focus exclusively to training-time data poisoning and supply chain attacks on foundation model pipelines"], correct: 1, explanation: "The OWASP Top 10 for LLMs 2025 evolved to address real-world deployment lessons and agentic AI architectures. Key changes include adding LLM07:2025 System Prompt Leakage as a new dedicated category, emphasizing system-wide security across the entire application stack beyond just prompt-level controls, providing specific guidance for RAG systems and plugin ecosystems, and addressing socio-technical risks. The framework shifted from focusing primarily on individual LLM interactions to considering the complete security posture of GenAI applications.", docLink: "https://genai.owasp.org/llm-top-10/" },
            { id: 25, question: "Which combination of security practices provides defense-in-depth for generative AI applications according to AWS and OWASP guidance?", options: ["Deploying Amazon Bedrock Guardrails as the sole security control since it covers all OWASP GenAI risk categories end-to-end", "Implementing perimeter security (WAF/Shield), authentication (Cognito with MFA), input validation, guardrails, output encoding, least privilege access, user-level authorization, and secure RAG with encryption", "Relying on the LLM vendor's built-in safety training and content moderation policies to prevent all application-layer threats", "Applying only IAM permission boundaries and VPC security groups to network-isolate the LLM from unauthorized access"], correct: 1, explanation: "Comprehensive GenAI security requires defense-in-depth across all layers: perimeter security with AWS WAF and Shield Advanced for DDoS/bot protection; strong authentication with Amazon Cognito MFA and adaptive auth; strict input validation and sanitization in application logic; Amazon Bedrock Guardrails for content filtering; output encoding to prevent injection; least privilege IAM roles; user-level authorization for data plane access; secure RAG implementation with encryption and access controls; and CloudWatch monitoring with sensitive data masking. No single control is sufficient - layered security is essential.", docLink: "https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/" }
        ];

        let currentMode = 'learn', currentQuestion = 0, userAnswers = [], answersRevealed = [], shuffledQuestions = [], timerInterval = null, timeRemaining = 0, inReviewMode = false, reviewedQuestions = [];
        function shuffleArray(a) { const b = [...a]; for (let i = b.length - 1; i > 0; i--) { const j = Math.floor(Math.random() * (i + 1)); [b[i], b[j]] = [b[j], b[i]]; } return b; }
        function shuffleQuestionOptions(q) { const idx = shuffleArray([0,1,2,3]); return { ...q, options: idx.map(i => q.options[i]), correct: idx.indexOf(q.correct) }; }
        function startQuiz(mode) { currentMode = mode; currentQuestion = 0; userAnswers = new Array(questions.length).fill(null); answersRevealed = new Array(questions.length).fill(false); shuffledQuestions = shuffleArray(questions).map(shuffleQuestionOptions); inReviewMode = false; reviewedQuestions = []; document.getElementById('modeSelector').classList.add('hidden'); document.getElementById('quizContent').classList.remove('hidden'); document.getElementById('resultsCard').classList.remove('show'); document.getElementById('totalQ').textContent = shuffledQuestions.length; document.getElementById('backToReviewBtn').style.display = 'none'; if (mode === 'exam') { timeRemaining = shuffledQuestions.length * 150; startTimer(); document.getElementById('shuffleBtn').classList.add('hidden'); } else { document.getElementById('timer').textContent = 'Learn Mode'; document.getElementById('shuffleBtn').classList.remove('hidden'); } renderQuestion(); }
        function startTimer() { updateTimerDisplay(); timerInterval = setInterval(() => { timeRemaining--; updateTimerDisplay(); if (timeRemaining <= 0) { clearInterval(timerInterval); showResults(); } }, 1000); }
        function updateTimerDisplay() { const m = Math.floor(timeRemaining / 60), s = timeRemaining % 60, el = document.getElementById('timer'); el.textContent = `${m}:${s.toString().padStart(2, '0')}`; el.classList.remove('warning', 'danger'); if (timeRemaining <= 60) el.classList.add('danger'); else if (timeRemaining <= 300) el.classList.add('warning'); }
        function renderQuestion() { const q = shuffledQuestions[currentQuestion], letters = ['A','B','C','D'], ans = userAnswers[currentQuestion], revealed = answersRevealed[currentQuestion]; let opts = q.options.map((o, i) => { let c = 'option'; if (currentMode === 'learn' && revealed) { c += ' disabled'; if (i === q.correct) c += ' correct'; else if (i === ans && ans !== q.correct) c += ' incorrect'; } else if (ans === i) c += ' selected'; return `<div class="${c}" onclick="selectOption(${i})"><span class="option-marker">${letters[i]}</span><span class="option-text">${o}</span></div>`; }).join(''); let exp = currentMode === 'learn' ? `<div class="explanation ${revealed ? 'show' : ''}"><div class="explanation-title">üìñ Explanation</div><div class="answer-header">Correct Answer: ${letters[q.correct]}</div><p class="explanation-text">${q.explanation}</p><a href="${q.docLink}" target="_blank" class="doc-link">üìÑ View Documentation ‚Üí</a></div>` : ''; document.getElementById('questionsContainer').innerHTML = `<div class="question-card"><div class="question-header"><span class="question-number">Question ${currentQuestion + 1}</span><span class="question-domain">GenAI Security</span></div><p class="question-text">${q.question}</p><div class="options">${opts}</div>${currentMode === 'learn' && !revealed ? '<button class="btn btn-primary" style="margin-top:1rem;width:100%;" onclick="revealAnswer()">Show Answer</button>' : ''}${exp}</div>`; document.getElementById('currentQ').textContent = currentQuestion + 1; document.getElementById('progressFill').style.width = `${((currentQuestion + 1) / shuffledQuestions.length) * 100}%`; document.getElementById('prevBtn').disabled = currentQuestion === 0; document.getElementById('nextBtn').textContent = currentQuestion === shuffledQuestions.length - 1 ? (currentMode === 'exam' ? 'Submit Quiz' : 'See Results') : 'Next ‚Üí'; }
        function selectOption(i) { if (currentMode === 'learn' && answersRevealed[currentQuestion]) return; userAnswers[currentQuestion] = i; renderQuestion(); }
        function revealAnswer() { if (userAnswers[currentQuestion] === null) userAnswers[currentQuestion] = -1; answersRevealed[currentQuestion] = true; renderQuestion(); }
        function navigate(d) { if (d === 1 && currentQuestion === shuffledQuestions.length - 1) { showResults(); return; } currentQuestion = Math.max(0, Math.min(currentQuestion + d, shuffledQuestions.length - 1)); renderQuestion(); }
        function shuffleQuiz() { if (currentMode !== 'learn') return; currentQuestion = 0; userAnswers = new Array(questions.length).fill(null); answersRevealed = new Array(questions.length).fill(false); shuffledQuestions = shuffleArray(questions).map(shuffleQuestionOptions); renderQuestion(); }
        function showResults() { if (timerInterval) clearInterval(timerInterval); let correct = shuffledQuestions.reduce((c, q, i) => c + (userAnswers[i] === q.correct ? 1 : 0), 0); const total = shuffledQuestions.length, pct = Math.round((correct / total) * 100), pass = pct >= 80; document.getElementById('quizContent').classList.add('hidden'); document.getElementById('resultsCard').classList.add('show'); document.getElementById('reviewSection').classList.remove('show'); document.getElementById('scorePercent').textContent = pct + '%'; document.getElementById('scorePercent').className = 'results-score ' + (pass ? 'pass' : 'fail'); document.getElementById('resultStatus').textContent = pass ? 'üéâ PASSED!' : 'üìö Keep Studying'; document.getElementById('resultStatus').className = 'results-status ' + (pass ? 'pass' : 'fail'); document.getElementById('resultDetails').textContent = `You answered ${correct} out of ${total} questions correctly.`; document.getElementById('correctCount').textContent = correct; document.getElementById('incorrectCount').textContent = total - correct; }
        function showReview() { const letters = ['A','B','C','D']; const tbody = document.getElementById('reviewTableBody'); tbody.innerHTML = shuffledQuestions.map((q, i) => { const userAns = userAnswers[i]; const isCorrect = userAns === q.correct; const userLetter = userAns !== null && userAns >= 0 ? letters[userAns] : 'No Answer'; const correctLetter = letters[q.correct]; const statusIcon = isCorrect ? '‚úì' : '‚úó'; const statusClass = isCorrect ? 'correct' : 'incorrect'; const qText = q.question.length > 120 ? q.question.substring(0, 120) + '...' : q.question; const reviewedClass = reviewedQuestions.includes(i) ? 'reviewed' : ''; return `<tr class="${reviewedClass}" onclick="reviewQuestion(${i})"><td class="review-q-num">${i + 1}</td><td class="review-q-text">${qText}</td><td class="review-answer ${userAns === q.correct ? 'correct' : 'incorrect'}">${userLetter}</td><td class="review-answer correct">${correctLetter}</td><td class="review-status ${statusClass}">${statusIcon}</td></tr>`; }).join(''); document.getElementById('resultsCard').classList.remove('show'); document.getElementById('reviewSection').classList.add('show'); inReviewMode = true; }
        function reviewQuestion(qIndex) { if (!reviewedQuestions.includes(qIndex)) { reviewedQuestions.push(qIndex); } currentMode = 'learn'; currentQuestion = qIndex; answersRevealed[qIndex] = true; document.getElementById('reviewSection').classList.remove('show'); document.getElementById('quizContent').classList.remove('hidden'); document.getElementById('timer').textContent = 'Learn Mode'; document.getElementById('shuffleBtn').style.display = 'none'; document.getElementById('backToReviewBtn').style.display = 'inline-block'; if (timerInterval) clearInterval(timerInterval); renderQuestion(); }
        function backToReview() { document.getElementById('quizContent').classList.add('hidden'); showReview(); }
        function backToResults() { document.getElementById('reviewSection').classList.remove('show'); document.getElementById('resultsCard').classList.add('show'); inReviewMode = false; }
        function restartQuiz() { document.getElementById('resultsCard').classList.remove('show'); document.getElementById('reviewSection').classList.remove('show'); document.getElementById('modeSelector').classList.remove('hidden'); document.getElementById('quizContent').classList.add('hidden'); document.getElementById('backToReviewBtn').style.display = 'none'; inReviewMode = false; reviewedQuestions = []; if (timerInterval) clearInterval(timerInterval); }
    </script>
</body>
</html>
