<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Practice Exam 1 | AWS Security Specialty (SCS-C03)</title>
    <style>
        :root { --bg-primary: #0a0e17; --bg-secondary: #141b2d; --bg-tertiary: #1a2236; --text-primary: #e2e8f0; --text-secondary: #94a3b8; --accent-primary: #6366f1; --accent-secondary: #818cf8; --success: #22c55e; --error: #ef4444; --warning: #eab308; --info: #3b82f6; --border: #2d3a52; }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', system-ui, sans-serif; background: var(--bg-primary); color: var(--text-primary); line-height: 1.6; min-height: 100vh; }
        .container { max-width: 900px; margin: 0 auto; padding: 2rem; }
        header { text-align: center; margin-bottom: 2rem; padding: 2rem; background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-tertiary) 100%); border-radius: 16px; border: 1px solid var(--border); }
        .badge { display: inline-block; background: var(--accent-primary); color: white; padding: 0.25rem 0.75rem; border-radius: 20px; font-size: 0.75rem; font-weight: 600; margin-bottom: 1rem; text-transform: uppercase; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
        .subtitle { color: var(--text-secondary); font-size: 1rem; }
        .stats-bar { display: flex; justify-content: center; gap: 2rem; margin-top: 1.5rem; padding-top: 1.5rem; border-top: 1px solid var(--border); }
        .stat { text-align: center; } .stat-value { font-size: 1.5rem; font-weight: 700; color: var(--accent-primary); } .stat-label { font-size: 0.75rem; color: var(--text-secondary); text-transform: uppercase; }
        .mode-selector { display: flex; gap: 1rem; justify-content: center; margin-bottom: 2rem; }
        .mode-btn { padding: 0.75rem 2rem; border: 2px solid var(--border); background: var(--bg-secondary); color: var(--text-primary); border-radius: 8px; cursor: pointer; font-size: 1rem; transition: all 0.3s ease; }
        .mode-btn:hover { border-color: var(--accent-primary); background: var(--bg-tertiary); }
        .progress-container { background: var(--bg-secondary); border-radius: 12px; padding: 1rem 1.5rem; margin-bottom: 1.5rem; border: 1px solid var(--border); }
        .progress-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 0.75rem; }
        .progress-text { font-size: 0.875rem; color: var(--text-secondary); }
        .timer { font-size: 1.25rem; font-weight: 600; color: var(--accent-primary); font-family: 'Consolas', monospace; }
        .timer.warning { color: var(--warning); animation: pulse 1s infinite; } .timer.danger { color: var(--error); animation: pulse 0.5s infinite; }
        @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
        .progress-bar { height: 8px; background: var(--bg-tertiary); border-radius: 4px; overflow: hidden; }
        .progress-fill { height: 100%; background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary)); border-radius: 4px; transition: width 0.3s ease; }
        .question-card { background: var(--bg-secondary); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; border: 1px solid var(--border); }
        .question-header { display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 1rem; }
        .question-number { background: var(--bg-tertiary); color: var(--accent-primary); padding: 0.25rem 0.75rem; border-radius: 6px; font-size: 0.875rem; font-weight: 600; }
        .question-domain { font-size: 0.75rem; color: var(--text-secondary); background: var(--bg-tertiary); padding: 0.25rem 0.5rem; border-radius: 4px; }
        .question-text { font-size: 1.125rem; margin-bottom: 1.5rem; line-height: 1.7; }
        .options { display: flex; flex-direction: column; gap: 0.75rem; }
        .option { display: flex; align-items: flex-start; gap: 1rem; padding: 1rem; background: var(--bg-tertiary); border: 2px solid var(--border); border-radius: 10px; cursor: pointer; transition: all 0.2s ease; }
        .option:hover:not(.disabled) { border-color: var(--accent-primary); background: rgba(99, 102, 241, 0.1); }
        .option.selected { border-color: var(--accent-primary); border-width: 3px; background: rgba(99, 102, 241, 0.2); box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.25), inset 0 0 12px rgba(99, 102, 241, 0.1); }
        .option.correct { border-color: var(--success); background: rgba(34, 197, 94, 0.15); }
        .option.incorrect { border-color: var(--error); background: rgba(239, 68, 68, 0.15); }
        .option.disabled { cursor: default; }
        .option-marker { width: 28px; height: 28px; border-radius: 50%; border: 2px solid var(--border); display: flex; align-items: center; justify-content: center; font-weight: 600; font-size: 0.875rem; flex-shrink: 0; }
        .option.selected .option-marker { border-color: var(--accent-primary); background: var(--accent-primary); color: white; }
        .option.correct .option-marker { border-color: var(--success); background: var(--success); color: white; }
        .option.incorrect .option-marker { border-color: var(--error); background: var(--error); color: white; }
        .option-text { flex: 1; padding-top: 2px; }
        .explanation { margin-top: 1.5rem; padding: 1.5rem; background: var(--bg-tertiary); border-radius: 10px; border-left: 4px solid var(--info); display: none; }
        .explanation.show { display: block; animation: fadeIn 0.3s ease; }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(-10px); } to { opacity: 1; transform: translateY(0); } }
        .explanation-title { display: flex; align-items: center; gap: 0.5rem; font-weight: 600; margin-bottom: 0.75rem; color: var(--info); }
        .answer-header { font-weight: 600; color: var(--success); margin-bottom: 0.5rem; }
        .explanation-text { color: var(--text-secondary); margin-bottom: 1rem; }
        .doc-link { display: inline-flex; align-items: center; gap: 0.5rem; color: var(--accent-primary); text-decoration: none; font-size: 0.875rem; padding: 0.5rem 1rem; background: rgba(99, 102, 241, 0.1); border-radius: 6px; }
        .doc-link:hover { background: rgba(99, 102, 241, 0.2); }
        .btn-container { display: flex; justify-content: space-between; gap: 1rem; margin-top: 1rem; }
        .btn { padding: 0.75rem 1.5rem; border: none; border-radius: 8px; font-size: 1rem; font-weight: 600; cursor: pointer; transition: all 0.2s ease; }
        .btn-primary { background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary)); color: white; }
        .btn-primary:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(99, 102, 241, 0.4); }
        .btn-secondary { background: var(--bg-tertiary); color: var(--text-primary); border: 1px solid var(--border); }
        .btn-secondary:hover { border-color: var(--accent-primary); }
        .btn:disabled { opacity: 0.5; cursor: not-allowed; }
        .results-card { background: var(--bg-secondary); border-radius: 16px; padding: 2rem; text-align: center; border: 1px solid var(--border); display: none; }
        .results-card.show { display: block; animation: fadeIn 0.5s ease; }
        .results-score { font-size: 4rem; font-weight: 700; margin: 1rem 0; }
        .results-score.pass { color: var(--success); } .results-score.fail { color: var(--error); }
        .results-status { font-size: 1.5rem; margin-bottom: 1rem; }
        .results-status.pass { color: var(--success); } .results-status.fail { color: var(--error); }
        .results-details { color: var(--text-secondary); margin-bottom: 2rem; }
        .results-breakdown { display: flex; justify-content: center; gap: 3rem; margin-bottom: 2rem; padding: 1.5rem; background: var(--bg-tertiary); border-radius: 10px; }
        .breakdown-item { text-align: center; } .breakdown-value { font-size: 2rem; font-weight: 700; } .breakdown-value.correct { color: var(--success); } .breakdown-value.incorrect { color: var(--error); } .breakdown-label { font-size: 0.875rem; color: var(--text-secondary); }
        .review-section { background: var(--bg-secondary); border-radius: 16px; padding: 2rem; border: 1px solid var(--border); display: none; }
        .review-section.show { display: block; animation: fadeIn 0.5s ease; }
        .review-header { text-align: center; margin-bottom: 2rem; }
        .review-table { width: 100%; border-collapse: collapse; margin-bottom: 2rem; }
        .review-table th { background: var(--bg-tertiary); color: var(--text-secondary); font-size: 0.875rem; font-weight: 600; text-transform: uppercase; padding: 1rem; text-align: left; border-bottom: 2px solid var(--border); }
        .review-table td { padding: 1rem; border-bottom: 1px solid var(--border); vertical-align: top; }
        .review-table tbody tr { cursor: pointer; transition: all 0.2s ease; }
        .review-table tbody tr:hover { background: rgba(99, 102, 241, 0.1); transform: translateX(4px); }
        .review-table tbody tr.reviewed { background: rgba(99, 102, 241, 0.25); border-left: 6px solid var(--accent-primary); box-shadow: inset 0 0 20px rgba(99, 102, 241, 0.15); }
        .review-table tbody tr.reviewed:hover { background: rgba(99, 102, 241, 0.3); }
        .review-table tbody tr.reviewed .review-q-num { color: var(--accent-secondary); font-weight: 800; }
        .review-table tbody tr.reviewed .review-q-num::before { content: "‚úì "; color: var(--accent-primary); font-size: 1rem; }
        .review-q-num { font-weight: 700; color: var(--accent-primary); white-space: nowrap; }
        .review-q-text { color: var(--text-primary); line-height: 1.6; }
        .review-answer { font-weight: 600; }
        .review-answer.correct { color: var(--success); }
        .review-answer.incorrect { color: var(--error); }
        .review-status { text-align: center; font-size: 1.25rem; }
        .review-hint { text-align: center; color: var(--text-secondary); font-size: 0.875rem; margin-bottom: 1rem; }
        .btn-group { display: flex; gap: 1rem; justify-content: center; }
        .hidden { display: none !important; }
        .shuffle-notice { text-align: center; padding: 0.75rem; background: rgba(99, 102, 241, 0.1); border-radius: 8px; margin-bottom: 1.5rem; color: var(--accent-secondary); font-size: 0.875rem; }
        @media (max-width: 640px) { .container { padding: 1rem; } h1 { font-size: 1.5rem; } .stats-bar { gap: 1rem; } .mode-selector, .btn-container, .results-breakdown { flex-direction: column; } }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <span class="badge">Full Practice Exam - All Domains</span>
            <h1>üìù Practice Exam 1</h1>
            <p class="subtitle">65 Questions - AWS Security Specialty (SCS-C03)</p>
            <div class="stats-bar">
                <div class="stat"><div class="stat-value">65</div><div class="stat-label">Questions</div></div>
                <div class="stat"><div class="stat-value">80%</div><div class="stat-label">Pass Score</div></div>
                <div class="stat"><div class="stat-value">162:30</div><div class="stat-label">Exam Time</div></div>
            </div>
        </header>
        <div class="mode-selector" id="modeSelector">
            <button class="mode-btn" onclick="startQuiz('learn')">üìö Learn Mode</button>
            <button class="mode-btn" onclick="startQuiz('exam')">‚è±Ô∏è Exam Mode</button>
        </div>
        <div id="quizContent" class="hidden">
            <div class="progress-container">
                <div class="progress-header">
                    <span class="progress-text">Question <span id="currentQ">1</span> of <span id="totalQ">65</span></span>
                    <span class="timer" id="timer">162:30</span>
                </div>
                <div class="progress-bar"><div class="progress-fill" id="progressFill" style="width: 0%"></div></div>
            </div>
            <div class="shuffle-notice" id="shuffleNotice">üîÄ Questions and options have been randomized</div>
            <div id="questionsContainer"></div>
            <div class="btn-container">
                <button class="btn btn-secondary" id="prevBtn" onclick="navigate(-1)" disabled>‚Üê Previous</button>
                <button class="btn btn-secondary" id="backToReviewBtn" onclick="backToReview()" style="display: none;">‚Üê Back to Review</button>
                <button class="btn btn-secondary" id="shuffleBtn" onclick="shuffleQuiz()">üîÄ Shuffle</button>
                <button class="btn btn-primary" id="nextBtn" onclick="navigate(1)">Next ‚Üí</button>
            </div>
        </div>
        <div class="results-card" id="resultsCard">
            <h2>Quiz Complete!</h2>
            <div class="results-score" id="scorePercent">0%</div>
            <div class="results-status" id="resultStatus">-</div>
            <p class="results-details" id="resultDetails"></p>
            <div class="results-breakdown">
                <div class="breakdown-item"><div class="breakdown-value correct" id="correctCount">0</div><div class="breakdown-label">Correct</div></div>
                <div class="breakdown-item"><div class="breakdown-value incorrect" id="incorrectCount">0</div><div class="breakdown-label">Incorrect</div></div>
            </div>
            <div class="btn-group">
                <button class="btn btn-primary" onclick="showReview()">üìä Review Answers</button>
                <button class="btn btn-secondary" onclick="restartQuiz()">üîÑ Restart Quiz</button>
            </div>
        </div>
        <div class="review-section" id="reviewSection">
            <div class="review-header">
                <h2>Answer Review</h2>
                <p class="results-details">Click any question to review the full answer and explanation</p>
            </div>
            <p class="review-hint">üí° Tip: Click on any row to see the full question and explanation in Learn mode</p>
            <table class="review-table">
                <thead>
                    <tr>
                        <th style="width: 5%">#</th>
                        <th style="width: 50%">Question</th>
                        <th style="width: 15%">Your Answer</th>
                        <th style="width: 15%">Correct Answer</th>
                        <th style="width: 10%">Status</th>
                    </tr>
                </thead>
                <tbody id="reviewTableBody"></tbody>
            </table>
            <div class="btn-group">
                <button class="btn btn-secondary" onclick="backToResults()">‚Üê Back to Results</button>
                <button class="btn btn-primary" onclick="restartQuiz()">üîÑ Restart Quiz</button>
            </div>
        </div>
    </div>
    <script>
        const questions = [
            { id: 1, question: "You are a Security Engineer at FinTech Corp. Amazon GuardDuty has detected a `Backdoor:EC2/C&CActivity.B` finding on a production EC2 instance running a payment processing application. The instance is part of an Auto Scaling group behind an Application Load Balancer. Your security team needs to investigate the instance while minimizing impact to production traffic. What is the CORRECT sequence of steps to remediate this potentially compromised EC2 instance?", options: ["Terminate the instance immediately, rely on the Auto Scaling group to launch a replacement, and review CloudTrail logs afterward.","Detach the instance from the Auto Scaling group, create a dedicated isolation security group with restricted access, associate the isolation security group with the instance, remove all other security group associations, and investigate for malware.","Stop the instance, take an EBS snapshot for forensic analysis, then terminate the instance and launch a new one from a known-good AMI.","Run an on-demand GuardDuty malware scan, wait for results, and if malware is detected, update the instance's security groups to block outbound traffic."], correct: 1, explanation: "The AWS-recommended remediation steps for a potentially compromised EC2 instance are:  1. **Identify** the potentially compromised instance and investigate for malware 2. **Isolate** the instance by creating a dedicated isolation security group that only allows specific inbound/outbound access, associating it with the instance, and removing all other security group associations 3. **Identify the source** of suspicious activity and stop unauthorized processes 4. If unable to remediate, terminate and replace the instance  Option A is incorrect because terminating immediately destroys forensic evidence. Option C stops the instance which may lose volatile memory data needed for investigation. Option D delays isolation while waiting for scan results, allowing potential continued C&C communication.  **Important Note:** Changing security groups does not terminate existing tracked connections‚Äîonly future traffic is blocked. For complete isolation, consider using NACLs.", docLink: "https://docs.aws.amazon.com/guardduty/latest/ug/compromised-ec2.html" },
            { id: 2, question: "A Security Analyst at your company discovers that an AWS access key has been posted on a public GitHub repository. The key ID begins with `ASIA`. The analyst needs to determine the scope of the compromise and take appropriate remediation steps. Which TWO statements are TRUE about this situation? (Select TWO)", options: ["The key is a long-term credential associated with an IAM user and should be immediately deleted or rotated via the IAM console.","The key is a temporary credential generated by AWS STS and cannot be viewed or managed in the AWS Management Console.","To identify how the key was obtained, the analyst should examine the `sessionIssuer` element in CloudTrail logs.","The key will remain valid indefinitely until explicitly revoked by deleting the associated IAM user."], correct: 1, explanation: "AWS access key IDs have specific prefixes that indicate their type: - **AKIA** = Long-term credentials associated with an IAM user or root account (managed in IAM console) - **ASIA** = Short-term temporary credentials from AWS STS (cannot be managed in console, expire automatically)  Since the key begins with `ASIA`: - It's a temporary STS credential (B is correct) - It was likely generated via AssumeRole, GetFederationToken, or GetSessionToken - The `sessionIssuer` element in CloudTrail reveals how the credential was obtained (C is correct) - It will expire automatically (D is incorrect‚Äîtemporary credentials have a defined lifetime)  Option A is incorrect because ASIA keys are temporary, not long-term. Option E is overkill and doesn't address the actual compromised temporary credential.", docLink: "https://docs.aws.amazon.com/guardduty/latest/ug/compromised-creds.html" },
            { id: 3, question: "Amazon GuardDuty has generated a `Policy:S3/BucketAnonymousAccessGranted` finding for a bucket containing customer PII data. Your investigation reveals that a bucket policy was modified 2 hours ago to allow public access. The bucket has versioning enabled and server access logging configured. Which combination of steps should you take to investigate and remediate this incident? (Select THREE)", options: ["Review S3 server access logs to identify any data access that occurred during the 2-hour window.","Immediately delete the bucket to prevent further unauthorized access.","Review CloudTrail logs to identify the IAM principal that modified the bucket policy.","Restore the previous bucket policy version using S3 versioning."], correct: 0, explanation: "For a compromised S3 bucket: 1. **Investigate** by reviewing CloudTrail to identify who modified the policy (C) and S3 access logs to see what data was accessed (A) 2. **Remediate** by removing public access and enabling S3 Block Public Access (F)  Option B (deleting the bucket) is destructive and loses evidence and data. Option D is incorrect because bucket policies are not versioned (only objects are versioned). Option E is incorrect because S3 Object Lock protects objects from deletion, not policies from modification.", docLink: "https://docs.aws.amazon.com/guardduty/latest/ug/compromised-s3.html" },
            { id: 4, question: "Your company runs containerized microservices on Amazon ECS with Fargate. GuardDuty Runtime Monitoring has detected a `RuntimeProtection:ECS/MaliciousFile` finding. The affected task is part of a service handling real-time transaction processing. What steps should you take to contain and investigate this threat while maintaining service availability?", options: ["Stop the ECS service immediately to prevent the malicious container from processing any more transactions.","Update the ECS task definition to use a known-good container image, then force a new deployment to replace the compromised tasks.","Enable VPC Flow Logs for the ECS cluster VPC to capture network traffic from the malicious container.","Modify the security group associated with the ECS tasks to block all outbound internet access, isolating the potentially compromised containers."], correct: 1, explanation: "For compromised ECS containers, the recommended approach is: 1. Identify the compromised task and container image 2. Update the task definition to use a verified clean image 3. Force a new deployment, which replaces running tasks with new ones from the clean image  This maintains service availability while replacing compromised containers. Option A and E cause service outages. Option C is useful for investigation but doesn't contain the threat. Option D may disrupt legitimate application functionality and doesn't replace the compromised container.", docLink: "https://docs.aws.amazon.com/guardduty/latest/ug/compromised-ecs.html" },
            { id: 5, question: "A healthcare company subject to HIPAA regulations needs to implement a logging strategy for their AWS environment. Requirements include: - All API calls must be logged across all AWS accounts - Logs must be instantly searchable and available for analysis for 90 days - For compliance, logs must be restorable for 7 years - The solution must be cost-effective Which approach meets ALL requirements?", options: ["Enable AWS CloudTrail in each account with logs delivered to a centralized S3 bucket. Configure Amazon CloudWatch Logs integration for real-time analysis. Set an S3 lifecycle policy to transition logs to S3 Glacier after 90 days and expire after 7 years.","Enable AWS CloudTrail in each account with logs delivered to Amazon CloudWatch Logs. Set log retention to 90 days. Export logs to S3 Glacier monthly for 7-year retention.","Enable AWS CloudTrail in the management account only with organization trail enabled. Deliver logs directly to S3 Glacier for cost savings. Use Athena with S3 Glacier Instant Retrieval for analysis.","Enable AWS CloudTrail Lake with 90-day retention. Configure event data stores for each account. Export to S3 annually for long-term storage."], correct: 0, explanation: "The requirements are: - **Instantly available for 90 days:** S3 Standard with CloudWatch Logs integration enables immediate search and analysis - **Restorable for 7 years:** S3 Glacier after 90 days provides cost-effective long-term storage - **All accounts:** Centralized S3 bucket with cross-account CloudTrail delivery  Option B loses logs after 90 days in CloudWatch and monthly exports create gaps. Option C delivers directly to Glacier which doesn't allow instant analysis (Glacier has retrieval delays). Option D's CloudTrail Lake has per-query costs that become expensive at scale for 7 years.", docLink: "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-examples.html" },
            { id: 6, question: "A company has implemented a VPC endpoint for AWS KMS with the following endpoint policy:  ```json {   \"Statement\": [{     \"Principal\": \"*\",     \"Action\": [\"kms:Encrypt\", \"kms:Decrypt\"],     \"Effect\": \"Allow\",     \"Resource\": \"arn:aws:kms:us-east-1:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab\"   }] } ```  A developer reports that their Lambda function (which has a key policy allowing `kms:GenerateDataKey` on the same key) is failing with \"Access Denied\" errors when calling `GenerateDataKey`. What is the cause of this failure?", options: ["The VPC endpoint policy does not allow the `kms:GenerateDataKey` action, so the request is denied even though the key policy permits it.","The Lambda function's IAM role is missing the `kms:ViaService` condition key.","The KMS key policy must explicitly include the VPC endpoint ID in a condition key.","VPC endpoints for KMS only support encryption and decryption operations; data key generation must go through the public endpoint."], correct: 0, explanation: "For a KMS request using a VPC endpoint to succeed, the request must be allowed by **BOTH**: 1. The KMS key policy (or IAM policy/grant) 2. The VPC endpoint policy  The endpoint policy only allows `kms:Encrypt` and `kms:Decrypt`. Even though the key policy allows `kms:GenerateDataKey`, the VPC endpoint policy denies it because `GenerateDataKey` is not in the allowed actions list.  This is a common pitfall: the VPC endpoint policy acts as an additional authorization layer, not a replacement for key policies.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/vpce-connect.html" },
            { id: 7, question: "Your organization requires that all KMS API calls must originate from within your corporate VPCs. You need to create a key policy condition that enforces this requirement. The VPCs have IDs `vpc-11111111` and `vpc-22222222`, and you have a KMS VPC endpoint with ID `vpce-1a2b3c4d`. Which condition block correctly restricts KMS key usage to only requests originating from within these VPCs?", options: ["(Invalid option)","(Invalid option)","(Invalid option)","(Invalid option)"], correct: 0, explanation: "There are two valid approaches to restrict access to VPC-only:  - **`aws:sourceVpce`** (Option A): Restricts to a specific VPC endpoint. Use this when you want to ensure requests come through a particular endpoint.  - **`aws:sourceVpc`** (Option B): Restricts to specific VPCs. Use this when you have multiple VPC endpoints across VPCs and want to allow any of them.  **Important:** These condition keys only have values when the request comes through a VPC endpoint. Requests from the public internet will have null values for these keys, causing the condition to evaluate to false (denying access).  Option C is incorrect because private IP addresses are not in `aws:sourceIP` for VPC endpoint requests. Option D restricts usage to a specific AWS service integration, not VPC access.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/vpce-policy-condition.html" },
            { id: 8, question: "A startup is designing their encryption strategy for AWS. They have three distinct use cases:  1. **Use Case A:** Encrypt S3 objects with minimal operational overhead; they don't need to manage keys or audit key usage. 2. **Use Case B:** Encrypt RDS databases with keys they can audit, rotate automatically, and use across multiple AWS services; they need CloudTrail logs of key usage. 3. **Use Case C:** Protect highly sensitive data with keys where they control the key material, can set expiration dates, and can delete the key material at any time to render data unrecoverable. Which key types should be used for each use case?", options: ["Use Case A: AWS owned keys, Use Case B: AWS managed keys, Use Case C: Customer managed keys with imported key material","Use Case A: AWS managed keys, Use Case B: Customer managed keys, Use Case C: AWS owned keys","Use Case A: SSE-S3, Use Case B: SSE-KMS with AWS managed key, Use Case C: Customer managed keys","Use Case A: Customer managed keys, Use Case B: AWS managed keys, Use Case C: AWS owned keys"], correct: 0, explanation: "| Key Type | Operational Overhead | Audit (CloudTrail) | Key Material Control | Automatic Rotation | |----------|---------------------|-------------------|---------------------|-------------------| | **AWS owned keys** | None | No | No | AWS managed | | **AWS managed keys** | Low | Yes | No | Annual (mandatory) | | **Customer managed keys** | Medium | Yes | Optional (imported) | Configurable |  - **Use Case A** (minimal overhead, no audit needed) ‚Üí AWS owned keys - **Use Case B** (audit needed, multi-service, automatic rotation) ‚Üí AWS managed keys provide this with low overhead - **Use Case C** (control key material, set expiration, delete at will) ‚Üí Customer managed keys with imported key material", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html" },
            { id: 9, question: "Your company's security policy requires that all encryption keys used in AWS must use key material generated by your on-premises HSM. You need to create a KMS key with imported key material for encrypting data in Amazon S3. What is the CORRECT sequence of steps to create a KMS key with imported key material?", options: ["1) Generate key material on your HSM, 2) Create a KMS key with origin AWS_KMS, 3) Import the key material, 4) Enable the key","1) Create a KMS key with origin EXTERNAL, 2) Download the wrapping public key and import token, 3) Encrypt the key material using the wrapping public key, 4) Import the encrypted key material with the import token","1) Create a KMS key with origin EXTERNAL, 2) Generate a CSR (Certificate Signing Request), 3) Have your HSM sign the CSR, 4) Upload the signed certificate to complete the import","1) Download the KMS wrapping key from AWS, 2) Create key material on your HSM, 3) Create a KMS key with origin EXTERNAL, 4) Import the wrapped key material"], correct: 1, explanation: "The correct sequence for importing key material is:  1. **Create a KMS key with no key material** - Origin must be `EXTERNAL` (this prevents AWS from generating key material) 2. **Download the wrapping public key and import token** - The wrapping key protects your key material in transit; the import token binds the import to your specific KMS key 3. **Encrypt your key material** - Use the downloaded wrapping public key to encrypt your key material generated externally 4. **Import the key material** - Upload the encrypted key material along with the import token  Option A is incorrect because origin must be EXTERNAL, not AWS_KMS. Option C describes certificate-based authentication, not key import. Option D has steps out of order (you must create the KMS key first to get the wrapping key specific to that key).", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-conceptual.html" },
            { id: 10, question: "You have created a KMS key with imported key material and set an expiration date. The key is used to encrypt objects in an S3 bucket. The expiration date has now passed. Which statements are TRUE about what happens when imported key material expires? (Select TWO)", options: ["The encrypted S3 objects are automatically re-encrypted with a new AWS-generated key.","The KMS key's key state changes to `PendingImport` and cannot be used for cryptographic operations.","The key material is automatically deleted by AWS KMS, but the KMS key metadata remains.","The S3 objects remain encrypted but become permanently inaccessible."], correct: 1, explanation: "When imported key material expires:  1. **AWS KMS deletes the key material** (but not the KMS key itself‚Äîthe key metadata, ARN, and policies remain) 2. **The key state changes to `PendingImport`** - the key cannot be used for any cryptographic operations 3. **Encrypted data remains encrypted** but cannot be decrypted until key material is reimported 4. **You must reimport the SAME key material** - different key material will not work because it won't match the ciphertext  Option A is incorrect because S3 doesn't automatically re-encrypt. Option D is incorrect because the data is not \"permanently\" inaccessible‚Äîit can be recovered by reimporting the original key material.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-conceptual.html" },
            { id: 11, question: "A company is building an API that requires message authentication to prevent tampering. They want to use AWS KMS to generate and verify HMAC tags for API requests without exposing the secret key to their application code. Which statements about HMAC KMS keys are TRUE? (Select THREE)", options: ["HMAC KMS keys can be used for both encryption and message authentication.","The HMAC key material never leaves AWS KMS unencrypted, and you generate and verify HMAC tags by calling KMS APIs.","HMAC KMS keys support automatic annual rotation like symmetric encryption keys.","You can import your own HMAC key material into a KMS key with origin set to EXTERNAL."], correct: 1, explanation: "About HMAC KMS keys: - **B is correct:** HMAC key material never leaves KMS unencrypted; you call `GenerateMac` and `VerifyMac` APIs - **D is correct:** You can import HMAC key material with origin EXTERNAL - **F is correct:** Supported key specs are HMAC_224, HMAC_256, HMAC_384, HMAC_512 (matching SHA-2 hash lengths)  Option A is incorrect‚ÄîHMAC keys are ONLY for message authentication, not encryption. Option C is incorrect‚ÄîHMAC keys do NOT support automatic rotation. Option E is incorrect‚ÄîHMAC is always symmetric (same key for generate and verify); asymmetric signing uses different key types.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/hmac.html" },
            { id: 12, question: "Your company uses a single-region KMS key with imported key material for encrypting sensitive data. Due to a compliance audit, you need to perform an immediate key rotation. The key currently encrypts data in DynamoDB and S3. How should you perform on-demand rotation for this KMS key?", options: ["Call the `RotateKeyOnDemand` API, which will automatically generate new key material and maintain the old material for decryption.","Generate new key material externally, download a new wrapping public key and import token, encrypt the new key material, and call `ImportKeyMaterial` with the new material.","Delete the imported key material using `DeleteImportedKeyMaterial`, then import new key material using the same import token you used originally.","Create a new KMS key with new imported key material, update your applications to use the new key, and re-encrypt all existing data with the new key."], correct: 1, explanation: "For a **single-region** KMS key with imported key material, on-demand rotation works as follows:  1. **Generate new key material** on your external system 2. **Download a NEW wrapping public key and import token** (you must get fresh ones) 3. **Encrypt the new key material** with the wrapping public key 4. **Call `ImportKeyMaterial`** with the new encrypted key material and import token  AWS KMS will add the new key material as a rotation, maintaining the old key material for decrypting existing ciphertext. The key ID and ARN remain the same.  Option A is incorrect because `RotateKeyOnDemand` generates AWS key material, which isn't allowed for imported key material keys. Option C is incorrect because you need a new import token (the old one expires). Option D is manual re-encryption which is unnecessary and complex.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-import-key-material.html" },
            { id: 13, question: "Your organization uses a multi-region KMS key with imported key material, with the primary key in `us-east-1` and replica keys in `eu-west-1` and `ap-southeast-1`. You need to rotate the key material across all regions. What is the CORRECT approach for rotating key material on multi-region keys with imported key material?", options: ["Rotate the key material on the primary key in `us-east-1`; the new key material will automatically replicate to all replica keys.","Import new key material to each regional key independently‚Äîprimary and all replicas require separate import operations with the same key material.","Delete all replica keys, rotate the primary key material, then recreate the replicas, which will inherit the new key material.","Multi-region keys with imported key material do not support rotation; you must create a new multi-region key set."], correct: 1, explanation: "For **multi-region** KMS keys with imported key material:  - Key material is **NOT automatically replicated** between primary and replica keys - You must import the **same key material** to each key (primary and all replicas) **independently** - Each import requires its own wrapping public key and import token from that specific regional key  This is different from multi-region keys with AWS-generated key material, where the key material is automatically synchronized.  **Important:** You must ensure you import the **exact same key material** to all related multi-region keys, or they won't be able to decrypt each other's ciphertext.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-import-key-material.html" },
            { id: 14, question: "Your security team at a healthcare company needs to implement different key rotation strategies for various KMS keys. They have three requirements: (1) a customer managed key that rotates every 180 days automatically, (2) an asymmetric signing key that needs rotation, and (3) a symmetric key that must be rotated immediately due to a security incident. Which combination of rotation approaches correctly addresses all three requirements?", options: ["Requirement 1: Enable automatic rotation with custom period; Requirement 2: Enable automatic rotation; Requirement 3: Call `RotateKeyOnDemand` API","Requirement 1: Enable automatic rotation with custom period; Requirement 2: Manual rotation by creating new key; Requirement 3: Call `RotateKeyOnDemand` API","Requirement 1: Schedule AWS Lambda to call `RotateKeyOnDemand` every 180 days; Requirement 2: Enable automatic rotation; Requirement 3: Manual rotation by creating new key","Requirement 1: Enable automatic rotation (annual only); Requirement 2: On-demand rotation; Requirement 3: Manual rotation with alias update"], correct: 1, explanation: "AWS KMS supports three rotation approaches with specific use cases:  **Automatic Rotation:** - Only available for symmetric encryption keys (not asymmetric, HMAC, or custom key store keys) - Can be customized from 90 days to 2560 days (7 years) - Requirement 1 (180 days) can use automatic rotation with custom period  **Manual Rotation:** - Required for asymmetric keys, HMAC keys, and keys in custom key stores - Create a new KMS key and update application aliases/references - Requirement 2 (asymmetric signing key) MUST use manual rotation since automatic rotation doesn't support asymmetric keys  **On-Demand Rotation:** - Available only for symmetric encryption keys - Triggers immediate rotation without affecting automatic rotation schedule - Requirement 3 (immediate rotation) can use on-demand rotation  Option A is incorrect because asymmetric keys don't support automatic rotation. Option C is incorrect because Lambda scheduling is unnecessary (automatic rotation with custom period is native). Option D is incorrect because automatic rotation is no longer limited to annual‚Äîyou can customize the period.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html" },
            { id: 15, question: "A global financial services company uses AWS KMS multi-region keys with imported key material for encrypting customer data. The primary key is in `us-east-1` with replicas in `eu-west-1` and `ap-southeast-1`. They need to rotate the key material across all regions while maintaining cross-region decryption capabilities. What is the correct approach for rotating key material on these multi-region keys?", options: ["Rotate the key material on the primary key in `us-east-1` using `ImportKeyMaterial`; AWS KMS automatically synchronizes the new key material to all replica keys within 24 hours.","Download wrapping keys and import tokens for each regional key separately, then import the same key material to the primary and each replica independently using separate import operations.","Use the `SynchronizeMultiRegionKey` API after importing new key material to the primary key to force replication of the key material to all replicas.","Delete all replica keys, import new key material to the primary key, then recreate replicas which will automatically inherit the new key material from the primary."], correct: 1, explanation: "For multi-region KMS keys with **external (imported) key material**, key material is **NOT automatically replicated** between the primary and replica keys. This is a critical distinction from multi-region keys with AWS-generated key material.  The correct process is: 1. Generate new key material externally (on-premises HSM or other secure system) 2. For each key (primary and all replicas):    - Download a unique wrapping public key and import token specific to that regional key    - Encrypt the **same** key material with that key's wrapping key    - Import the encrypted key material using `ImportKeyMaterial`  **Critical requirement:** You must import the **exact same key material** to all related multi-region keys, or they won't be able to decrypt each other's ciphertext (breaking the multi-region functionality).  Option A is incorrect because key material does not auto-replicate for external origin keys. Option C is incorrect because `SynchronizeMultiRegionKey` synchronizes metadata (policies, tags), not key material. Option D is unnecessary and causes service disruption.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-import.html" },
            { id: 16, question: "Your company's primary AWS region is `us-east-1`, but due to data residency regulations, you need to change the primary region for a multi-region KMS key to `eu-central-1`. The key currently has the primary in `us-east-1` and replicas in `eu-central-1`, `eu-west-1`, and `ap-southeast-1`. Which THREE statements are TRUE about changing the primary region? (Select THREE)", options: ["The `UpdatePrimaryRegion` operation changes the key ARN of the KMS key to reflect the new primary region.","Both the current primary key and the replica key being promoted must be in the Enabled state when the operation starts.","After the primary region change, the former primary key in `us-east-1` automatically becomes a replica key.","You must update applications to use the new key ID after changing the primary region."], correct: 1, explanation: "When you change the primary region using `UpdatePrimaryRegion`:  **What changes:** - The regional key in the new primary region becomes the primary key (C is correct) - The former primary key becomes a replica key (C is correct) - You may want to update the key policy to grant `kms:ReplicateKey` to manage replicas (F is correct)  **What does NOT change:** - The key ARN remains the same (A is incorrect) - The key ID, key material, key spec, and key usage remain unchanged (E is correct) - Applications do not need key ID updates (D is incorrect)  **Prerequisites:** - Both keys must be in Enabled state (B is correct) - The operation is asynchronous and may take time to complete  The primary key serves as the source for shared properties, so it's important for managing replication permissions and policies.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-update.html" },
            { id: 17, question: "A Security Engineer discovers that a KMS key used by multiple applications was accidentally scheduled for deletion with the minimum 7-day waiting period. The key encrypts data in DynamoDB, S3, and EBS volumes. It's now day 5 of the waiting period. Which THREE statements are TRUE about this situation? (Select THREE)", options: ["The key can still be used for decryption operations during the waiting period, but not encryption operations.","AWS KMS will not automatically rotate the key material while it's in the \"Pending deletion\" state.","The actual deletion may occur up to 24 hours after the scheduled deletion date.","If the key is an asymmetric key, users can still use the public key outside AWS to encrypt data that will become permanently undecryptable after deletion."], correct: 1, explanation: "When a KMS key is pending deletion:  **True statements:** - **B:** AWS KMS does not rotate key material during pending deletion - **C:** The actual deletion may occur up to 24 hours later than the scheduled date (verify with `DescribeKey`) - **D:** For asymmetric keys, this is a critical risk‚Äîthe public key can be used externally to encrypt data that will be permanently undecryptable after the private key is deleted  **False statements:** - **A:** The key cannot be used for ANY cryptographic operations (encryption or decryption) while pending deletion - **E:** `CancelKeyDeletion` restores the key to **Disabled** state, not Enabled‚Äîyou must manually re-enable it - **F:** Data is NOT automatically re-encrypted; encrypted data becomes permanently undecryptable if the key is deleted  **Best practice:** Create CloudWatch alarms to detect attempts to use keys pending deletion, and have a process to cancel deletion and identify dependencies before the waiting period expires.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html" },
            { id: 18, question: "Your security team wants to detect when applications attempt to use a KMS key that is pending deletion. You need to create a CloudWatch alarm that triggers when such attempts occur, sending notifications via SNS. Which combination of AWS services and configurations is required? (Select THREE)", options: ["Enable AWS CloudTrail to log KMS API calls to CloudWatch Logs","Create a CloudWatch Logs metric filter with pattern: `{ $.errorMessage = \"*Key ARN is pending deletion*\" }`","Create a CloudWatch alarm on the custom metric from the metric filter that triggers when the metric >= 1","Configure GuardDuty to monitor for `Policy:KMS/KeyPendingDeletion` findings"], correct: 0, explanation: "To detect attempts to **use** (not schedule deletion of) a key pending deletion, you need:  1. **CloudTrail + CloudWatch Logs integration (A):** CloudTrail captures KMS API calls; sending them to CloudWatch Logs enables real-time analysis  2. **Metric filter (B):** Create a filter pattern that matches the error message \"Key ARN is pending deletion\" which appears when applications try to use the key  3. **CloudWatch alarm (C):** Create an alarm on the custom metric that triggers when count >= 1, sending SNS notifications  **Why other options are incorrect:** - **D:** GuardDuty doesn't have a finding type for key pending deletion attempts - **E:** AWS Config rules detect configuration changes, not runtime API call errors - **F:** EventBridge on `ScheduleKeyDeletion` detects when deletion is **scheduled**, not when applications attempt to **use** the key afterward  **Important:** The alarm will NOT trigger for operations that are still permitted on keys pending deletion: `ListKeys`, `CancelKeyDeletion`, `PutKeyPolicy`, `DescribeKey`.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-creating-cloudwatch-alarm.html" },
            { id: 19, question: "A healthcare company is building a secure messaging application where two parties need to establish a shared secret for symmetric encryption without transmitting the secret over the network. Both parties have asymmetric KMS keys of type `ECC_NIST_P256` with key usage `KEY_AGREEMENT`. How should they use AWS KMS to derive a shared secret?", options: ["Party A calls `DeriveSharedSecret` with their private key and Party B's public key; Party B calls `DeriveSharedSecret` with their private key and Party A's public key; both operations produce the same shared secret.","Party A encrypts a random value with Party B's public key using `Encrypt`; Party B decrypts it with their private key using `Decrypt`; the decrypted value is the shared secret.","Both parties call `GenerateDataKey` with the same KMS key and encryption context to derive an identical shared secret.","Party A calls `GetPublicKey` on Party B's KMS key, then uses the Diffie-Hellman algorithm locally to compute the shared secret outside of AWS KMS."], correct: 0, explanation: "`DeriveSharedSecret` implements the Elliptic Curve Diffie-Hellman (ECDH) key agreement protocol:  **How it works:** 1. Party A calls `DeriveSharedSecret` specifying:    - Their own KMS key (private key stays in KMS)    - Party B's KMS key ID (KMS retrieves the public key)    - Returns a shared secret     2. Party B calls `DeriveSharedSecret` specifying:    - Their own KMS key (private key stays in KMS)    - Party A's KMS key ID (KMS retrieves the public key)    - Returns the **same** shared secret  **Key properties:** - The private keys never leave AWS KMS - Both parties derive the same shared secret independently - The shared secret can be used as a symmetric encryption key - Supported key specs: `ECC_NIST_P256`, `ECC_NIST_P384`, `ECC_NIST_P521`, `ECC_SECG_P256K1`  Option B uses RSA encryption, not key agreement. Option C doesn't produce matching secrets for different parties. Option D defeats the purpose of KMS (key material leaves the service).", docLink: "https://docs.aws.amazon.com/kms/latest/APIReference/API_DeriveSharedSecret.html" },
            { id: 20, question: "Your company has an organization trail in CloudTrail that logs all API activity. Due to the high volume of KMS API calls (millions per day from S3 and EBS encryption), you're incurring significant CloudTrail storage costs. Management wants to reduce costs by excluding KMS events while maintaining audit capability for all other services. What should you do?", options: ["Enable the `ExcludeManagementEventSources` parameter in the trail configuration with value `kms.amazonaws.com` to exclude all KMS events.","Create an S3 lifecycle policy on the CloudTrail bucket to delete logs with `eventSource: \"kms.amazonaws.com\"` immediately after delivery.","Disable CloudTrail for the entire organization and enable individual trails per account excluding KMS, then aggregate logs manually.","Use AWS Config to track KMS key configuration changes instead of CloudTrail, then disable KMS logging in CloudTrail."], correct: 0, explanation: "To exclude AWS KMS events from a CloudTrail trail, use the `ExcludeManagementEventSources` parameter:  **Configuration:** ```json {   \"ExcludeManagementEventSources\": [\"kms.amazonaws.com\"] } ```  This excludes all AWS KMS events from the trail, significantly reducing volume and costs for KMS-heavy workloads (S3 encryption, EBS encryption, etc. generate many KMS API calls).  **Important considerations:** - This setting is applied at the trail level - You lose audit visibility into KMS API calls (who accessed which keys, when) - For compliance, you may need to keep KMS events despite costs - You can still use CloudWatch metrics to monitor KMS without full CloudTrail logs  Option B is incorrect because S3 lifecycle policies can't filter by JSON content (and you'd lose non-KMS events in the same log files). Option C creates operational complexity. Option D is incorrect because AWS Config tracks configuration, not API activity.", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/logging-using-cloudtrail.html" },
            { id: 21, question: "A Security Engineer needs to configure alerts for various KMS key lifecycle events. They want to receive notifications when specific events occur. Which THREE events can be detected using Amazon CloudWatch? (Select THREE)", options: ["The imported key material in a KMS key is nearing its expiration date","A KMS key that is pending deletion is still being used","The key material in a KMS key was automatically rotated","A KMS key was deleted"], correct: 0, explanation: "CloudWatch can detect KMS events through two mechanisms: **CloudWatch Events (EventBridge)** and **CloudWatch Metrics**.  **Events that CloudWatch CAN detect:**  **A - Imported key material expiration (CloudWatch Metric):** - AWS KMS publishes the `SecondsUntilKeyMaterialExpiration` metric - You can create an alarm when this metric falls below a threshold (e.g., 7 days)  **B - Key pending deletion is being used (CloudWatch Logs + Metric Filter):** - Create a metric filter on CloudTrail logs for error message \"Key ARN is pending deletion\" - Set an alarm on the custom metric  **C - Automatic key rotation (CloudWatch Event):** - EventBridge receives `KMS RotateKey` events - Create a rule to trigger SNS notification  **Events that CloudWatch CANNOT directly detect:**  **D - Key deleted:** No specific CloudWatch metric or event for deletion completion (you can detect scheduling via CloudTrail)  **E - Key policy modified:** Detected through CloudTrail, not CloudWatch metrics  **F - Request quota exceeded:** KMS uses service quotas, not CloudWatch metrics", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/monitoring-cloudwatch.html" },
            { id: 22, question: "Your company wants to automate the deployment of AWS Control Tower landing zones using infrastructure-as-code. A DevOps engineer is planning to use the AWS Control Tower API to create a landing zone programmatically. What are the correct prerequisites and expectations for using the Control Tower API to create a landing zone? (Select THREE)", options: ["The AWS account must already be the management account of an AWS Organization with at least one organizational unit created.","You must pre-create an Amazon S3 bucket in the management account for Control Tower to store baseline resources.","The API creates the Log Archive and Audit accounts automatically as part of landing zone creation.","You must manually create IAM roles with specific trust policies before calling the API to create the landing zone."], correct: 0, explanation: "When using the Control Tower API to create a landing zone programmatically:  **Required prerequisites:** - **A:** Account must be the AWS Organizations management account with at least one OU - **E:** Organizations must have \"All features\" enabled (SCPs, tag policies, etc.) - **F:** You must provide email addresses for the Log Archive and Audit accounts in the API call  **What Control Tower handles:** - **C is correct in practice:** Control Tower creates the Log Archive and Audit accounts - **B is incorrect:** Control Tower creates the necessary S3 buckets automatically - **D is incorrect:** Control Tower creates the required IAM roles automatically  The API approach allows automation but requires careful planning for: - Email addresses (must be unique and accessible) - OU structure design - Guardrail selection - VPC CIDR ranges  Unlike the console wizard which guides you through each step, the API requires you to specify all parameters upfront.", docLink: "https://docs.aws.amazon.com/controltower/latest/userguide/getting-started-expectations-api.html" },
            { id: 23, question: "A financial services company subject to strict data privacy regulations wants to ensure that customer data stored in AWS is never used to improve AI/ML services. They're using AWS Control Tower to manage their multi-account environment. How can they opt out of having their data used by AI services across all accounts in their organization?", options: ["Enable the \"AI services opt-out\" guardrail in Control Tower, which applies an SCP that denies AI service data collection across all member accounts.","Configure the AI services opt-out policy in AWS Organizations, then use Control Tower to deploy the policy to all OUs.","Create an SCP that denies the `ai-opt-out:PutOptOutPolicy` action and attach it to the root of the organization.","Manually opt out of each AI service (Rekognition, Comprehend, Lex, etc.) in each account through the service consoles."], correct: 1, explanation: "AWS provides centralized control for AI services data usage through **AI services opt-out policies** in AWS Organizations:  **Correct approach:** 1. Create an AI services opt-out policy in AWS Organizations (management account) 2. Attach the policy to OUs or individual accounts 3. Control Tower can help manage and deploy these policies across your organization  **How it works:** - When opted out, your content processed by AI services (Rekognition, Transcribe, Comprehend, Lex, Polly, Translate, Textract) is not used to develop or improve Amazon or third-party services - The policy applies to all accounts in the attached OU/organization - This is different from SCPs‚Äîit's a specialized policy type for AI services  **Why other options are incorrect:** - **A:** There's no such \"AI services opt-out guardrail\"‚Äîthis is managed through Organizations policies - **C:** This denies the ability to manage opt-out policies, not the data collection itself - **D:** Manual opt-out is inefficient and doesn't scale; Organizations policies provide centralized control", docLink: "https://docs.aws.amazon.com/controltower/latest/userguide/ai-opt-out.html" },
            { id: 24, question: "AWS Control Tower creates a VPC automatically in each new member account. A network architect needs to understand the default VPC configuration to plan IP addressing and connectivity. Which THREE statements accurately describe the VPC created by Control Tower? (Select THREE)", options: ["The VPC uses a /16 CIDR block by default, providing 65,536 IP addresses per account.","Control Tower creates public subnets only; you must manually create private subnets for workloads that don't need internet access.","The VPC includes one public subnet and one private subnet in each Availability Zone (typically 3 AZs, so 6 subnets total).","Control Tower automatically attaches an Internet Gateway and creates route tables for public subnet internet access."], correct: 0, explanation: "Control Tower's automatic VPC configuration includes:  **Default configuration (A, C, D are correct):** - **A:** Uses a /16 CIDR block (default: 172.31.0.0/16) - **C:** Creates **both** public and private subnets in each AZ (typically 3 AZs = 6 subnets) - **D:** Includes an Internet Gateway and appropriate route tables for public subnets  **What Control Tower does NOT configure:** - **E is incorrect:** VPC Flow Logs are not enabled by default (you can enable via guardrails) - **F is incorrect:** NAT Gateways are not created by default (you must add them manually or via customizations)  **Key considerations:** - The CIDR block can be customized during account provisioning - For production workloads, you may want to:   - Create NAT Gateways for private subnet internet access   - Enable VPC Flow Logs for security monitoring   - Adjust CIDR blocks to avoid conflicts with on-premises networks   - Use AWS Control Tower Customizations (CfCT) to automate additional configurations", docLink: "https://docs.aws.amazon.com/controltower/latest/userguide/vpc-concepts.html" },
            { id: 25, question: "A Security Analyst discovers that a user's IAM Identity Center credentials were compromised. The user has active sessions across multiple AWS accounts via permission sets. You need to immediately revoke all active sessions for this user. What is the CORRECT sequence of steps to revoke active IAM role sessions created by IAM Identity Center permission sets?", options: ["1) Delete the user from IAM Identity Center; 2) Wait for sessions to expire naturally (up to 12 hours)","1) Disable the user in IAM Identity Center; 2) For each affected account, attach an IAM policy with `\"Condition\": {\"DateLessThan\": {\"aws:TokenIssueTime\": \"<current-timestamp>\"}}` to revoke sessions issued before now","1) Remove the user from all permission set assignments in IAM Identity Center; 2) Call `DeleteAccessKey` API for each active session","1) Update the trust policy of the IAM roles in each account to add a `\"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"revoked\"}}` condition; 2) Force the user to log out"], correct: 1, explanation: "To revoke active IAM role sessions created by IAM Identity Center permission sets:  **Step 1: Disable the user in IAM Identity Center** - Prevents the user from authenticating and getting new sessions - Does NOT terminate existing active sessions  **Step 2: Revoke existing sessions using token issue time** - For each account where the user has active sessions, attach an IAM policy to the role: ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Effect\": \"Deny\",     \"Action\": \"*\",     \"Resource\": \"*\",     \"Condition\": {       \"DateLessThan\": {         \"aws:TokenIssueTime\": \"2026-02-09T10:00:00Z\"       }     }   }] } ```  This denies all actions for sessions issued before the specified timestamp (current time), effectively revoking all active sessions.  **Why other options are incorrect:** - **A:** Deleting doesn't revoke active sessions; waiting up to 12 hours is too slow - **C:** Permission sets don't create access keys; `DeleteAccessKey` is for IAM users - **D:** Changing trust policies doesn't affect existing sessions; ExternalId is for cross-account roles  **Alternative method:** You can also use the AWS CLI: `aws iam put-role-policy` to attach the deny policy.", docLink: "https://docs.aws.amazon.com/singlesignon/latest/userguide/revoke-user-permissions.html" },
            { id: 26, question: "A company's security policy requires that all EBS volumes must be encrypted with customer managed KMS keys (not AWS managed keys). An application team reports that their EC2 instances with encrypted volumes are experiencing performance issues during key rotation. Which THREE statements are TRUE about EBS encryption with KMS keys? (Select THREE)", options: ["When you enable automatic key rotation for a customer managed key used by EBS, all existing encrypted volumes are automatically re-encrypted with the new key material.","EBS encryption supports only symmetric encryption KMS keys; asymmetric keys cannot be used for EBS volume encryption.","After key rotation, new EBS volumes use the current key material, while existing volumes continue using their original key material for decryption.","You must grant the AWS service principal `ebs.amazonaws.com` permission to use your customer managed key through the key policy."], correct: 1, explanation: "**EBS encryption with KMS keys:**  **B is correct:** EBS only supports **symmetric encryption keys**. Asymmetric KMS keys cannot be used for EBS volume encryption because the encryption/decryption operations require the same key.  **C is correct:** When automatic key rotation occurs: - AWS KMS retains all previous key versions - New EBS volumes and snapshots use the current (rotated) key material - Existing volumes continue using their original key material for decryption - **No re-encryption is needed**‚Äîthis is transparent to applications  **E is correct:** To change the KMS key used by an EBS snapshot, you must: - Copy the snapshot - Specify a different KMS key during the copy operation - The original snapshot remains encrypted with the original key  **Incorrect statements:** - **A:** Volumes are NOT automatically re-encrypted after rotation‚ÄîAWS KMS handles this transparently - **D:** You grant permission to the EC2 service, not specifically `ebs.amazonaws.com` - **F:** The AWS managed key `aws/ebs` rotates **annually** (every 1 year), not every 3 years", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/services-ebs.html" },
            { id: 27, question: "A data analytics team wants to encrypt data at rest and in transit for their Amazon EMR cluster processing sensitive healthcare data. They need to understand how EMR integrates with AWS KMS for encryption. Which statements about EMR encryption with KMS are TRUE? (Select TWO)", options: ["EMR supports encrypting EMRFS data in S3 using KMS, but local disk encryption requires LUKS encryption configured at the OS level.","When using KMS for EMR encryption, each EMR cluster node retrieves the data encryption key directly from KMS for every read/write operation.","EMR supports both SSE-KMS (server-side encryption) and CSE-KMS (client-side encryption) for S3 data, with CSE-KMS providing encryption before data leaves the EMR cluster.","You can use the same KMS key for encrypting both local disk volumes and EMRFS data in S3."], correct: 2, explanation: "**EMR encryption with KMS:**  **C is correct:** EMR supports two encryption modes for S3 data (EMRFS): - **SSE-KMS:** Server-side encryption where S3 encrypts data using KMS - **CSE-KMS:** Client-side encryption where EMR encrypts data before sending to S3, providing stronger security  **D is correct:** You can use the same customer managed KMS key for: - Local disk encryption (EBS volumes attached to cluster nodes) - EMRFS data encryption in S3 - This simplifies key management  **Incorrect statements:** - **A:** EMR natively supports KMS for **both** EMRFS and local disk encryption‚ÄîLUKS is not required - **B:** EMR uses envelope encryption with data keys, not direct KMS calls for every operation (which would be too slow) - **E:** While you need proper IAM permissions, EMR can use service roles that AWS creates automatically  **Additional considerations:** - EMR also supports encryption in transit using TLS - You can enable encryption at-rest at the security configuration level - Different encryption keys can be used for different encryption targets", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/services-emr.html" },
            { id: 28, question: "A financial services company uses Amazon Redshift to analyze transaction data. They want to migrate from an unencrypted Redshift cluster to one encrypted with a customer managed KMS key. The cluster contains 10 TB of data and must remain available during migration. What is the correct approach to migrate to an encrypted Redshift cluster?", options: ["Enable encryption on the existing cluster using the AWS CLI `modify-cluster` command with `--encrypted` and `--kms-key-id` parameters; Redshift encrypts data in place over 24-48 hours.","Create a manual snapshot of the unencrypted cluster, copy the snapshot with encryption enabled specifying the KMS key, restore a new cluster from the encrypted snapshot, then redirect applications to the new cluster.","Use AWS Database Migration Service (DMS) to replicate data from the unencrypted cluster to a new encrypted cluster with zero downtime.","Enable encryption at the database level using Redshift's `ALTER DATABASE ENCRYPT` command, which encrypts data table-by-table during maintenance windows."], correct: 1, explanation: "**Migrating to an encrypted Redshift cluster:**  Amazon Redshift **does not support** enabling encryption on an existing unencrypted cluster. The correct migration path is:  1. **Create a manual snapshot** of the unencrypted cluster 2. **Copy the snapshot** and enable encryption during the copy operation, specifying your customer managed KMS key 3. **Restore a new cluster** from the encrypted snapshot 4. **Update application connection strings** to point to the new encrypted cluster 5. **Decommission the old unencrypted cluster** after validation  **Why other options are incorrect:** - **A:** You cannot modify an existing cluster to add encryption‚Äîencryption must be set at cluster creation time - **C:** While DMS can be used for some migrations, the standard approach for encryption is snapshot copy and restore - **D:** Redshift doesn't have an `ALTER DATABASE ENCRYPT` command‚Äîencryption is cluster-wide, not per-database  **Important considerations:** - Plan for downtime (during restore) or use read replicas in some architectures - The KMS key must be in the same region as the new cluster - You need `kms:CreateGrant` permission for Redshift to use the KMS key - Encrypted snapshots can only be restored to encrypted clusters", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/services-redshift.html" },
            { id: 29, question: "Your organization wants to implement Attribute-Based Access Control (ABAC) for KMS keys. Development teams should only be able to use KMS keys that match their team tag. Team Alpha has users tagged with `Team=Alpha` and keys tagged with `Project=Alpha-Backend`. Which IAM policy correctly implements ABAC to allow users to use only their team's KMS keys?", options: ["(Invalid option)","(Invalid option)","(Invalid option)","(Invalid option)"], correct: 0, explanation: "**ABAC for KMS requires:**  1. **Tag the principals** (users/roles) with attributes (e.g., `Team=Alpha`) 2. **Tag the KMS keys** with corresponding attributes (e.g., `Team=Alpha` or in this case the policy would need `Team` tags on keys) 3. **Create a policy** that compares principal tags to resource tags  **Option A is correct** (assuming keys are tagged with `Team`):** - Uses `kms:ResourceTag/Team` to check the key's tag - Compares it to `${aws:PrincipalTag/Team}` (the user's Team tag value) - Allows access only when tags match  **Why other options are incorrect:** - **B:** Uses `aws:ResourceTag` instead of `kms:ResourceTag` (wrong namespace), and tries to match `Project` tag on resource to `Team` tag on principal (mismatch) - **C:** Uses `aws:RequestTag` which checks tags in the API request, not tags on the existing resource - **D:** Hardcodes specific team names instead of using dynamic ABAC with principal tags  **Note:** For the scenario as stated (users tagged `Team=Alpha`, keys tagged `Project=Alpha-Backend`), you'd need to either: - Retag keys with `Team=Alpha`, or - Use a policy that maps `Project=Alpha-Backend` to `Team=Alpha`", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/abac.html" },
            { id: 30, question: "A DevOps team has the ability to create IAM roles for their applications. The security team wants to ensure that the DevOps team cannot create roles with more permissions than their own role has. The DevOps role has `AdministratorAccess` within their account but should not be able to escalate privileges. How should you implement this restriction using IAM permissions boundaries?", options: ["Attach a permissions boundary to the DevOps role that denies `iam:CreateRole` and `iam:PutRolePolicy` actions.","Create a permissions boundary policy that defines maximum allowed permissions, then require the DevOps team to attach this boundary to all roles they create by denying `iam:CreateRole` unless `iam:PermissionsBoundary` is set.","Use an SCP to prevent the DevOps team from creating roles with the `AdministratorAccess` policy attached.","Create a resource-based policy on IAM roles that prevents the DevOps team from assuming roles with elevated permissions."], correct: 1, explanation: "**Permissions boundaries** are an advanced feature for delegating permissions management:  **How it works:** 1. **Create a permissions boundary policy** that defines the maximum permissions (e.g., no sensitive actions like `iam:Delete*`, `organizations:*`) 2. **Require its use** through the DevOps team's IAM policy: ```json {   \"Effect\": \"Allow\",   \"Action\": [\"iam:CreateRole\", \"iam:PutRolePolicy\"],   \"Resource\": \"*\",   \"Condition\": {     \"StringEquals\": {       \"iam:PermissionsBoundary\": \"arn:aws:iam::ACCOUNT:policy/DevOpsTeamBoundary\"     }   } } ```  **What this achieves:** - DevOps can create roles, but MUST attach the boundary - The boundary limits the maximum permissions those roles can have - Even if they attach `AdministratorAccess` policy, the boundary restricts actual permissions - DevOps cannot remove the boundary (they'd need separate permission)  **Why other options are incorrect:** - **A:** Denying `iam:CreateRole` prevents role creation entirely, defeating the purpose - **C:** SCPs affect all principals in the account, not just the DevOps team - **D:** IAM roles don't support resource-based policies (only trust policies)  **Key concept:** Permissions boundaries define the **maximum** permissions an entity can have. The effective permissions are the intersection of the identity policy AND the boundary.", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html" },
            { id: 31, question: "Your company uses IAM Identity Center (AWS SSO) for federated access. Users from the Finance team should be automatically redirected to the EC2 console in the `us-west-2` region when they sign in, rather than the default AWS console home page. How can you configure the relay state to redirect Finance team users to the EC2 console in `us-west-2`?", options: ["Set the relay state in the permission set to: `https://console.aws.amazon.com/ec2/v2/home?region=us-west-2`","Configure an AWS Lambda function that intercepts the SAML response and modifies the redirect URL based on user attributes.","Set the relay state in the permission set to: `https://us-west-2.console.aws.amazon.com/ec2`","Add a SAML attribute mapping in the identity provider configuration with `RelayState=ec2` and `Region=us-west-2`."], correct: 0, explanation: "**IAM Identity Center Relay State** allows you to specify a URL where users are redirected after successful authentication.  **Correct format:** ``` https://console.aws.amazon.com/[service]/[path]?region=[region-code] ```  **For EC2 console in us-west-2:** ``` https://console.aws.amazon.com/ec2/v2/home?region=us-west-2 ```  **How to configure:** 1. In IAM Identity Center, edit the permission set 2. Set the **Session duration** and **Relay state** fields 3. Enter the full console URL with region parameter  **Why other options are incorrect:** - **B:** Relay state is a built-in IAM Identity Center feature‚Äîno Lambda needed - **C:** Incorrect URL format; AWS console URLs use the region as a query parameter, not a subdomain - **D:** Relay state is a URL string, not SAML attribute mappings  **Common use cases:** - Deep linking to specific service consoles - Directing different teams to different starting points - Skipping the AWS console home page  **Note:** Users can still navigate to other services after the redirect.", docLink: "https://docs.aws.amazon.com/singlesignon/latest/userguide/howtopermrelaystate.html" },
            { id: 32, question: "Your organization uses IAM Identity Center with an external IdP (Okta). You want to implement ABAC so users can only access S3 buckets tagged with their department. Users have a `Department` attribute in Okta (e.g., `Department=Finance`). What is the correct implementation sequence?", options: ["1) Configure attribute mappings in IAM Identity Center to map Okta's `Department` to session tag `Department`; 2) Create permission sets with IAM policies using `${aws:PrincipalTag/Department}` variable; 3) Apply permission sets to users","1) Create IAM users for each Okta user with Department tags; 2) Configure IAM Identity Center to assume these IAM users; 3) Use tag-based policies","1) Export user attributes from Okta to DynamoDB; 2) Create Lambda function to inject tags during authentication; 3) Use CloudFormation to deploy user-specific IAM policies","1) Configure permission sets with inline policies containing department names; 2) Assign permission sets to users based on Okta groups; 3) Users inherit department from group membership"], correct: 0, explanation: "**ABAC in IAM Identity Center requires three components:**  **1. Attribute Mapping (IdP ‚Üí Session Tags):** Configure IAM Identity Center to map attributes from your IdP to session tags: ``` Okta attribute: Department ‚Üí Session tag: Department ```  **2. Permission Set with ABAC Policy:** Create a permission set with a policy that uses principal tags: ```json {   \"Effect\": \"Allow\",   \"Action\": \"s3:*\",   \"Resource\": \"arn:aws:s3:::*\",   \"Condition\": {     \"StringEquals\": {       \"s3:ExistingObjectTag/Department\": \"${aws:PrincipalTag/Department}\"     }   } } ```  **3. Assign Permission Sets:** Assign the permission set to users/groups. When users authenticate: - IdP sends `Department` attribute - IAM Identity Center maps it to a session tag - The tag is available as `${aws:PrincipalTag/Department}` in policies  **Why other options are incorrect:** - **B:** IAM Identity Center doesn't \"assume\" IAM users‚Äîit creates temporary credentials via roles - **C:** Overly complex; attribute mapping is built-in - **D:** This is traditional group-based access control (GBAC), not ABAC  **Benefits of ABAC:** - No need to update policies when users change departments - Single permission set works for all users - Scales better than creating user-specific policies", docLink: "https://docs.aws.amazon.com/singlesignon/latest/userguide/configure-abac-policies.html" },
            { id: 33, question: "Your company uses Amazon Bedrock with a resource-based policy that allows the Bedrock service to access an S3 bucket. A security review identified a potential confused deputy vulnerability where an attacker in a different account could potentially trick Bedrock into accessing your S3 bucket. Which condition keys should you use to prevent the confused deputy problem, and what's the difference?", options: ["Use `aws:SourceAccount` to restrict access to Bedrock resources in your account; this prevents any cross-account confused deputy attacks.","Use `aws:SourceArn` to restrict to specific Bedrock resources; use `aws:SourceAccount` as a backup when you don't know the full ARN (e.g., for resources not yet created).","Use `aws:PrincipalArn` to verify that only your account's Bedrock service principal can access the S3 bucket.","Use both `aws:SourceArn` AND `aws:SourceAccount` together with an AND condition to provide maximum protection."], correct: 1, explanation: "**Confused Deputy Prevention:**  When AWS services access resources on your behalf, you should prevent attackers from tricking the service into acting as a confused deputy.  **`aws:SourceArn` (Preferred):** - Restricts access to a **specific resource ARN** - Most secure option when you know the full ARN - Example: `arn:aws:bedrock:us-east-1:123456789012:agent/AGENTID`  ```json \"Condition\": {   \"ArnEquals\": {     \"aws:SourceArn\": \"arn:aws:bedrock:us-east-1:123456789012:agent/*\"   } } ```  **`aws:SourceAccount` (Broader):** - Restricts access to resources in your **account** - Use when you don't know specific ARNs (e.g., future resources) - Less secure than SourceArn but better than nothing  **When to use each:** - **Known ARN:** Use `aws:SourceArn` (most secure) - **Unknown ARN:** Use `aws:SourceAccount` (broader but still protective) - **Don't use both with AND:** This would be overly restrictive  **Why other options are incorrect:** - **A:** SourceAccount alone is less secure than SourceArn when you know the ARN - **C:** `aws:PrincipalArn` checks the calling identity, not the source resource - **D:** Using both with AND is unnecessarily restrictive and not recommended", docLink: "https://docs.aws.amazon.com/bedrock/latest/userguide/cross-service-confused-deputy-prevention.html" },
            { id: 34, question: "A security engineer is configuring AWS Security Lake to collect logs from multiple AWS services. They want to ensure they're not creating duplicate logging configurations. Which statement is TRUE about collecting data from AWS services in Security Lake?", options: ["You must first enable CloudTrail logging, VPC Flow Logs, and Route 53 query logs in each source account before Security Lake can collect them.","Security Lake automatically collects data from supported AWS services through an independent stream of events; you do not need to separately configure logging in these services.","Security Lake creates AWS Config rules in each member account to collect configuration data and transforms it into OCSF format.","You must create custom Lambda functions to extract logs from CloudWatch Logs and send them to Security Lake's S3 bucket."], correct: 1, explanation: "**Security Lake Internal Sources:**  AWS Security Lake can natively collect logs from these AWS services: - AWS CloudTrail (management and data events) - Amazon VPC Flow Logs - Amazon Route 53 Resolver query logs - AWS Security Hub findings - Amazon EKS audit logs - AWS WAF logs  **Key point:** Security Lake pulls data through an **independent stream of events**‚Äîyou do **NOT** need to separately configure logging in these source services.  **How it works:** 1. Enable Security Lake 2. Select which AWS services to use as sources 3. Security Lake automatically:    - Collects the data    - Converts it to OCSF (Open Cybersecurity Schema Framework)    - Stores it in Parquet format in your Security Lake S3 bucket  **Why other options are incorrect:** - **A:** You do NOT need pre-existing logging configurations‚ÄîSecurity Lake collects directly - **C:** Security Lake doesn't use AWS Config rules for log collection - **D:** No custom Lambda functions needed for supported AWS services  **Benefits:** - Simplified configuration (no duplicate logging setup) - Centralized security data lake - Standardized format (OCSF) across all sources - Ready for analytics and security tools", docLink: "https://docs.aws.amazon.com/security-lake/latest/userguide/internal-sources.html" },
            { id: 35, question: "Your company's security team needs to completely disable AWS Security Lake across your organization. The Security Lake delegated administrator is in a dedicated security account, and Security Lake is collecting data from 50 member accounts across multiple regions. What is the correct procedure to fully disable Security Lake?", options: ["In the Security Lake delegated administrator account, use the console to disable Security Lake for all regions and member accounts; this automatically removes the delegated administrator designation.","In the management account, use the AWS CLI command `aws securitylake deregister-data-lake-delegated-administrator` to remove the delegated administrator, then disable Security Lake in each member account individually.","Use AWS Organizations to remove the Security Lake service principal from trusted services, which cascades the disable operation to all member accounts.","In the delegated administrator account, delete all Security Lake S3 buckets; this automatically disables Security Lake across the organization."], correct: 1, explanation: "**To disable Security Lake in an organization:**  The correct procedure is:  **1. Use the management account:** You must run this command from the **AWS Organizations management account**, not the delegated administrator account:  ```bash aws securitylake deregister-data-lake-delegated-administrator \\   --account-id <delegated-admin-account-id> ```  **2. Then disable Security Lake:** After removing the delegated administrator, you should disable Security Lake in member accounts  **Why this approach:** - The delegated administrator designation is managed at the organization level - Only the management account can remove delegated administrators - Removing the delegated administrator is the first critical step - The console approach in the delegated admin account doesn't fully clean up the organization-level configuration  **Why other options are incorrect:** - **A:** The delegated administrator account cannot remove its own delegation‚Äîonly the management account can - **C:** Removing the service principal doesn't properly deregister the delegated administrator - **D:** Deleting S3 buckets doesn't disable Security Lake and may cause errors  **Important:** Always use the CLI command from the management account to ensure proper cleanup.", docLink: "https://docs.aws.amazon.com/security-lake/latest/userguide/multi-account-management.html" },
            { id: 36, question: "Your company's security policy requires that all AWS API calls use TLS 1.2 or higher. A security engineer needs to identify AWS service endpoints that are still receiving connections using older TLS versions (TLS 1.0 or 1.1) to update client configurations. What AWS service and approach should they use?", options: ["Enable VPC Flow Logs and query for TLS version information using Athena; create alerts for TLS 1.0/1.1 connections.","Use AWS CloudTrail Lake to run SQL queries against CloudTrail events, filtering by the `tlsDetails.tlsVersion` attribute to identify older TLS connections.","Configure AWS WAF to inspect TLS handshake versions and log connections using older protocols to CloudWatch Logs.","Use AWS Config's `required-tls-version` managed rule to detect and report AWS service endpoints accepting older TLS versions."], correct: 1, explanation: "**CloudTrail Lake** provides a managed data lake for storing and querying CloudTrail events using SQL.  **For TLS version detection:**  CloudTrail events include `tlsDetails` information containing: - `tlsVersion`: The TLS version used (e.g., \"TLSv1.2\", \"TLSv1.0\") - `cipherSuite`: The cipher suite negotiated - `clientProvidedHostHeader`: The host header from the request  **SQL Query Example:** ```sql SELECT userIdentity.principalId, eventTime, eventName,         tlsDetails.tlsVersion, awsRegion FROM <event-data-store-id> WHERE tlsDetails.tlsVersion IN ('TLSv1.0', 'TLSv1.1')   AND eventTime > '2026-01-01' ORDER BY eventTime DESC; ```  **Why CloudTrail Lake:** - Pre-indexed for fast queries - SQL-based analysis (no need to write custom code) - Can query across years of historical data - Identifies which principals/applications are using old TLS  **Why other options are incorrect:** - **A:** VPC Flow Logs don't capture TLS version information - **C:** AWS WAF protects web applications, not AWS API endpoints - **D:** AWS Config doesn't have a managed rule for TLS version detection  **Use case:** This is particularly useful for: - Preparing for AWS's deprecation of TLS 1.0/1.1 - Compliance requirements (PCI DSS requires TLS 1.2+) - Identifying legacy applications needing updates", docLink: "https://aws.amazon.com/blogs/mt/using-aws-cloudtrail-lake-to-identify-older-tls-connections-to-aws-service-endpoints/" },
            { id: 37, question: "A healthcare company is building a HIPAA-compliant application that processes sensitive patient data in AWS Nitro Enclaves. They need to establish TLS connections from within the enclave to external APIs and want to use AWS Certificate Manager to manage certificates. Which statement is TRUE about using ACM with Nitro Enclaves?", options: ["Nitro Enclaves can directly call the ACM API to retrieve private certificates from ACM Private CA.","AWS Certificate Manager for Nitro Enclaves uses a local agent (ACM for Nitro Enclaves package) that runs inside the enclave and communicates with ACM via a local vsock connection to the parent EC2 instance.","Certificates from ACM must be exported as PEM files and manually loaded into the Nitro Enclave at build time.","Nitro Enclaves can only use self-signed certificates generated within the enclave; ACM integration is not supported."], correct: 1, explanation: "**ACM for Nitro Enclaves** provides a secure way to use ACM certificates in isolated enclave environments:  **Architecture:** 1. **ACM for Nitro Enclaves agent** runs **inside** the enclave 2. The agent communicates with the parent EC2 instance via **vsock** (a secure local socket) 3. The parent instance acts as a proxy to ACM API 4. Private keys **never leave** the enclave in plaintext  **How it works:** - The enclave application requests a certificate via the local agent - Agent requests certificate from ACM (via parent instance) - ACM delivers the certificate and encrypted private key - Private key is decrypted inside the enclave's secure memory - Application uses certificate for TLS connections  **Security benefits:** - Private keys protected by enclave's cryptographic isolation - No network exposure of sensitive key material - Automated certificate rotation support - Attestation ensures code integrity  **Why other options are incorrect:** - **A:** Enclaves don't directly call ACM API‚Äîthey use the local agent + parent instance proxy - **C:** Manual PEM export defeats the security purpose; keys would be exposed - **D:** ACM integration IS supported and recommended for production use  **Use cases:** - Processing PHI/PII in enclaves with external API calls - Secure microservices communication - Confidential computing workloads", docLink: "https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-refapp.html" },
            { id: 38, question: "A development team needs to install SSL/TLS certificates on EC2 instances running NGINX web servers. They want to use AWS Certificate Manager but discover that standard ACM certificates cannot be exported for use on EC2. What should they do to use ACM for EC2 instances?", options: ["Use AWS Certificate Manager to issue exportable public certificates that can be downloaded along with the private key for installation on EC2 instances.","Request certificates from ACM Private CA and export them using the `GetCertificate` API with the `CertificateArn` parameter.","Use Let's Encrypt with Certbot running on the EC2 instances to automatically request and renew certificates.","Configure an Application Load Balancer in front of EC2 instances and use standard ACM certificates on the ALB, with HTTP between ALB and EC2."], correct: 0, explanation: "**Options for TLS certificates on EC2 instances:**  **Option A (ACM Exportable Certificates):** AWS Certificate Manager now supports **exportable public certificates**: - Request an exportable public certificate from ACM - Download the certificate and **private key** (using a passphrase) - Install on EC2 instances manually or via automation - You are responsible for renewal (not automatic like standard ACM)  **Use case:** When you need certificates directly on EC2 instances (e.g., NGINX, Apache)  **Option D (ALB with ACM - Most Common):** The recommended AWS approach: - Use Application Load Balancer with standard ACM certificates - ACM handles automatic renewal - TLS termination at the ALB - Communication between ALB and EC2 can be HTTP (private subnet)  **Use case:** Standard web applications with load balancing  **Why option C might be considered:** Let's Encrypt is a valid option for free certificates, but the question asks specifically about using ACM.  **Key differences:**  | Feature | Standard ACM | Exportable ACM | Let's Encrypt | |---------|--------------|----------------|---------------| | Auto-renewal | Yes (ALB/CloudFront) | No | Yes (with Certbot) | | Private key access | No | Yes | Yes | | Cost | Free | Free | Free | | Use on EC2 | No | Yes | Yes |  **Best practice:** Use ALB with standard ACM when possible; use exportable ACM only when you must install certificates directly on instances.", docLink: "https://docs.aws.amazon.com/acm/latest/userguide/acm-exportable-certificates.html" },
            { id: 39, question: "A Security Analyst receives a GuardDuty finding indicating that one of the company's AWS credentials may be compromised. The credential is actually a Secrets Manager secret used by a Lambda function for database access. What GuardDuty finding type would indicate that a Secrets Manager secret is potentially compromised, and what should be done?", options: ["`UnauthorizedAccess:SecretsManager/MaliciousIPCaller.Custom` - Rotate the secret immediately using Secrets Manager rotation.","`CredentialAccess:SecretsManager/AnomalousGetSecretValue` - Investigate the Lambda function's execution role and rotate the secret if confirmed compromised.","`Policy:SecretsManager/SecretPermissive` - Update the resource policy on the secret to restrict access.","GuardDuty does not monitor Secrets Manager activity; use CloudTrail to investigate `GetSecretValue` API calls manually."], correct: 1, explanation: "**GuardDuty monitors Secrets Manager** for suspicious activity including:  **Relevant findings:** - Anomalous `GetSecretValue` API calls from unusual locations - Access to secrets from unexpected principals - Secrets accessed from compromised EC2 instances  **Response to potential compromise:**  **1. Investigate:** - Review CloudTrail logs for `GetSecretValue` calls - Check the GuardDuty finding details (source IP, principal, timestamp) - Verify if the access is legitimate (e.g., from your Lambda function's known IP)  **2. Rotate if compromised:** ```bash aws secretsmanager rotate-secret --secret-id MyDatabaseSecret ```  Secrets Manager supports automatic rotation for: - RDS databases - Redshift clusters - DocumentDB clusters - Custom rotation using Lambda  **3. Update access controls:** - Review and tighten resource policies on secrets - Use VPC endpoints for private access - Enable versioning and auditing  **Why other options are incorrect:** - **A:** This finding type is for IAM credentials, not Secrets Manager secrets (though the response action is correct) - **C:** This would be a policy issue, not a compromise indication - **D:** GuardDuty DOES monitor Secrets Manager (this is one of its data sources)  **Prevention:** - Use VPC endpoints for Secrets Manager - Implement least-privilege access policies - Enable CloudTrail logging for audit - Use Secrets Manager rotation", docLink: "https://docs.aws.amazon.com/secretsmanager/latest/userguide/monitoring-guardduty.html" },
            { id: 40, question: "A security team wants to enhance vulnerability scanning of their Amazon Linux 2023 EC2 instances. Standard Amazon Inspector scanning detects OS and programming language packages, but they want to scan for vulnerabilities in application dependencies bundled inside JAR files and Python packages. Which TWO statements are TRUE about Amazon Inspector deep inspection for EC2 instances? (Select TWO)", options: ["Deep inspection is automatically enabled for all EC2 instances when Amazon Inspector is activated at the account level.","Deep inspection requires the Systems Manager (SSM) agent to be installed and running on the EC2 instances.","Deep inspection can scan for vulnerabilities in package dependencies bundled within JAR files (Java) and wheel/egg files (Python).","Deep inspection works on Windows Server instances but not on Linux-based instances."], correct: 1, explanation: "**Amazon Inspector Deep Inspection** extends vulnerability scanning beyond OS packages to include:  **What deep inspection scans (C is correct):** - **Java:** Dependencies inside JAR, WAR, EAR files - **Python:** Dependencies in wheel (.whl) and egg (.egg) files - **Node.js:** Dependencies in packed applications - Nested package dependencies (transitive dependencies)  **Requirements (B is correct):** - **SSM Agent must be installed** and running on instances - Agent version must support deep inspection (latest recommended) - Instances must have proper IAM instance profile with SSM permissions  **How to enable:** 1. Activate Amazon Inspector in the account 2. Enable EC2 scanning 3. Enable deep inspection specifically (it's a separate toggle) 4. Ensure SSM agent is running on target instances  **Why other options are incorrect:** - **A:** Deep inspection is NOT automatically enabled‚Äîit requires explicit activation - **D:** Deep inspection is available for **Linux-based** instances (Amazon Linux 2, Ubuntu, etc.), not Windows - **E:** Deep inspection is for EC2 instances; container image scanning is a separate Inspector feature (ECR scanning)  **Benefits:** - Discovers vulnerabilities hidden in application dependencies - Compliance with security frameworks requiring deep scanning - Better visibility into supply chain risks", docLink: "https://docs.aws.amazon.com/inspector/latest/user/deep-inspection.html" },
            { id: 41, question: "Your company (Account A: 111111111111) needs to allow developers from a partner company (Account B: 222222222222) to assume a role for accessing S3 buckets in your account. The security team requires that partner developers must authenticate with MFA before assuming the role. Where should the MFA requirement be configured?", options: ["In the IAM policy attached to the developers' users in Account B, add a condition requiring `aws:MultiFactorAuthPresent = true`.","In the trust policy of the IAM role in Account A, add a condition requiring `aws:MultiFactorAuthPresent = true`.","In the S3 bucket policy in Account A, add a condition requiring `aws:MultiFactorAuthPresent = true`.","Enable MFA in AWS Organizations at the root level, which applies to all cross-account access automatically."], correct: 1, explanation: "**MFA for Cross-Account Access** is enforced in the **trust policy** of the role being assumed:  **Correct implementation (Account A - Role Trust Policy):** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Effect\": \"Allow\",     \"Principal\": {       \"AWS\": \"arn:aws:iam::222222222222:root\"     },     \"Action\": \"sts:AssumeRole\",     \"Condition\": {       \"Bool\": {         \"aws:MultiFactorAuthPresent\": \"true\"       }     }   }] } ```  **How it works:** 1. Developer in Account B calls `aws sts assume-role` with MFA token 2. STS checks the trust policy of the target role 3. If `aws:MultiFactorAuthPresent` is true (MFA was used), access is granted 4. If MFA was not used, the assume-role operation is denied  **Why other options are incorrect:** - **A:** IAM policies in Account B can suggest MFA, but can't enforce it for cross-account roles (the trust policy in Account A controls this) - **C:** S3 bucket policies control bucket access, not role assumption - **D:** Organizations doesn't have automatic MFA enforcement for cross-account access  **Important notes:** - Developers must use `--serial-number` and `--token-code` parameters when calling `assume-role` - MFA verification happens at role assumption time, not at API call time - The MFA requirement applies to the Console, CLI, and SDK access", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html" },
            { id: 42, question: "Your company wants to require MFA for all IAM users who perform sensitive operations like deleting S3 buckets or terminating EC2 instances, but regular read operations should not require MFA. How should you implement this MFA requirement?", options: ["Attach an IAM policy to users with `Deny` effect on sensitive actions when `aws:MultiFactorAuthPresent != true`.","Enable MFA Delete on all S3 buckets and require MFA device association for all IAM users.","Configure AWS Organizations SCP to deny sensitive actions unless MFA is present.","Use IAM Access Analyzer to detect sensitive operations and trigger Step Functions workflow requiring MFA approval."], correct: 0, explanation: "**MFA for API operations in the current account** is enforced through IAM policies with conditional `Deny` statements:  **Implementation:** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Sid\": \"DenyAllExceptListedIfNoMFA\",     \"Effect\": \"Deny\",     \"Action\": [       \"ec2:TerminateInstances\",       \"ec2:StopInstances\",       \"s3:DeleteBucket\",       \"s3:DeleteObject\",       \"rds:DeleteDBInstance\"     ],     \"Resource\": \"*\",     \"Condition\": {       \"BoolIfExists\": {         \"aws:MultiFactorAuthPresent\": \"false\"       }     }   }] } ```  **How it works:** 1. Users must call `aws sts get-session-token --serial-number <MFA-device-ARN> --token-code <MFA-code>` 2. Use the temporary credentials returned (which have `aws:MultiFactorAuthPresent = true`) 3. Sensitive operations succeed; without MFA credentials, they're denied  **Why other options are incorrect:** - **B:** MFA Delete is only for S3 bucket versioned object deletion and requires the root account - **C:** SCPs could work but affect all principals in the account (less granular than IAM policies) - **D:** IAM Access Analyzer doesn't provide MFA enforcement or approval workflows  **Best practices:** - Use `BoolIfExists` instead of `Bool` to handle cases where the key isn't present - Apply this policy in addition to regular permissions policies - Educate users on using `get-session-token` with MFA", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html" },
            { id: 43, question: "Your company has S3 buckets containing financial data. You want to require MFA for any `s3:DeleteObject` operations on these buckets, regardless of which IAM principal attempts the deletion. How should you implement this MFA requirement?", options: ["Add an IAM policy to all users and roles requiring MFA for S3 delete operations.","Add a condition to the S3 bucket policy denying `s3:DeleteObject` when `aws:MultiFactorAuthPresent != true`.","Enable MFA Delete on the S3 bucket through versioning configuration; this automatically requires MFA for all delete operations.","Use S3 Object Lock in compliance mode, which inherently requires MFA for all object deletions."], correct: 1, explanation: "**MFA with resource-based policies** is enforced in the resource policy itself (e.g., S3 bucket policy):  **Implementation:** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Sid\": \"DenyDeleteWithoutMFA\",     \"Effect\": \"Deny\",     \"Principal\": \"*\",     \"Action\": \"s3:DeleteObject\",     \"Resource\": \"arn:aws:s3:::my-financial-bucket/*\",     \"Condition\": {       \"BoolIfExists\": {         \"aws:MultiFactorAuthPresent\": \"false\"       }     }   }] } ```  **Key aspects:** - Applies to **all principals** (users, roles, federated users) - Works for same-account and cross-account access - Requires users to obtain MFA session credentials before deleting - An explicit Deny always overrides Allow policies  **Difference from MFA Delete:** - **MFA Delete (C):** Requires root account MFA for versioned object deletion only - **Resource policy MFA:** Requires any authenticated principal to use MFA credentials  **Why other options are incorrect:** - **A:** IAM policies on all principals is management-intensive and doesn't cover future principals - **C:** MFA Delete is different‚Äîit's for versioned objects and requires root MFA specifically - **D:** S3 Object Lock prevents deletion based on retention periods, not MFA  **Important:** Principals must use `aws sts get-session-token` with MFA to get temporary credentials that satisfy this condition.", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html" },
            { id: 44, question: "A financial services company has S3 buckets with versioning enabled storing transaction records. They want to ensure that even administrators cannot permanently delete object versions or disable versioning without MFA authentication. Which TWO statements about S3 MFA Delete are TRUE? (Select TWO)", options: ["MFA Delete can be enabled by any IAM user with `s3:PutBucketVersioning` permissions.","MFA Delete requires using the root account credentials to enable or disable the feature.","When MFA Delete is enabled, deleting an object version requires MFA authentication; adding a delete marker does not require MFA.","MFA Delete protects against both accidental and malicious permanent deletion of object versions."], correct: 1, explanation: "**S3 MFA Delete** provides an additional layer of protection for versioned buckets:  **Requirements and behavior:**  **B is correct:** MFA Delete can ONLY be enabled/disabled by: - The AWS account root user - Using AWS CLI or API (not the console) - With MFA authentication  **D is correct:** MFA Delete protects against: - Accidental deletion of object versions - Malicious deletion by compromised credentials (without MFA) - Permanent deletion (deleting version IDs)  **C is partially correct:** When MFA Delete is enabled: - **Permanent deletion** (deleting a specific version ID) requires MFA - **Adding a delete marker** (deleting without version ID) does NOT require MFA - **Disabling versioning** requires MFA  **How to enable:** ```bash aws s3api put-bucket-versioning \\   --bucket my-bucket \\   --versioning-configuration Status=Enabled,MFADelete=Enabled \\   --mfa \"arn:aws:iam::123456789012:mfa/root-account-mfa-device XXXXXX\" ```  **Why other options are incorrect:** - **A:** Only root can enable MFA Delete, not regular IAM users - **E:** MFA Delete requires versioning to already be enabled  **Use case:** Critical data requiring highest level of deletion protection (audit logs, financial records, compliance data).", docLink: "https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html" },
            { id: 45, question: "Your company's security policy requires that all AWS resources must be created only in `us-east-1`, `us-west-2`, and `eu-west-1` for data residency compliance. You need to prevent users from creating resources in other regions. Which IAM policy correctly implements this restriction?", options: ["(Invalid option)","(Invalid option)","(Invalid option)","(Invalid option)"], correct: 0, explanation: "**Restricting resource creation by region** requires the `aws:RequestedRegion` condition key:  **Policy explanation (Option A):** - **Effect: Deny** - Denies actions (always overrides Allow) - **Action: \"\\*\"** - Applies to all actions - **aws:RequestedRegion** - The region where the resource will be created - **StringNotEquals** - Denies if region is NOT in the allowed list  **Important distinction:** - **`aws:RequestedRegion`** - Where the API call is creating/modifying resources - **`aws:SourceRegion`** - Only valid for certain services (not general restriction) - **`aws:Region`** - Deprecated, use `aws:RequestedRegion` instead  **Why other options are incorrect:** - **B:** `Allow` with condition only allows listed regions but doesn't prevent users with other Allow policies from using denied regions - **C:** `aws:SourceRegion` is for cross-region operations (like S3 replication), not general resource creation - **D:** Only restricts EC2 instances; other services would not be restricted  **Best practice:** Apply this as: - An SCP at the organization/OU level for broad enforcement - Or an IAM permissions boundary for delegated administrators  **Exception:** Global services (IAM, Route 53, CloudFront) aren't affected by region restrictions.", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html" },
            { id: 46, question: "Your company wants to ensure that AWS API calls can only be made from corporate office IPs or VPN endpoints. You need to deny all AWS actions from IP addresses outside the approved range. Which IAM policy correctly denies access from non-corporate IP addresses?", options: ["(Invalid option)","(Invalid option)","(Invalid option)","(Invalid option)"], correct: 0, explanation: "**IP-based access control** uses the `aws:SourceIp` condition key:  **Policy explanation (Option A):** - **Effect: Deny** - Denies actions from non-approved IPs - **NotIpAddress** - Matches IPs NOT in the specified CIDR ranges - **aws:SourceIp** - The source IP of the API request  **Critical considerations:**  **Problem with Option A:** This policy will deny access from AWS services (CloudFormation, Lambda, etc.) making calls on your behalf because those services don't have your corporate IP.  **Better implementation:** ```json {   \"Effect\": \"Deny\",   \"Action\": \"*\",   \"Resource\": \"*\",   \"Condition\": {     \"NotIpAddress\": {       \"aws:SourceIp\": [\"203.0.113.0/24\", \"198.51.100.0/24\"]     },     \"StringNotEquals\": {       \"aws:PrincipalServiceName\": [\"cloudformation.amazonaws.com\", \"lambda.amazonaws.com\"]     }   } } ```  **Why other options are incorrect:** - **B:** `Allow` with IP restriction doesn't prevent other Allow policies from granting access - **C:** `aws:VpcSourceIp` is for VPC endpoints, not general IP restriction - **D:** `StringNotLike` with wildcards works but `NotIpAddress` with CIDR is more precise  **Alternative:** Use `aws:ViaAWSService` condition to allow AWS services: ```json \"Condition\": {   \"BoolIfExists\": {\"aws:ViaAWSService\": \"false\"},   \"NotIpAddress\": {\"aws:SourceIp\": [...]} } ```", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html" },
            { id: 47, question: "Your company is deploying a generative AI application on AWS that will make recommendations for loan approvals. The security and compliance team wants to ensure the system adheres to AWS's Responsible AI principles throughout the AI lifecycle. Which THREE dimensions are part of AWS's Responsible AI framework? (Select THREE)", options: ["Fairness - Considering impacts on different stakeholder groups and ensuring equitable outcomes","Profitability - Ensuring AI systems maximize revenue while minimizing operational costs","Explainability - Understanding and evaluating AI system outputs with clear reasoning","Automation - Eliminating human oversight to increase processing speed and efficiency"], correct: 0, explanation: "**AWS's Responsible AI Framework** defines **eight key dimensions** across the AI lifecycle:  1. **Fairness (A is correct):** Considering impacts on different stakeholder groups to ensure equitable outcomes 2. **Transparency:** Enabling stakeholders to make informed choices about AI system engagement 3. **Explainability (C is correct):** Understanding and evaluating system outputs with clear reasoning 4. **Accountability:** Incorporating best practices into the AI supply chain and governance 5. **Privacy and Security (E is correct):** Appropriately obtaining, using, and protecting data and models 6. **Safety:** Reducing harmful system output and misuse 7. **Controllability (F is correct):** Having mechanisms to monitor and steer AI system behavior 8. **Veracity and Robustness:** Achieving correct outputs even with unexpected inputs  **Why other options are incorrect:** - **B (Profitability):** While business value matters, profitability is NOT a core Responsible AI principle‚Äîthe framework focuses on ethical and safe AI deployment - **D (Automation without oversight):** This contradicts Responsible AI principles. For consequential decisions (loans, health, fundamental rights), AWS **requires appropriate human oversight**, not elimination of humans  **Key requirement:** For use cases impacting fundamental rights, health, or safety, AWS mandates: - Appropriate human oversight - Rigorous testing - Use case-specific safeguards  **AWS's approach:** People-centric framework that prioritizes education and integrates responsible AI across the entire lifecycle: design, develop, deploy, and operate phases.", docLink: "https://d1.awsstatic.com/products/generative-ai/responsbile-ai/AWS-Responsible-Use-of-AI-Guide-Final.pdf" },
            { id: 48, question: "A healthcare company is building a patient support chatbot using Amazon Bedrock with Claude foundation model. They need to implement safeguards to prevent the AI from providing medical advice, protect patient privacy, and detect when the model generates inaccurate information (hallucinations). Which combination of Amazon Bedrock Guardrails policies should they implement? (Select THREE)", options: ["Content moderation to filter harmful content including hate speech and violence","PII redaction to detect and mask personally identifiable information in inputs and responses","Topic classification to define and block denied topics such as medical diagnosis and treatment advice","Automated cost optimization to reduce foundation model inference charges"], correct: 1, explanation: "**Amazon Bedrock Guardrails** provides six policy types for responsible AI:  **For this healthcare chatbot scenario:**  **B - PII Redaction (Required):** - Detects and masks PII in inputs and responses - Critical for HIPAA compliance and patient privacy - Prevents exposure of patient names, medical record numbers, SSNs, etc.  **C - Topic Classification (Required):** - Define denied topics (medical diagnosis, treatment advice, prescriptions) - Block conversations attempting to get medical guidance - Custom topic definitions ensure chatbot stays within support role  **E - Hallucination Detection (Required):** - Uses contextual grounding checks to validate responses - Automated Reasoning provides 99% verification accuracy - Ensures chatbot doesn't fabricate medical information  **Other guardrails available (but less critical for this scenario):** - **A - Content moderation:** Useful but not the primary concern for healthcare support - **F - Geographic restriction:** Not a Bedrock Guardrails feature - **D - Cost optimization:** Not a guardrails policy  **Key capabilities:** - **Cross-model consistency:** Guardrails work across any foundation model (Bedrock, self-hosted, OpenAI, Google Gemini) - **Policy-based enforcement:** IAM policies can mandate specific guardrails using `bedrock:GuardrailIdentifier` condition - **Deterministic explainability:** Automated Reasoning uses formal mathematical logic for auditable validation  **Implementation:** ```json {   \"Effect\": \"Allow\",   \"Action\": \"bedrock:InvokeModel\",   \"Resource\": \"*\",   \"Condition\": {     \"StringEquals\": {       \"bedrock:GuardrailIdentifier\": \"arn:aws:bedrock:region:account:guardrail/healthcare-chatbot\"     }   } } ```", docLink: "https://aws.amazon.com/bedrock/guardrails/" },
            { id: 49, question: "A financial services company is implementing an AI system on AWS to automate credit risk assessments for mortgage applications. The system will analyze applicant financial data and make preliminary lending decisions. The compliance team is reviewing whether this requires human oversight under AWS Responsible AI guidelines. According to AWS Responsible AI principles, which approach correctly implements oversight for this use case?", options: ["Fully automate the lending decisions since AI models are more objective and consistent than humans; deploy the system with automated model monitoring only.","Implement human oversight for all lending decisions because this is a consequential decision impacting financial rights; include human review, rigorous testing, and use case-specific safeguards.","Use AI for final lending decisions but require human approval only for loan denials to reduce bias and improve customer experience.","Implement a hybrid approach where AI makes decisions for loan amounts under $50,000, and humans review only larger loans to balance efficiency with oversight."], correct: 1, explanation: "**AWS Responsible AI - Consequential Decisions Framework:**  AWS mandates that for **consequential decisions** impacting: - **Fundamental rights** (employment, credit, housing) - **Health and safety** - **Legal status**  Organizations MUST implement: 1. **Appropriate human oversight** 2. **Rigorous testing and validation** 3. **Use case-specific safeguards**  **Mortgage lending qualifies as consequential** because it affects fundamental financial rights and can significantly impact people's lives.  **Required implementation (Option B is correct):**  **Human Oversight:** - Human experts review AI recommendations before final decisions - Humans can override AI decisions with documented reasoning - \"Human-in-the-loop\" or \"Human-on-the-loop\" design patterns  **Rigorous Testing:** - Fairness testing across demographic groups - Validation against historical lending outcomes - Bias detection and mitigation - Ongoing performance monitoring  **Use Case-Specific Safeguards:** - Explainable AI (XAI) to understand decision factors - Audit trails for all decisions - Regular model recalibration - Appeals process for applicants  **Why other options are incorrect:** - **A:** Fully automated consequential decisions violate AWS Responsible AI principles‚Äîhuman oversight is mandatory - **C:** All lending decisions require oversight, not just denials (approvals can also be biased or incorrect) - **D:** Dollar threshold doesn't change the consequential nature‚Äîall mortgage decisions impact fundamental rights  **Additional considerations:** - Regulatory requirements (Equal Credit Opportunity Act, Fair Housing Act) - Model cards documenting intended use, limitations, and fairness metrics - Regular fairness audits and bias assessments - Transparency with applicants about AI usage  **AWS's position:** \"Responsible AI requires a shared commitment between developers, deployers, and end users of AI systems,\" with organizations maintaining ultimate accountability for outcomes.", docLink: "https://d1.awsstatic.com/products/generative-ai/responsbile-ai/AWS-Responsible-Use-of-AI-Guide-Final.pdf" },
            { id: 59, question: "An IAM user named `developer-john` belongs to a group called `Developers` and has an inline policy directly attached. The user attempts to delete an S3 object at `s3://production-data/config.json`. The policies are as follows: What is the result of the delete operation?", options: ["The operation is ALLOWED because the S3 bucket policy explicitly allows the action for all principals in the account, and resource-based policies take precedence over identity-based policies.","The operation is DENIED because the explicit Deny in the user inline policy overrides all Allow statements, following the principle that Deny always wins in IAM policy evaluation.","The operation is ALLOWED because the group policy provides Allow permission, and group policies are evaluated before user policies, granting access before the Deny is evaluated.","The operation is evaluated based on the timestamp of policy creation, with the most recently created or modified policy taking precedence, resulting in unpredictable behavior."], correct: 1, explanation: "**Why B is correct:**  IAM policy evaluation follows a specific logic where **explicit Deny always wins**, regardless of any Allow statements:  **IAM Policy Evaluation Logic:**  1. **By default, all requests are DENIED** (implicit deny) 2. **Evaluate all applicable policies:**    - Identity-based policies (user policies, group policies, role policies)    - Resource-based policies (S3 bucket policies, KMS key policies, etc.)    - Permissions boundaries    - Service Control Policies (SCPs) in Organizations    - Session policies (for assumed roles) 3. **If ANY policy has an explicit DENY ‚Üí request is DENIED** (Deny wins) 4. **If NO explicit Deny AND at least one explicit ALLOW ‚Üí request is ALLOWED** 5. **If NO explicit Allow ‚Üí request is DENIED** (default deny)  **Summary: Deny > Allow > Default Deny**  **In this scenario:**  1. **Group policy (Developers):** Explicit Allow for `s3:*` on `production-data/*` ‚úÖ 2. **User inline policy:** Explicit Deny for `s3:DeleteObject` on `production-data/config.json` ‚ùå 3. **S3 bucket policy:** Explicit Allow for `s3:DeleteObject` ‚úÖ  **Evaluation result:** - Multiple Allow statements exist (group policy + bucket policy) - ONE explicit Deny exists (user inline policy) - **Deny wins** ‚Üí Operation is DENIED  **Visual representation:** ``` ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  IAM Policy Evaluation Flow         ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ  1. Any Explicit DENY? ‚Üí YES ‚ùå     ‚îÇ ‚îÇ     ‚îî‚îÄ> DENY (stop here)            ‚îÇ ‚îÇ                                     ‚îÇ ‚îÇ  2. Any Explicit ALLOW? ‚Üí Not checked‚îÇ ‚îÇ     (already denied in step 1)      ‚îÇ ‚îÇ                                     ‚îÇ ‚îÇ  3. Default DENY ‚Üí Not reached      ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ```  **Why other options are incorrect:**  **A (Resource-based policies take precedence):** FALSE - Resource-based policies do NOT override explicit Deny in identity-based policies - Resource-based policies and identity-based policies are evaluated together - An explicit Deny in ANY policy type will deny the request - **Common misconception:** Resource-based policies have different evaluation for cross-account access, but Deny still always wins  **C (Group policies evaluated before user policies):** FALSE - IAM does NOT evaluate policies in a sequential order based on attachment type - All policies are evaluated simultaneously/in parallel - Policy evaluation is NOT \"first match wins\" - The evaluation logic is: \"Deny anywhere = Deny everywhere\" - Order of evaluation does not matter‚ÄîDeny always wins  **D (Timestamp-based precedence):** FALSE - IAM policy evaluation is deterministic, not based on creation time - The evaluation logic is always consistent: Deny > Allow > Default Deny - AWS does not consider timestamps when evaluating policies - This would create unpredictable security behavior (anti-pattern)  **Key concepts:**  **IAM Policy Types (all evaluated together):**  | Policy Type | Scope | Example | |-------------|-------|---------| | **Identity-based** | Attached to users, groups, roles | User inline policy, Group policy | | **Resource-based** | Attached to resources | S3 bucket policy, KMS key policy | | **Permissions boundaries** | Max permissions for identity | Set boundary on IAM user/role | | **Service Control Policies (SCP)** | Max permissions in Org account | Organization-level restrictions | | **Session policies** | Limit assumed role permissions | Passed when assuming role |  **Policy Evaluation Order:** 1. **Explicit Deny in ANY policy ‚Üí DENY** 2. **Explicit Allow in ANY policy (and no Deny) ‚Üí ALLOW** 3. **No explicit Allow ‚Üí DENY (default)**  **Cross-Account Access Exception:** For cross-account access, you need BOTH: - Allow in the source account (identity-based policy) - Allow in the destination account (resource-based policy)  But even then, an explicit Deny in either account will deny the request.  **Practical examples:**  **Example 1: Protect critical resources** ```json {   \"Sid\": \"DenyDeleteProductionDatabase\",   \"Effect\": \"Deny\",   \"Action\": \"rds:DeleteDBInstance\",   \"Resource\": \"arn:aws:rds:us-east-1:123456789012:db:production-db\" } ``` Even if a user has admin permissions, this Deny prevents database deletion.  **Example 2: Prevent privilege escalation** ```json {   \"Sid\": \"DenyIAMPrivilegeEscalation\",   \"Effect\": \"Deny\",   \"Action\": [     \"iam:AttachUserPolicy\",     \"iam:AttachGroupPolicy\",     \"iam:AttachRolePolicy\",     \"iam:PutUserPolicy\",     \"iam:PutGroupPolicy\",     \"iam:PutRolePolicy\"   ],   \"Resource\": \"*\",   \"Condition\": {     \"ArnEquals\": {       \"iam:PolicyARN\": \"arn:aws:iam::aws:policy/AdministratorAccess\"     }   } } ```  **Best practices:** - Use explicit Deny to enforce guardrails (prevent deletion of critical resources) - Use SCPs to set organization-wide restrictions - Avoid conflicting Allow/Deny in the same policy (confusing) - Test policies with IAM Policy Simulator before deployment - Document the reason for explicit Deny statements  **Debugging policy evaluation:** - Use AWS CloudTrail to see `errorCode: \"AccessDenied\"` - Check `errorMessage` for which policy denied access - Use IAM Policy Simulator to test specific scenarios - Review all attached policies (user, group, resource, SCP, boundaries)", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html" },
            { id: 60, question: "Your mobile app uses Amazon Cognito with both User Pool authentication and social identity providers (Google and Facebook). After authentication, users need AWS credentials from a Cognito Identity Pool. You want to implement role-based access control where: - Users authenticated via User Pool get assigned the `AuthenticatedUserRole` - Users authenticated via Google get the `GoogleUserRole` with enhanced permissions - Users authenticated via Facebook get the `FacebookUserRole` with restricted permissions Which Cognito Identity Pool configuration accomplishes this requirement?", options: ["Configure the Identity Pool with a single IAM role for authenticated users. Use IAM policy variables like `${cognito-identity.amazonaws.com:provider}` in the role's permission policy to grant different access levels based on the authentication provider.","Configure the Identity Pool with Rules-based mapping. Create mapping rules that evaluate the `cognito:preferred_role` claim from each provider, then assign roles dynamically based on rule matching with precedence order.","Configure the Identity Pool with Token-based mapping. Set up role mappings for each identity provider (User Pool, Google, Facebook) with the \"Use default role\" option, assigning different IAM roles to each authentication provider.","Configure separate Identity Pools for each authentication provider (one for User Pool, one for Google, one for Facebook). Each Identity Pool is associated with its respective IAM role, and the mobile app selects the appropriate Identity Pool based on the authentication method used."], correct: 2, explanation: "**Why C is correct:**  **Token-based mapping with provider-specific roles** is the standard Cognito Identity Pool feature for this use case:  **Configuration:**  1. **Create three IAM roles:**    - `AuthenticatedUserRole` (for User Pool users)    - `GoogleUserRole` (for Google users)    - `FacebookUserRole` (for Facebook users)  2. **Configure Identity Pool mappings:**  ```json {   \"IdentityPoolId\": \"us-east-1:12345678-1234-1234-1234-123456789012\",   \"IdentityPoolName\": \"MyAppIdentityPool\",   \"AllowUnauthenticatedIdentities\": false,   \"CognitoIdentityProviders\": [{     \"ProviderName\": \"cognito-idp.us-east-1.amazonaws.com/us-east-1_ABC123\",     \"ClientId\": \"7cdefghijk123456789\",     \"ServerSideTokenCheck\": false   }],   \"SupportedLoginProviders\": {     \"accounts.google.com\": \"GOOGLE_CLIENT_ID\",     \"graph.facebook.com\": \"FACEBOOK_APP_ID\"   },   \"RoleMappings\": {     \"cognito-idp.us-east-1.amazonaws.com/us-east-1_ABC123:7cdefghijk123456789\": {       \"Type\": \"Token\",       \"AmbiguousRoleResolution\": \"AuthenticatedRole\",       \"RoleArn\": \"arn:aws:iam::123456789012:role/AuthenticatedUserRole\"     },     \"accounts.google.com\": {       \"Type\": \"Token\",       \"AmbiguousRoleResolution\": \"AuthenticatedRole\",       \"RoleArn\": \"arn:aws:iam::123456789012:role/GoogleUserRole\"     },     \"graph.facebook.com\": {       \"Type\": \"Token\",       \"AmbiguousRoleResolution\": \"AuthenticatedRole\",       \"RoleArn\": \"arn:aws:iam::123456789012:role/FacebookUserRole\"     }   } } ```  **How it works:** 1. User authenticates with one of the providers 2. Mobile app exchanges token with Identity Pool 3. Identity Pool looks at the token issuer/provider 4. Maps to the appropriate IAM role based on provider 5. Calls STS AssumeRoleWithWebIdentity with the mapped role 6. Returns temporary credentials with that role's permissions  **Mobile app code (simplified):** ```javascript // After User Pool authentication AWS.config.credentials = new AWS.CognitoIdentityCredentials({   IdentityPoolId: 'us-east-1:12345678-1234-1234-1234-123456789012',   Logins: {     'cognito-idp.us-east-1.amazonaws.com/us-east-1_ABC123': userPoolIdToken   } }); // Gets AuthenticatedUserRole  // After Google authentication   AWS.config.credentials = new AWS.CognitoIdentityCredentials({   IdentityPoolId: 'us-east-1:12345678-1234-1234-1234-123456789012',   Logins: {     'accounts.google.com': googleIdToken   } }); // Gets GoogleUserRole ```  **Why other options are incorrect:**  **A (Single role with policy variables):** Insufficient for different permissions - Cognito Identity Pool provides `${cognito-identity.amazonaws.com:provider}` variable - However, this only tells you the provider name (e.g., \"graph.facebook.com\") - IAM policies have limitations on conditional logic complexity - Cannot grant fundamentally different permission sets via policy variables alone - **Example limitation:** Cannot conditionally allow S3 access for Google but deny for Facebook using just policy variables‚Äîyou'd need different roles - While you could use conditions for fine-grained restrictions, it's not the recommended approach for significantly different permission sets  **B (Rules-based mapping with claims):** Overcomplicated and not the standard pattern - Rules-based mapping evaluates claims/attributes in the token (e.g., groups, custom attributes) - Best for scenarios like: \"If user has claim department=engineering, assign EngineeringRole\" - Not needed when you're just mapping by authentication provider - Adds unnecessary complexity - `cognito:preferred_role` is not automatically set by external providers - **Token-based mapping** (option C) is simpler and designed for provider-based role assignment  **D (Separate Identity Pools per provider):** Operational nightmare - Unnecessary duplication of infrastructure - Three separate Identity Pools to manage - Mobile app must contain logic to select the right pool - More complex monitoring and troubleshooting - Violates principle of simplicity - **Anti-pattern:** One Identity Pool can handle multiple providers‚Äîthat's its purpose  **Key concepts:**  **Cognito Identity Pool Role Mapping Types:**  | Type | Use Case | Configuration | |------|----------|---------------| | **Token-based** | Map by identity provider | Simple provider ‚Üí role mapping | | **Rules-based** | Map by token claims/attributes | Evaluate claims, match rules, assign role |  **Token-based mapping (this scenario):** - Simplest configuration - Maps authentication provider to IAM role - Example: All Google users ‚Üí GoogleRole  **Rules-based mapping example:** ```json {   \"Type\": \"Rules\",   \"AmbiguousRoleResolution\": \"Deny\",   \"RulesConfiguration\": {     \"Rules\": [       {         \"Claim\": \"custom:department\",         \"MatchType\": \"Equals\",         \"Value\": \"engineering\",         \"RoleArn\": \"arn:aws:iam::123456789012:role/EngineeringRole\"       },       {         \"Claim\": \"custom:department\",         \"MatchType\": \"Equals\",         \"Value\": \"finance\",         \"RoleArn\": \"arn:aws:iam::123456789012:role/FinanceRole\"       }     ]   } } ```  **Use rules-based when:** - Need to assign roles based on user attributes (department, job title, etc.) - Same identity provider, but users need different permissions - Custom claims in tokens determine access level  **Use token-based when:** - Different permissions per identity provider (this scenario) - Simple provider-to-role mapping - Don't need claim evaluation  **Best practices:** - Use token-based mapping for provider-based roles (simpler) - Use rules-based mapping for attribute-based access control (ABAC) - Set `AmbiguousRoleResolution` to handle edge cases (AuthenticatedRole or Deny) - Test with all authentication providers before production - Monitor role assumption in CloudTrail  **Monitoring:** ``` CloudTrail event: AssumeRoleWithWebIdentity - requestParameters.roleArn: Shows which role was assumed - userIdentity.principalId: Cognito Identity ID - requestParameters.roleSessionName: Contains provider info ```", docLink: "https://docs.aws.amazon.com/cognito/latest/developerguide/role-based-access-control.html" },
            { id: 61, question: "Your company has a KMS key used to encrypt sensitive data in S3. Security policy requires that all KMS API calls must originate from within specific VPCs (`vpc-11111111` and `vpc-22222222`) through a KMS VPC endpoint (`vpce-1a2b3c4d`). A Lambda function with an IAM role that has `kms:Decrypt` permission attempts to decrypt data. The Lambda is configured with VPC settings to run in `vpc-11111111`. What is the result when the Lambda function attempts to decrypt?", options: ["Decryption SUCCEEDS because the Lambda has IAM permission for kms:Decrypt, runs in the required VPC, and the key policy allows access from the account root principal, satisfying all requirements.","Decryption FAILS because Lambda functions, even when VPC-configured, do not route KMS API calls through the VPC endpoint by default. The aws:sourceVpce condition is not satisfied unless the VPC endpoint policy explicitly allows it.","Decryption SUCCEEDS only if the Lambda function's security group allows outbound HTTPS traffic to the KMS VPC endpoint, and the VPC endpoint's security group allows inbound traffic from the Lambda's security group.","Decryption FAILS because the KMS key policy requires aws:sourceVpce condition, but the Lambda IAM role policy does not include a matching condition. Both policies must have identical conditions for the request to succeed."], correct: 1, explanation: "**Why B is correct:**  This is a **subtle but important distinction** about how AWS service API calls work from VPC-configured Lambda functions:  **The problem:**  1. **Lambda is VPC-configured** (runs in vpc-11111111 with ENIs) 2. **KMS key policy requires** `aws:sourceVpce = vpce-1a2b3c4d` 3. **But:** Lambda's KMS API calls go through AWS's service network, NOT through the VPC endpoint by default  **How Lambda VPC networking works:**  - VPC-configured Lambda has Elastic Network Interfaces (ENIs) in specified subnets - Lambda can access resources in the VPC (RDS, ElastiCache, etc.) - BUT AWS service API calls (KMS, S3, DynamoDB, etc.) use AWS's internal network - These API calls do NOT automatically route through VPC endpoints  **To make this work, you need:**  1. **VPC endpoint for KMS** (already exists: vpce-1a2b3c4d) 2. **VPC endpoint DNS resolution must be configured** 3. **Route table associated with Lambda's subnets** must route KMS traffic to the endpoint 4. **Most importantly:** VPC endpoint must be properly configured and the DNS resolution will direct kms.us-east-1.amazonaws.com to the VPC endpoint  **Correct architecture:**  ``` Lambda (VPC) ‚Üí Route Table ‚Üí KMS VPC Endpoint ‚Üí KMS Service                                      ‚Üì                           aws:sourceVpce = vpce-1a2b3c4d ```  **Without proper VPC endpoint routing:**  ``` Lambda (VPC) ‚Üí AWS Network ‚Üí KMS Service                                 ‚Üì                     aws:sourceVpce = <not set>                          ‚Üí KMS DENIES (condition not met) ```  **Why the API call doesn't use VPC endpoint automatically:** - Lambda uses the AWS SDK which resolves service endpoints via DNS - Default: kms.us-east-1.amazonaws.com ‚Üí public KMS endpoint - With VPC endpoint properly configured: DNS resolution changes - VPC endpoint DNS: kms.us-east-1.amazonaws.com ‚Üí vpce endpoint - Only then does `aws:sourceVpce` get set  **The fix:** Ensure VPC endpoint configuration includes: - **Enable private DNS** on the VPC endpoint (critical!) - Associate VPC endpoint with route tables for Lambda subnets - VPC endpoint policy must allow kms:Decrypt  **Why other options are incorrect:**  **A (Lambda VPC = automatic VPC endpoint usage):** FALSE - VPC-configured Lambda does NOT automatically route AWS API calls through VPC endpoints - Requires proper VPC endpoint configuration with private DNS enabled - The `aws:sourceVpce` condition key is only set when the request actually goes through the VPC endpoint - Common misconception that VPC configuration alone is sufficient  **C (Security group restrictions):** Misleading - Security groups do matter for connectivity, but this is not the primary issue - Even with correct security group rules, the API call might not use the VPC endpoint - The real issue is routing and DNS resolution, not security group rules - KMS VPC endpoints use AWS PrivateLink (doesn't require security group rules in the same way) - **Note:** Security groups are important, but not the root cause here  **D (Matching conditions in both policies):** FALSE - IAM policy and key policy do NOT need matching conditions - Conditions are evaluated where they're defined - Key policy condition checks the request's attributes (`aws:sourceVpce`) - IAM policy grants permission (Action on Resource) - Conditions in one policy don't need to be mirrored in the other  **Key concepts:**  **VPC Endpoints for AWS Services:**  | Endpoint Type | Services | Use Case | |---------------|----------|----------| | **Interface (PrivateLink)** | KMS, Secrets Manager, Systems Manager, etc. | Private IP, ENI-based | | **Gateway** | S3, DynamoDB | Route table entries |  **KMS VPC Endpoint Configuration:**  ```bash # Create VPC endpoint aws ec2 create-vpc-endpoint \\   --vpc-id vpc-11111111 \\   --service-name com.amazonaws.us-east-1.kms \\   --subnet-ids subnet-aaaaaaaa subnet-bbbbbbbb \\   --security-group-ids sg-12345678 \\   --private-dns-enabled  # CRITICAL! ```  **Correct KMS key policy pattern:**  ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"AllowVPCEndpointAccess\",       \"Effect\": \"Allow\",       \"Principal\": {\"AWS\": \"arn:aws:iam::123456789012:root\"},       \"Action\": \"kms:*\",       \"Resource\": \"*\",       \"Condition\": {         \"StringEquals\": {           \"aws:sourceVpce\": \"vpce-1a2b3c4d\"         }       }     },     {       \"Sid\": \"AllowKeyAdministration\",       \"Effect\": \"Allow\",       \"Principal\": {\"AWS\": \"arn:aws:iam::123456789012:role/Admin\"},       \"Action\": \"kms:*\",       \"Resource\": \"*\"     }   ] } ```  **IMPORTANT:** Always include admin policy without VPC condition, or you'll lock yourself out!  **Troubleshooting:**  1. **Check VPC endpoint exists:** `aws ec2 describe-vpc-endpoints --vpc-endpoint-ids vpce-1a2b3c4d` 2. **Verify private DNS enabled:** Look for `PrivateDnsEnabled: true` 3. **Check route tables:** Ensure Lambda subnets' route tables are associated with endpoint 4. **Test from VPC:** Use EC2 instance in same subnet to test KMS API calls 5. **CloudTrail:** Check `vpcEndpointId` field in KMS API events  **Testing:** ```bash # From EC2 in the VPC aws kms decrypt --ciphertext-blob fileb://encrypted.dat \\   --query Plaintext --output text | base64 --decode  # Check CloudTrail for vpcEndpointId aws cloudtrail lookup-events \\   --lookup-attributes AttributeKey=EventName,AttributeValue=Decrypt ```  **Best practices:** - Enable private DNS on KMS VPC endpoints - Include admin statements without VPC endpoint conditions (avoid lockout) - Use `aws:sourceVpc` for broader VPC-level restrictions - Use `aws:sourceVpce` for specific endpoint restrictions - Test thoroughly before enforcing in production - Document VPC endpoint requirements in runbooks  **Condition key comparison:**  | Condition Key | Checks | Use Case | |---------------|--------|----------| | `aws:sourceVpce` | Specific VPC endpoint ID | Restrict to particular endpoint | | `aws:sourceVpc` | VPC ID | Restrict to any endpoint in VPC | | `aws:sourceIp` | Public IP address | Restrict to IP ranges |", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/kms-vpc-endpoint.html" },
            { id: 62, question: "You're configuring an S3 bucket policy for a bucket containing application logs. The requirements are: - Allow CloudWatch Logs service to write logs to the bucket - Allow Lambda functions in your account to read logs - Deny all access if requests don't use TLS/SSL - Allow access only from specific VPC endpoints Which policy statements MUST be included? (Select TWO)", options: ["(Invalid option)","(Invalid option)","(Invalid option)","(Invalid option)"], correct: 0, explanation: "**Why A is correct:**  This is the **standard pattern for enforcing TLS/SSL** on S3 buckets:  ```json {   \"Sid\": \"DenyNonTLS\",   \"Effect\": \"Deny\",   \"Principal\": \"*\",   \"Action\": \"s3:*\",   \"Resource\": [     \"arn:aws:s3:::my-logs-bucket\",      // Bucket operations     \"arn:aws:s3:::my-logs-bucket/*\"     // Object operations   ],   \"Condition\": {     \"Bool\": {       \"aws:SecureTransport\": \"false\"    // Matches non-TLS requests     }   } } ```  **How it works:** - `aws:SecureTransport` is `true` when request uses HTTPS/TLS - `aws:SecureTransport` is `false` when request uses HTTP (unencrypted) - Condition matches when value is `false` (non-TLS) - Effect is `Deny` ‚Üí all non-TLS requests are denied - Applies to ALL actions (`s3:*`) and ALL principals (`*`) - **Critical:** Must include both bucket and object resources  **Why this approach:** - Explicit Deny always wins (even if other statements Allow) - Protects against accidental unencrypted access - Industry best practice for compliance (PCI-DSS, HIPAA, etc.)  **Why B is correct:**  This is the **proper pattern for CloudWatch Logs to S3 delivery**:  ```json {   \"Sid\": \"AllowCloudWatchLogs\",   \"Effect\": \"Allow\",   \"Principal\": {     \"Service\": \"logs.amazonaws.com\"   },   \"Action\": \"s3:PutObject\",   \"Resource\": \"arn:aws:s3:::my-logs-bucket/*\",   \"Condition\": {     \"StringEquals\": {       \"s3:x-amz-acl\": \"bucket-owner-full-control\",       \"aws:SourceAccount\": \"123456789012\"     }   } } ```  **Why these conditions are necessary:**  1. **`s3:x-amz-acl\": \"bucket-owner-full-control\"`**    - CloudWatch Logs sets this ACL when writing objects    - Ensures bucket owner has full control over log objects    - Without this, CloudWatch Logs write would fail (doesn't match its behavior)  2. **`aws:SourceAccount\": \"123456789012\"`**    - Prevents confused deputy problem    - Ensures CloudWatch Logs service can only write logs from YOUR account    - Without this, CloudWatch Logs from other accounts could potentially write to your bucket    - Security best practice for service principals  **Complete CloudWatch Logs to S3 policy also needs:** ```json {   \"Sid\": \"AWSLogDeliveryAclCheck\",   \"Effect\": \"Allow\",   \"Principal\": {     \"Service\": \"logs.amazonaws.com\"   },   \"Action\": \"s3:GetBucketAcl\",   \"Resource\": \"arn:aws:s3:::my-logs-bucket\",   \"Condition\": {     \"StringEquals\": {       \"aws:SourceAccount\": \"123456789012\"     }   } } ```  **Why other options are incorrect:**  **C (Allow with TLS condition):** Wrong approach for TLS enforcement - Uses `Effect\": \"Allow\"` with `aws:SecureTransport\": \"true\"` - This allows TLS requests but does NOT deny non-TLS requests - Without explicit Deny, non-TLS requests could be allowed by other policies - Also requires encryption condition, which is not mentioned in requirements - **The requirement is to DENY non-TLS, not to ALLOW TLS** (subtle but important distinction)  **Comparison:** ``` Option A (Correct):  Explicit DENY non-TLS ‚Üí blocks all unencrypted access Option C (Wrong):    Explicit ALLOW TLS ‚Üí doesn't block unencrypted access ```  **D (Allow Lambda via VPC endpoint):** Insufficient for VPC endpoint requirement - Uses `Effect\": \"Allow\"` which doesn't enforce VPC endpoint requirement - Lambda functions could still access via public internet (if they have Allow elsewhere) - Doesn't prevent non-VPC endpoint access - Need Deny statement (like option E) to enforce, but...  **E (Deny non-VPC endpoint access):** Would break CloudWatch Logs - CloudWatch Logs service does NOT use VPC endpoints - CloudWatch Logs writes from AWS's service network - This Deny would block CloudWatch Logs (requirement violation) - You cannot use both \"restrict to VPC endpoint\" AND \"allow CloudWatch Logs service\" - **Conflicting requirements:** VPC endpoint restriction is incompatible with service principal access  **The fundamental conflict:** - CloudWatch Logs service writes from AWS's network (not through your VPC endpoint) - Enforcing VPC endpoint for ALL access would block CloudWatch Logs - Need to either:   - Accept CloudWatch Logs writes bypass VPC endpoint requirement, OR   - Use alternative log delivery mechanism (Kinesis Firehose via VPC)  **Key concepts:**  **aws:SecureTransport patterns:**  ```json // CORRECT: Deny non-TLS {   \"Effect\": \"Deny\",   \"Condition\": {\"Bool\": {\"aws:SecureTransport\": \"false\"}} }  // INSUFFICIENT: Allow TLS (doesn't deny non-TLS) {   \"Effect\": \"Allow\",   \"Condition\": {\"Bool\": {\"aws:SecureTransport\": \"true\"}} } ```  **Service principals and confused deputy:**  | Without aws:SourceAccount | With aws:SourceAccount | |---------------------------|------------------------| | Service can act on behalf of ANY account | Service can only act on behalf of YOUR account | | Vulnerable to confused deputy | Protected from confused deputy |  **Confused deputy example:** 1. Attacker creates CloudWatch log group in their account (attacker-account) 2. Attacker configures log export to your bucket (victim-bucket) 3. Without `aws:SourceAccount`, CloudWatch Logs service writes attacker's logs to your bucket 4. With `aws:SourceAccount`, CloudWatch Logs checks and denies (different account)  **Best practice S3 bucket policy template:**  ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"DenyNonTLS\",       \"Effect\": \"Deny\",       \"Principal\": \"*\",       \"Action\": \"s3:*\",       \"Resource\": [         \"arn:aws:s3:::bucket\",         \"arn:aws:s3:::bucket/*\"       ],       \"Condition\": {         \"Bool\": {\"aws:SecureTransport\": \"false\"}       }     },     {       \"Sid\": \"DenyUnencryptedObjectUploads\",       \"Effect\": \"Deny\",       \"Principal\": \"*\",       \"Action\": \"s3:PutObject\",       \"Resource\": \"arn:aws:s3:::bucket/*\",       \"Condition\": {         \"StringNotEquals\": {           \"s3:x-amz-server-side-encryption\": \"aws:kms\"         }       }     },     {       \"Sid\": \"AllowServicePrincipal\",       \"Effect\": \"Allow\",       \"Principal\": {\"Service\": \"logs.amazonaws.com\"},       \"Action\": [\"s3:PutObject\", \"s3:GetBucketAcl\"],       \"Resource\": [         \"arn:aws:s3:::bucket\",         \"arn:aws:s3:::bucket/*\"       ],       \"Condition\": {         \"StringEquals\": {           \"aws:SourceAccount\": \"123456789012\",           \"s3:x-amz-acl\": \"bucket-owner-full-control\"         }       }     }   ] } ```", docLink: "https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html" },
            { id: 63, question: "Your company (Account A: 111111111111) provides a SaaS monitoring service. Customer accounts (Account B: 222222222222) create IAM roles that your service assumes to collect CloudWatch metrics. To prevent the confused deputy security problem, you require customers to include an ExternalId in the role's trust policy. Why is ExternalId necessary in this scenario?", options: ["ExternalId acts as an additional authentication factor (like MFA) that encrypts the AssumeRole request, preventing man-in-the-middle attacks when assuming roles across accounts.","ExternalId prevents the confused deputy problem where a malicious customer could trick your service into assuming a role in a different customer's account by providing the ARN of another customer's role.","ExternalId ensures that only the monitoring service can assume the role, preventing the customer's own IAM users from assuming the role and impersonating the monitoring service to access resources.","ExternalId is required by AWS for all cross-account role assumptions and serves as an audit identifier in CloudTrail logs to track which external service assumed the role."], correct: 1, explanation: "**Why B is correct:**  **The Confused Deputy Problem** is a classic security vulnerability in cross-account access:  **Without ExternalId - The Attack:**  1. **Setup:**    - Your SaaS service (Account A) has functionality to assume customer roles    - Customer 1 (Account C1) creates RoleC1 for your service    - Customer 2 (Account C2) creates RoleC2 for your service    - Attacker is a malicious user in Customer 1's account  2. **The Attack:**    - Attacker (in C1) signs up for your monitoring service    - During setup, attacker provides RoleC2 ARN (victim's role) instead of RoleC1    - Your service (deputized by C1) attempts to assume RoleC2    - If RoleC2 trusts Account A without ExternalId, assumption succeeds    - Your service now accesses C2's resources on behalf of the attacker    - Attacker gains unauthorized access to victim's account through your service  **Sequence diagram:** ``` Attacker (C1) ‚Üí Your Service (A) ‚Üí \"Assume RoleC2 (C2)\"                                     ‚Üì                         RoleC2: Trusts Account A (no ExternalId check)                                     ‚Üì                             Access Granted ‚ùå                                     ‚Üì                     Your Service reads C2's data                                     ‚Üì                         Returns data to Attacker ```  **With ExternalId - Protection:**  1. **Setup:**    - Your service generates unique ExternalId for each customer (e.g., UUID)    - Customer 1 gets ExternalId: \"c1-unique-12345\"    - Customer 2 gets ExternalId: \"c2-unique-67890\"    - Customer role trust policies require matching ExternalId  2. **Attack Prevention:**    - Attacker provides RoleC2 ARN with C1's ExternalId \"c1-unique-12345\"    - Your service calls AssumeRole with ExternalId \"c1-unique-12345\"    - RoleC2 requires ExternalId \"c2-unique-67890\"    - **Mismatch** ‚Üí AssumeRole DENIED ‚úÖ    - Attack prevented  **Best practices for ExternalId:**  1. **Generate unique ExternalId per customer:** ```python import uuid external_id = str(uuid.uuid4())  # e.g., \"c7f4b8e1-2d3a-4b5c-8e9f-1a2b3c4d5e6f\" ```  2. **Store mapping securely:** ``` Customer ID ‚Üí ExternalId ‚Üí Role ARN customer-123 ‚Üí uuid-abc ‚Üí arn:aws:iam::222222222222:role/Role ```  3. **Include in customer setup instructions:** ``` Step 1: Create IAM role with trust policy: {   \"Effect\": \"Allow\",   \"Principal\": {\"AWS\": \"arn:aws:iam::111111111111:role/ServiceRole\"},   \"Action\": \"sts:AssumeRole\",   \"Condition\": {     \"StringEquals\": {       \"sts:ExternalId\": \"YOUR_EXTERNAL_ID_HERE\"     }   } } ```  **Why other options are incorrect:**  **A (Authentication factor / encryption):** Incorrect understanding - ExternalId is NOT an encryption mechanism - Does not encrypt the AssumeRole request - Not comparable to MFA (multi-factor authentication) - ExternalId is transmitted in plaintext as part of the API call - Does not prevent MITM attacks (TLS does that) - **Misconception:** ExternalId is about authorization, not encryption  **C (Prevent customer's users from assuming role):** Wrong purpose - ExternalId does NOT prevent the customer's own users from assuming their own role - If customer's IAM user has `sts:AssumeRole` permission, they can assume it (with correct ExternalId) - Customer can see their own trust policy and knows the ExternalId - **The point:** ExternalId is a secret between YOU (service provider) and CUSTOMER, not protection from customer themselves - To prevent customer's users: Use role trust policy with specific Principal (not AWS account root)  **D (Required by AWS for all cross-account):** Factually wrong - ExternalId is OPTIONAL, not required by AWS - Many cross-account patterns don't use ExternalId:   - Organization cross-account roles (trust based on Org structure)   - Specific IAM principal trusts (not account root) - ExternalId IS a best practice for third-party service integrations - Primary purpose is security (confused deputy), not auditing (though CloudTrail does log it)  **Key concepts:**  **When to use ExternalId:**  | Scenario | Use ExternalId? | |----------|----------------| | **Third-party SaaS assuming customer roles** | ‚úÖ YES (confused deputy risk) | | **Your service users customer-provided role ARNs** | ‚úÖ YES (user-controlled ARNs) | | **Organization SCPs with cross-account roles** | ‚ùå NO (trusted environment) | | **Specific principal trusts (not account root)** | ‚ùå NO (not confused deputy scenario) |  **Confused deputy patterns:**  **Vulnerable pattern:** ```json {   \"Principal\": {\"AWS\": \"arn:aws:iam::SAAS_ACCOUNT:root\"},   \"Action\": \"sts:AssumeRole\" } ``` Problem: Any principal in SAAS_ACCOUNT can assume this role.  **Protected pattern:** ```json {   \"Principal\": {\"AWS\": \"arn:aws:iam::SAAS_ACCOUNT:root\"},   \"Action\": \"sts:AssumeRole\",   \"Condition\": {     \"StringEquals\": {\"sts:ExternalId\": \"unique-secret\"}   } } ``` Solution: Only principals with correct ExternalId can assume.  **ExternalId requirements:** - String between 2 and 1,224 characters - Can include letters, numbers, and: `+=,.@:/-` - Case-sensitive - Should be unique per customer - Should be treated as a secret (not publicly shared)  **CloudTrail logging:** ```json {   \"eventName\": \"AssumeRole\",   \"requestParameters\": {     \"roleArn\": \"arn:aws:iam::222222222222:role/CustomerRole\",     \"roleSessionName\": \"monitoring-session\",     \"externalId\": \"unique-customer-id-12345\"  // Logged in CloudTrail   },   \"responseElements\": {     \"assumedRoleUser\": {       \"arn\": \"arn:aws:sts::222222222222:assumed-role/CustomerRole/monitoring-session\"     }   } } ```  **Alternative protection (not using ExternalId):** Trust specific IAM principal instead of account root: ```json {   \"Principal\": {     \"AWS\": \"arn:aws:iam::111111111111:role/SpecificServiceRole\"   },   \"Action\": \"sts:AssumeRole\" } ```  This works if: - You control the specific principal - Customers don't provide role ARNs (you create them) - Not a confused deputy scenario", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/confused-deputy.html" },
            { id: 64, question: "Your mobile banking app uses Amazon Cognito User Pools for authentication. The security team requires protection against credential compromise and account takeover attacks. You need to implement risk-based adaptive authentication that can detect suspicious sign-in attempts (e.g., from unusual locations or devices) and require additional verification. The solution should also prevent automated bot attacks during sign-up. Which combination of Cognito features BEST addresses these requirements?", options: ["Enable Cognito Advanced Security Features for risk-based authentication. Configure adaptive authentication with MFA for medium and high-risk events. Implement Amazon Cognito user pool triggers (Pre-Authentication Lambda) to detect unusual login patterns and block suspicious requests.","Enable Cognito Advanced Security Features with automatic risk mitigation. Configure CAPTCHA (reCAPTCHA) integration for sign-up and sign-in forms. Use Cognito's built-in compromised credentials detection to block passwords found in data breaches. Require SMS MFA for all users regardless of risk level.","Implement AWS WAF with rate limiting rules in front of API Gateway that processes Cognito authentication requests. Use Cognito user pool triggers (Post-Authentication Lambda) to log authentication events to CloudWatch for security analysis. Enable account recovery via security questions.","Enable Cognito Advanced Security Features for risk-based authentication with adaptive MFA based on risk score. Integrate with Amazon Fraud Detector to create custom fraud detection models. Use device tracking and remembering to establish trusted devices and reduce MFA prompts for known devices."], correct: 0, explanation: "**Why A is correct:**  This option combines Cognito's native security features with extensibility through Lambda triggers:  **1. Cognito Advanced Security Features:** - **Risk-based authentication:** Analyzes sign-in patterns for anomalies - **Risk scoring:** Assigns low, medium, or high risk scores - **Signals analyzed:**   - IP address geolocation and reputation   - Device fingerprinting   - Velocity checks (rapid auth attempts)   - Time since last authentication   - Location changes   - Device changes  **2. Adaptive Authentication with MFA:** ```json {   \"RiskConfiguration\": {     \"CompromisedCredentialsRiskConfiguration\": {       \"Actions\": {         \"EventAction\": \"BLOCK\"       }     },     \"AccountTakeoverRiskConfiguration\": {       \"Actions\": {         \"LowAction\": {\"Notify\": true, \"EventAction\": \"NO_ACTION\"},         \"MediumAction\": {\"Notify\": true, \"EventAction\": \"MFA_IF_CONFIGURED\"},         \"HighAction\": {\"Notify\": true, \"EventAction\": \"MFA_REQUIRED\"}       }     }   } } ```  **Configuration:** - **Low risk:** Allow with optional notification - **Medium risk:** Require MFA if configured for user - **High risk:** Always require MFA or block  **3. Pre-Authentication Lambda Trigger:** ```python def lambda_handler(event, context):     # Access risk information     request = event['request']     risk_level = request.get('userContextData', {}).get('riskLevel', 'LOW')          # Custom logic: detect unusual patterns     user_ip = request['userContextData']['sourceIp'][0]     if is_blacklisted_ip(user_ip):         raise Exception(\"Authentication failed - suspicious IP\")          # Check for impossible travel     if impossible_travel_detected(event['userName'], user_ip):         raise Exception(\"Authentication failed - impossible travel detected\")          return event  # Allow authentication to proceed ```  **Benefits:** - Native Cognito risk detection (no infrastructure) - Adaptive MFA (better UX than always-on MFA) - Extensible with custom logic (Lambda triggers) - Blocks compromised credentials automatically  **Why other options are incorrect:**  **B (Automatic risk mitigation + CAPTCHA + SMS MFA for all):** Mix of good and suboptimal - ‚úÖ Advanced Security Features: Good - ‚úÖ CAPTCHA integration: Good for bot prevention - ‚úÖ Compromised credentials detection: Good - ‚ùå **SMS MFA for all users regardless of risk:** Poor UX and NOT adaptive   - Defeats the purpose of risk-based authentication   - Users with consistent good behavior still face MFA every time   - SMS MFA has security concerns (SIM swapping, interception)   - Better: Adaptive MFA based on risk (option A)  **Comparison:** ``` Option A: Low risk = no MFA, High risk = MFA (adaptive) Option B: All users = always MFA (not adaptive) ```  **C (WAF rate limiting + Post-Auth Lambda + security questions):** Several issues - ‚ùå **WAF rate limiting:** Addresses DDoS, not credential compromise or account takeover   - Different security layer (network vs application)   - Doesn't provide risk-based authentication - ‚ùå **Post-Authentication Lambda:** Runs AFTER authentication succeeds   - Cannot block suspicious authentication (already authenticated)   - Should use Pre-Authentication for blocking - ‚ùå **Security questions:** Deprecated security practice   - Weak authentication factor (easily researched/guessed)   - Not recommended by modern security standards   - Cognito doesn't have native security questions feature  **D (Advanced Security + Fraud Detector + device tracking):** Over-engineered - ‚úÖ Advanced Security Features: Good - ‚úÖ Adaptive MFA: Good - ‚úÖ Device tracking: Good (Cognito native feature) - ‚ùå **Amazon Fraud Detector integration:** Unnecessary complexity   - Cognito Advanced Security already provides fraud detection   - Fraud Detector is for transaction fraud (e-commerce), not authentication   - Requires custom ML model training and maintenance   - Significant additional cost   - **Overkill:** Use Fraud Detector for payment fraud, not authentication  **Key concepts:**  **Cognito Advanced Security Features:**  | Feature | Capability | Use Case | |---------|------------|----------| | **Risk-based authentication** | Detects anomalies in sign-in | Adaptive MFA based on risk | | **Compromised credentials** | Checks passwords against breach databases | Block known compromised passwords | | **Device tracking** | Remembers user devices | Reduce MFA for trusted devices | | **Adaptive authentication** | Dynamic security controls | Balance security and UX |  **Risk Actions:**  ``` ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Risk Level  ‚îÇ Action Options                   ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Low         ‚îÇ - Allow                          ‚îÇ ‚îÇ             ‚îÇ - Notify (email/SMS)             ‚îÇ ‚îÇ             ‚îÇ - No additional action           ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Medium      ‚îÇ - MFA if configured              ‚îÇ ‚îÇ             ‚îÇ - Notify + Allow                 ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ High        ‚îÇ - Always require MFA             ‚îÇ ‚îÇ             ‚îÇ - Block                          ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ```  **Cognito User Pool Triggers:**  | Trigger | When | Use Case | |---------|------|----------| | **Pre-Authentication** | Before authenticating user | Custom validation, IP checks | | **Post-Authentication** | After successful auth | Logging, analytics | | **Pre-Sign-up** | Before registering user | Custom validation, auto-confirm | | **Post-Confirmation** | After user confirms | Send welcome email, provision resources | | **Pre-Token-Generation** | Before token issued | Add custom claims to JWT |  **Pre-Authentication Lambda example:** ```python import json import boto3  def lambda_handler(event, context):     # Event structure     username = event['userName']     user_context = event['request'].get('userContextData', {})     ip_address = user_context.get('sourceIp', ['Unknown'])[0]          # Custom logic     if is_suspicious(username, ip_address):         # Block authentication         raise Exception(\"Suspicious activity detected\")          # Allow authentication to proceed     return event ```  **Device tracking and remembering:** ```python # Configure device tracking user_pool.update_user_pool(     UserPoolId='us-east-1_ABC123',     DeviceConfiguration={         'ChallengeRequiredOnNewDevice': True,         'DeviceOnlyRememberedOnUserPrompt': False     } ) ```  **Benefits of device tracking:** - Reduced MFA prompts for trusted devices - Detect device changes (risk signal) - User can see and manage devices - Automatic device remembering  **Cost considerations:**  | Feature | Pricing | |---------|---------| | **Basic Cognito User Pool** | 50,000 MAU free, then $0.0055/MAU | | **Advanced Security Features** | Additional $0.05/MAU | | **SMS MFA** | $0.00645/message (expensive for high volume) | | **TOTP MFA** | Free (software-based) |  **Best practices:** - Enable Advanced Security Features for production applications - Use adaptive MFA (not always-on SMS MFA) - Prefer TOTP over SMS for MFA (more secure, free) - Configure device tracking to reduce MFA friction - Use Pre-Authentication triggers for custom validation - Monitor risk events in CloudWatch - Set appropriate risk action thresholds for your use case  **Monitoring:** ```python # CloudWatch metrics - SignInAttempts (with Risk dimension) - RiskDetected - TokenRefreshAttempts - AccountTakeoverRisk ```", docLink: "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-advanced-security.html" },
            { id: 65, question: "Your application runs in multiple AWS regions (us-east-1 and eu-west-1) for disaster recovery. You use AWS KMS to encrypt application data in S3 and DynamoDB. Security requirements mandate that the same key must be usable in both regions, encrypted data must remain accessible during a regional outage, and key policies must be centrally managed without per-region configuration. Which approach BEST meets these requirements?", options: ["Create separate CMKs in each region with identical key policies. Use S3 Cross-Region Replication (CRR) with SSE-KMS encryption to replicate objects between regions, specifying the destination region's CMK. Replicate DynamoDB global tables with region-specific KMS keys and use multi-region encryption.","Create a multi-region KMS key in us-east-1 as the primary key. Enable key replication to eu-west-1 to create a replica key. Configure S3 and DynamoDB to use the multi-region key ARN, which automatically resolves to the local replica key in each region. Update the key policy once on the primary key, which propagates to all replicas.","Create a KMS key in us-east-1 and grant cross-region access by adding eu-west-1 principals to the key policy. Configure S3 and DynamoDB in eu-west-1 to use the us-east-1 KMS key ARN for encryption, accepting the cross-region latency for cryptographic operations.","Create a KMS key in us-east-1 and export the key material. Import the same key material into a separate CMK in eu-west-1, creating functionally identical keys in both regions. Configure applications to use the local key in each region while maintaining the same underlying cryptographic material."], correct: 1, explanation: "**Why B is correct:**  **Multi-region KMS keys** are specifically designed for this use case:  **Architecture:**  1. **Create multi-region primary key in us-east-1:** ```bash aws kms create-key \\   --description \"Multi-region application key\" \\   --multi-region true \\   --region us-east-1 ```  2. **Replicate to eu-west-1:** ```bash aws kms replicate-key \\   --key-id mrk-1234567890abcdef \\   --replica-region eu-west-1 \\   --region us-east-1 ```  3. **Key properties:** ``` Primary: arn:aws:kms:us-east-1:123456789012:key/mrk-1234567890abcdef Replica: arn:aws:kms:eu-west-1:123456789012:key/mrk-1234567890abcdef Multi-region key ID: mrk-1234567890abcdef (same in both regions) ```  **Key benefits:**  **1. Same key ID in all regions:** - Encrypt in us-east-1 with `mrk-1234567890abcdef` - Decrypt in eu-west-1 with the same key ID - Ciphertext is portable across regions  **2. Automatic key resolution:** ```python # Application code (works in any region) s3 = boto3.client('s3', region_name=os.environ['AWS_REGION']) s3.put_object(     Bucket='my-bucket',     Key='data.json',     Body=data,     ServerSideEncryption='aws:kms',     SSEKMSKeyId='mrk-1234567890abcdef'  # Resolves to local replica ) ``` - AWS automatically uses the replica key in the local region - No region-specific configuration needed  **3. Centralized key policy management:** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Sid\": \"Enable IAM User Permissions\",     \"Effect\": \"Allow\",     \"Principal\": {\"AWS\": \"arn:aws:iam::123456789012:root\"},     \"Action\": \"kms:*\",     \"Resource\": \"*\"   }] } ``` - Update policy on primary key in us-east-1 - Policy automatically propagates to all replica keys - No need to update each region separately  **4. Disaster recovery:** - If us-east-1 is unavailable, applications use eu-west-1 replica - No cross-region KMS API calls (low latency) - Encrypted data readable in any region with replica  **S3 integration:** ```bash # Encrypt object in us-east-1 aws s3 cp file.txt s3://bucket/ \\   --sse aws:kms \\   --sse-kms-key-id mrk-1234567890abcdef \\   --region us-east-1  # Access same object in eu-west-1 (after CRR) aws s3 cp s3://bucket/file.txt . \\   --region eu-west-1 # Decrypts using eu-west-1 replica automatically ```  **DynamoDB Global Tables:** ```python # Create global table with multi-region KMS key dynamodb = boto3.client('dynamodb') dynamodb.create_global_table(     GlobalTableName='MyGlobalTable',     ReplicationGroup=[         {'RegionName': 'us-east-1'},         {'RegionName': 'eu-west-1'}     ] )  # Update table to use multi-region KMS key dynamodb.update_table(     TableName='MyGlobalTable',     SSESpecification={         'Enabled': True,         'SSEType': 'KMS',         'KMSMasterKeyId': 'mrk-1234567890abcdef'     } ) ```  **Why other options are incorrect:**  **A (Separate CMKs in each region):** Operational overhead and limitations - ‚ùå **Separate keys:** Different key IDs, not the same key - ‚ùå **Manual policy synchronization:** Must update policies in each region - ‚ùå **Not truly portable:** Ciphertext encrypted with us-east-1 key cannot be decrypted with eu-west-1 key - ‚ùå **S3 CRR re-encryption:** Objects must be re-encrypted with destination key during replication (additional processing) - ‚ùå **DynamoDB:** Global Tables replicate plaintext, then re-encrypt with destination key - **Fails requirement:** \"same key must be usable in both regions\"  **C (Cross-region KMS API calls):** Performance and availability issues - ‚ùå **Latency:** Every encrypt/decrypt operation calls us-east-1 KMS API from eu-west-1 (added latency) - ‚ùå **Single point of failure:** If us-east-1 is unavailable, eu-west-1 applications cannot encrypt/decrypt - ‚ùå **Increased costs:** Cross-region data transfer charges - ‚ùå **Not supported:** Many AWS services (S3, DynamoDB, EBS) require KMS key in the same region - **Fails requirement:** \"encrypted data must remain accessible during a regional outage\"  **D (Import same key material to multiple CMKs):** Complexity and risk - ‚ùå **Complex:** Requires external key management and manual import process - ‚ùå **Separate key IDs:** Different ARNs in each region (arn:aws:kms:us-east-1:...:key/different-id) - ‚ùå **Manual key policy sync:** Must update policies separately in each region - ‚ùå **Key material expiration:** Imported keys can have expiration, requiring renewal - ‚ùå **Less secure:** Key material exists outside AWS KMS (exposure risk) - **Fails requirement:** \"key policies must be centrally managed without per-region configuration\"  **Key concepts:**  **Multi-region key properties:**  | Property | Value | |----------|-------| | **Key spec** | SYMMETRIC_DEFAULT (AES-256) | | **Key usage** | ENCRYPT_DECRYPT | | **Origin** | AWS_KMS (managed by KMS) | | **Multi-region** | True | | **Primary/Replica** | One primary, multiple replicas |  **Multi-region vs Single-region keys:**  | Feature | Multi-region | Single-region | |---------|--------------|---------------| | **Key ID format** | mrk-xxxxxx | xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx | | **Usable regions** | Primary + replicas | Single region only | | **Ciphertext portability** | Cross-region | Same region only | | **Policy management** | Centralized | Per-key | | **Use case** | DR, multi-region apps | Single region apps |  **When to use multi-region keys:**  ‚úÖ **Use multi-region keys when:** - Applications run in multiple regions - Need disaster recovery across regions - S3 Cross-Region Replication with encryption - DynamoDB Global Tables with encryption - Cross-region data sharing  ‚ùå **Don't use multi-region keys when:** - Application is single-region only (unnecessary complexity) - Data never leaves one region - Regulatory requirements prevent cross-region key usage  **Limitations and considerations:**  1. **Cannot convert:**    - Cannot convert single-region key to multi-region    - Cannot convert multi-region key to single-region    - Must create as multi-region from the start  2. **Primary key promotion:**    - Can promote replica to primary if primary region fails    - Only one primary at a time  3. **Deletion:**    - Must delete all replicas before deleting primary    - Each key has separate deletion schedule  4. **Pricing:**    - Each replica key incurs monthly key storage cost ($1/month per key)    - API calls billed in the region where called  **Best practices:**  ```bash # 1. Create with appropriate policy from the start aws kms create-key \\   --description \"Multi-region app key\" \\   --multi-region true \\   --policy file://key-policy.json  # 2. Tag for management aws kms tag-resource \\   --key-id mrk-xxx \\   --tags TagKey=Environment,TagValue=Production  # 3. Enable CloudWatch monitoring aws kms put-key-policy \\   --key-id mrk-xxx \\   --policy-name default \\   --policy file://policy-with-cloudtrail.json  # 4. Replicate to DR regions aws kms replicate-key \\   --key-id mrk-xxx \\   --replica-region eu-west-1  # 5. Grant usage to services aws kms create-grant \\   --key-id mrk-xxx \\   --grantee-principal arn:aws:iam::123456789012:role/AppRole \\   --operations Decrypt Encrypt GenerateDataKey ```  **Monitoring:** ```python # CloudWatch metrics per region - NumberOfDecrypts - NumberOfEncrypts - APIErrors ```", docLink: "https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html" },
            { id: 50, question: "You are a Security Engineer at a mobile gaming startup. The development team wants to add user authentication to their iOS and Android apps. Users should be able to sign up with email/password, authenticate with social identity providers (Google, Facebook), and securely access AWS resources like S3 (for storing game saves) and DynamoDB (for leaderboards). The solution must not require managing server infrastructure for authentication. Which AWS architecture BEST meets these requirements?", options: ["Use IAM users for each game player with programmatic access keys embedded in the mobile app. Store credentials in the app's secure keychain and rotate them monthly using a Lambda function triggered by EventBridge.","Implement Amazon Cognito User Pools for authentication and Cognito Identity Pools for AWS credential vending. Configure the User Pool with email and social identity providers, then use the Identity Pool to exchange User Pool tokens for temporary AWS credentials with an IAM role.","Deploy a custom authentication server on EC2 with Auto Scaling behind an ALB. Use this server to authenticate users and generate IAM session tokens using AWS STS AssumeRole, returning temporary credentials to the mobile app via HTTPS API.","Use AWS IAM Identity Center (AWS SSO) with SAML 2.0 federation. Configure mobile apps to redirect users to the SSO portal for authentication, then receive SAML assertions that are exchanged for AWS credentials using STS AssumeRoleWithSAML."], correct: 1, explanation: "**Why B is correct:**  Amazon Cognito is purpose-built for this use case and consists of two main components:  1. **Cognito User Pools:** Provide user directory services with:    - Built-in sign-up and sign-in UI    - Email/password authentication    - Social identity provider integration (Google, Facebook, Amazon, Apple)    - Multi-factor authentication (MFA)    - Account verification and password recovery    - JWT tokens (ID token, Access token, Refresh token)  2. **Cognito Identity Pools (Federated Identities):** Provide AWS credential vending:    - Exchange User Pool tokens (or tokens from other IdPs) for temporary AWS credentials    - Uses STS AssumeRoleWithWebIdentity internally    - Supports both authenticated and unauthenticated (guest) access    - Maps users to IAM roles based on authentication provider  **The authentication flow is:** 1. User authenticates with User Pool (email/password or social provider) 2. User Pool returns JWT tokens (ID token, access token) 3. Mobile app exchanges ID token with Identity Pool 4. Identity Pool returns temporary AWS credentials (access key, secret key, session token) 5. Mobile app uses credentials to directly access S3, DynamoDB, etc.  **Why other options are incorrect:**  **A (IAM users with embedded credentials):** This is a **severe security anti-pattern**: - Violates the principle of not embedding long-term credentials in applications - IAM credentials in mobile apps can be extracted via reverse engineering or device compromise - Doesn't scale (millions of game players = millions of IAM users, hitting service limits) - Credential rotation is operationally complex - No built-in sign-up/sign-in UI or social provider integration - **Critical flaw:** Long-term credentials in client applications are the #1 AWS security mistake  **C (Custom authentication server):** Unnecessary and operationally complex: - Requires managing EC2 infrastructure (patching, scaling, monitoring) - Doesn't leverage AWS-managed authentication services - Must implement social provider integration yourself - No built-in features like account verification, password reset, MFA - Higher development and operational cost - While technically functional, it's reinventing what Cognito provides out-of-the-box  **D (IAM Identity Center with SAML):** Wrong use case: - IAM Identity Center is designed for **workforce identity** (employees), not **customer identity** (app users) - SAML is designed for browser-based SSO, not native mobile app authentication - Redirecting mobile app users to a web-based SSO portal creates poor user experience - Doesn't provide social identity provider integration - Not designed to handle millions of end-user identities  **Key concepts:**  - **User Pools vs Identity Pools:**   - User Pools = Authentication (who are you?)   - Identity Pools = Authorization (what can you do?)   - They work together but serve different purposes  - **Token types from User Pool:**   - **ID token:** Contains user identity claims (sub, email, etc.)   - **Access token:** Used to authorize access to User Pool APIs and custom scopes   - **Refresh token:** Used to get new ID/access tokens without re-authentication  - **Best practice:** Use least-privilege IAM roles with Identity Pools, scoped per authentication provider and user attributes", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_federation_common_scenarios.html" },
            { id: 51, question: "Your company already uses Auth0 as the primary identity provider for all applications. You're building a new mobile app that needs to access AWS resources (API Gateway, S3, Lambda). The security team requires that all authentication continue using Auth0 (not migrating to Cognito User Pools), but the mobile app needs temporary AWS credentials to directly invoke API Gateway endpoints secured with IAM authorization. What is the MOST appropriate architecture?", options: ["Configure Auth0 as an OIDC identity provider in IAM. Create an IAM role with a trust policy that trusts the Auth0 OIDC provider. Mobile app authenticates with Auth0, receives an ID token, then exchanges it for AWS credentials using STS AssumeRoleWithWebIdentity.","Configure Auth0 to act as a SAML 2.0 identity provider in IAM. Mobile app redirects users to Auth0 for SAML authentication, receives a SAML assertion, then exchanges it for AWS credentials using STS AssumeRoleWithSAML.","Create a Lambda authorizer (custom authorizer) in API Gateway that validates Auth0 tokens. Mobile app authenticates with Auth0 and includes the Auth0 access token in API Gateway requests. API Gateway does NOT provide AWS credentials; it only authorizes API requests.","Use Cognito Identity Pools configured with Auth0 as an external OIDC identity provider. Mobile app authenticates with Auth0, receives an OIDC token, exchanges it with Cognito Identity Pool for temporary AWS credentials, then accesses AWS resources directly."], correct: 3, explanation: "**Why D is correct:**  This is a common pattern when you have an existing enterprise identity provider and want AWS credential vending:  **Architecture:** 1. Configure Cognito Identity Pool (NOT User Pool‚Äîyou're using Auth0 for authentication) 2. Add Auth0 as an external OIDC identity provider in the Identity Pool configuration 3. Create IAM role(s) with trust policy trusting the Cognito Identity Pool 4. Specify role mapping rules (e.g., based on Auth0 token claims)  **Authentication flow:** 1. Mobile app authenticates user with Auth0 (via Auth0 SDK) 2. Auth0 returns OIDC tokens (ID token, access token) 3. Mobile app calls Cognito Identity Pool with Auth0 ID token 4. Cognito Identity Pool validates the token, maps user to IAM role 5. Cognito calls STS AssumeRoleWithWebIdentity internally 6. Returns temporary AWS credentials to mobile app 7. Mobile app uses credentials to access S3, API Gateway, etc.  **Why this is better than direct STS:** Cognito Identity Pools provide: - Simplified credential management - Built-in token validation and caching - Guest (unauthenticated) access support if needed - Enhanced authentication flow (simplified auth flows) - Role mapping based on claims/attributes - Better handling of token expiration and refresh  **Why other options are incorrect:**  **A (Direct STS AssumeRoleWithWebIdentity):** While technically possible, it has limitations: - Mobile app must handle STS API calls directly (more complex client code) - Must implement token validation logic - No built-in support for credential caching - No support for unauthenticated access patterns - Cognito Identity Pools is the AWS-recommended abstraction layer for this use case - **Important distinction:** While A would work functionally, D is the \"MOST appropriate\" because it's the AWS best practice and provides additional features  **B (SAML federation):** Wrong protocol: - Auth0 supports OIDC natively (modern, JSON-based, mobile-friendly) - SAML is XML-based, designed for browser redirects, not native mobile apps - AssumeRoleWithSAML requires POST binding with base64-encoded SAML assertions - Poor user experience in mobile apps (requires web views) - OIDC is the modern standard for mobile app authentication  **C (Lambda authorizer only):** Doesn't meet the requirement: - Lambda authorizer validates tokens for API Gateway access control - Does NOT provide AWS credentials to the mobile app - Mobile app can only call API Gateway, not directly access S3, Lambda, or other AWS services - Requirement specifically states: \"needs temporary AWS credentials to directly invoke API Gateway endpoints secured with IAM authorization\" - IAM authorization at API Gateway requires AWS credentials (SigV4 signing)  **Key concepts:**  **OIDC vs SAML:** - **OIDC:** Modern, JSON-based, built on OAuth 2.0, designed for mobile/SPAs - **SAML:** XML-based, designed for enterprise browser-based SSO  **Cognito Identity Pools supports:** - Cognito User Pools - Social identity providers (Amazon, Facebook, Google, Apple) - OIDC providers (Auth0, Okta, custom) - SAML providers - Developer authenticated identities (custom auth backend)  **Trust policy example for Identity Pool:** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Effect\": \"Allow\",     \"Principal\": {       \"Federated\": \"cognito-identity.amazonaws.com\"     },     \"Action\": \"sts:AssumeRoleWithWebIdentity\",     \"Condition\": {       \"StringEquals\": {         \"cognito-identity.amazonaws.com:aud\": \"us-east-1:12345678-1234-1234-1234-123456789012\"       },       \"ForAnyValue:StringLike\": {         \"cognito-identity.amazonaws.com:amr\": \"authenticated\"       }     }   }] } ```", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_federation_common_scenarios.html" },
            { id: 52, question: "Your enterprise uses Active Directory Federation Services (AD FS) as the corporate identity provider. Developers need to programmatically access AWS APIs using AWS CLI and SDKs with their corporate credentials. Security requirements mandate that AWS access must be temporary, tied to corporate identity, and not require IAM users. The solution must work from developer workstations (not browser-based SSO). What workflow BEST accomplishes this requirement?", options: ["Configure AD FS as a SAML identity provider in IAM. Developers run a custom script that authenticates with AD FS (using Windows Integrated Authentication), receives a SAML assertion, calls STS AssumeRoleWithSAML to get temporary credentials, and writes them to ~/.aws/credentials.","Use AWS IAM Identity Center (AWS SSO) with AD FS as the identity source. Developers run `aws sso login` which opens a browser for AD FS authentication, receives SAML assertion via browser redirect, exchanges it for temporary credentials, and caches them locally.","Create IAM users for each developer with programmatic access. Configure MFA and implement a Lambda function that validates Active Directory credentials via LDAP, then uses IAM to rotate the access keys daily.","Install AWS Tools for PowerShell with AD FS integration. Configure PowerShell to use Windows Integrated Authentication to AD FS, which automatically provides Kerberos tickets that AWS SDK uses to authenticate API calls without STS."], correct: 0, explanation: "**Why A is correct:**  This is the classic **SAML-based federation for API access** pattern described in AWS documentation:  **Workflow steps:** 1. **User initiates authentication:** Developer runs script/tool on workstation 2. **Authenticate with IdP (AD FS):** Script uses Windows Integrated Authentication (Kerberos) or prompts for credentials 3. **IdP returns SAML assertion:** AD FS authenticates user against Active Directory and returns signed SAML 2.0 assertion containing user identity and attributes 4. **Call STS AssumeRoleWithSAML:** Script sends SAML assertion to STS along with:    - PrincipalArn (ARN of SAML provider in IAM)    - RoleArn (ARN of IAM role to assume)    - SAMLAssertion (base64-encoded SAML response) 5. **Receive temporary credentials:** STS returns:    - AccessKeyId    - SecretAccessKey    - SessionToken    - Expiration (1-12 hours, default 1 hour) 6. **Write credentials locally:** Script writes credentials to ~/.aws/credentials or sets environment variables 7. **AWS CLI/SDK use credentials:** Standard AWS tools automatically use the temporary credentials  **Example script logic (simplified):** ```python import boto3 import requests from bs4 import BeautifulSoup  # 1. Authenticate with AD FS (may use Windows Integrated Auth) # 2. Parse SAML response from AD FS # 3. Extract SAML assertion  sts = boto3.client('sts') response = sts.assume_role_with_saml(     RoleArn='arn:aws:iam::123456789012:role/ADFS-Developers',     PrincipalArn='arn:aws:iam::123456789012:saml-provider/ADFS',     SAMLAssertion=saml_assertion )  # 4. Write credentials to ~/.aws/credentials ```  **Configuration requirements:** - AD FS configured as SAML identity provider in IAM (`iam create-saml-provider`) - IAM role(s) with trust policy trusting the SAML provider - AD FS configured with AWS as a relying party trust - SAML attributes mapped (e.g., roles as SAML attributes)  **Why other options are incorrect:**  **B (AWS IAM Identity Center):** While this works, it's NOT the best answer for this scenario: - IAM Identity Center requires browser-based authentication flow - Question specifically asks for \"programmatic access\" and \"not browser-based SSO\" - `aws sso login` opens a web browser for authentication (violates requirement) - Better suited for browser-based SSO to AWS Console - For pure API/CLI access, direct SAML federation (option A) is more appropriate - **Note:** This is a subtle distinction‚Äîboth work, but the question asks for API-focused workflow  **C (IAM users with daily rotation):** Defeats the purpose of federation: - Creates IAM users, violating \"not require IAM users\" requirement - Daily credential rotation is operational overhead - Doesn't tie AWS access to corporate identity (no SSO) - Lambda validation of AD credentials is unnecessary complexity - Doesn't automatically revoke AWS access when user leaves company (AD account disabled)  **D (Windows Integrated Auth with Kerberos for AWS API):** Not how AWS works: - AWS APIs do not support Kerberos authentication - AWS requires AWS Signature Version 4 (SigV4) for API authentication - SigV4 requires AWS credentials (access key, secret key, optional session token) - You cannot use Kerberos tickets directly with AWS APIs - Must use STS to exchange Kerberos-authenticated SAML assertion for AWS credentials  **Key concepts:**  **SAML 2.0 assertion requirements:** ```xml <saml:Attribute Name=\"https://aws.amazon.com/SAML/Attributes/Role\">   <saml:AttributeValue>     arn:aws:iam::123456789012:role/ADFS-Developers,     arn:aws:iam::123456789012:saml-provider/ADFS   </saml:AttributeValue> </saml:Attribute> ```  **IAM role trust policy:** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Effect\": \"Allow\",     \"Principal\": {       \"Federated\": \"arn:aws:iam::123456789012:saml-provider/ADFS\"     },     \"Action\": \"sts:AssumeRoleWithSAML\",     \"Condition\": {       \"StringEquals\": {         \"SAML:aud\": \"https://signin.aws.amazon.com/saml\"       }     }   }] } ```  **Benefits of this approach:** - No IAM users to manage - Centralized identity management (Active Directory) - Automatic access revocation (disable AD account = no AWS access) - Temporary credentials (enhanced security) - Audit trail (CloudTrail logs include SAML principal)  **Common tools:** - **aws-adfs** (Python package) - **aws-azure-login** (for Azure AD) - **aws-google-auth** (for Google Workspace) - Custom PowerShell scripts", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html" },
            { id: 53, question: "You're building a serverless API for a mobile app using Amazon Cognito User Pools for authentication, API Gateway for the API layer, and Lambda for business logic. User-specific data is stored in DynamoDB. The mobile app should authenticate users via Cognito, then access protected API endpoints. The API must validate JWT tokens and authorize access based on Cognito user identity. Which approach BEST implements this architecture?", options: ["Configure API Gateway with a Cognito User Pool authorizer. Mobile app includes the Cognito ID token in the Authorization header. API Gateway validates the token signature and expiration, then passes user claims (including `sub`) to Lambda via request context. Lambda uses the `sub` claim as the DynamoDB partition key.","Configure API Gateway with an IAM authorizer. Mobile app exchanges Cognito User Pool token with Cognito Identity Pool for AWS credentials, signs API requests with SigV4, and sends them to API Gateway. Lambda receives authenticated user identity via request context.","Implement a Lambda authorizer (custom authorizer) that validates Cognito tokens by calling Cognito UserInfo endpoint. Cache authorization decisions in ElastiCache Redis with a 1-hour TTL. Lambda function queries DynamoDB using the user ID from the cached authorization context.","Mobile app sends Cognito access token to Lambda via API Gateway with no authorizer configured. Lambda validates the token by downloading JWKS from Cognito public keys endpoint, verifies signature and expiration, then extracts user identity for DynamoDB queries."], correct: 0, explanation: "**Why A is correct:**  This is the recommended pattern for **Cognito User Pool + API Gateway + Lambda backend**:  **Architecture components:**  1. **Cognito User Pool Authorizer in API Gateway:**    - Native integration (no custom code)    - Automatically validates JWT token:      - Signature (using JWKS public keys from Cognito)      - Expiration (`exp` claim)      - Issuer (`iss` claim matches User Pool)      - Audience (`aud` claim matches App Client ID)    - Token source: `Authorization` header or custom header    - Can use either ID token or Access token (ID token is more common for user identity)  2. **Token passed to Lambda:**    - API Gateway passes token claims to Lambda via `$context.authorizer.claims`    - Lambda can access user identity without token validation code    - Example in Lambda: `event['requestContext']['authorizer']['claims']['sub']`  3. **DynamoDB access pattern:**    - Use `sub` (subject) claim as partition key or GSI key    - `sub` is a UUID that uniquely identifies the user    - Stable identifier (doesn't change unlike email)  **Example API Gateway configuration:** ```json {   \"type\": \"COGNITO_USER_POOLS\",   \"authorizerId\": \"abc123\",   \"providerARNs\": [     \"arn:aws:cognito-idp:us-east-1:123456789012:userpool/us-east-1_ABC123\"   ] } ```  **Lambda access to user identity:** ```python def lambda_handler(event, context):     user_sub = event['requestContext']['authorizer']['claims']['sub']     user_email = event['requestContext']['authorizer']['claims']['email']     # Query DynamoDB using sub as key ```  **Why other options are incorrect:**  **B (IAM authorizer with Identity Pool):** Adds unnecessary complexity: - Requires Cognito Identity Pool (not needed if only accessing API Gateway) - Mobile app must exchange tokens AND sign requests with SigV4 (more complex client code) - Identity Pool is designed for direct AWS service access (S3, DynamoDB), not API Gateway with Lambda - While functional, it's overkill when you just need authenticated API access - **Use Identity Pool when:** Mobile app needs to directly access AWS services (S3, DynamoDB, etc.) - **Use User Pool authorizer when:** Mobile app only needs to access your custom API (this scenario)  **C (Lambda authorizer calling Cognito UserInfo):** Inefficient and unnecessary: - Adds latency (external HTTP call to Cognito UserInfo endpoint on every request) - Cognito User Pool authorizer already validates tokens‚Äîwhy revalidate? - ElastiCache adds operational complexity and cost - Lambda authorizer with caching is appropriate for third-party IdPs (Auth0, Okta), not when API Gateway has native Cognito integration - **Anti-pattern:** Using Lambda authorizer when native authorizer exists  **D (No authorizer - Lambda validates tokens):** Security vulnerability: - API endpoints are OPEN without API Gateway authorization - Anyone can call the API endpoint (even without a token) - Token validation in Lambda is boilerplate code (error-prone) - Lambda execution time wasted on validation - No authorization caching (validates on every invocation) - **Critical flaw:** Lambda shouldn't be responsible for authentication‚Äîthat's API Gateway's job  **Key concepts:**  **Cognito token types and use cases:**  | Token | Purpose | Use with API Gateway | |-------|---------|---------------------| | **ID Token** | User identity claims (sub, email, name, etc.) | ‚úÖ Best for User Pool authorizer | | **Access Token** | Authorization (scopes, groups) | ‚úÖ Also supported, better for OAuth2 scopes | | **Refresh Token** | Get new ID/Access tokens | ‚ùå Never send to backend APIs |  **Token validation checklist (what API Gateway does automatically):** 1. ‚úÖ Signature verification (using JWKS) 2. ‚úÖ Expiration check (`exp` claim) 3. ‚úÖ Issuer validation (`iss` claim) 4. ‚úÖ Audience validation (`aud` claim) 5. ‚úÖ Token not used before issued (`iat` claim) 6. ‚úÖ Token type (`token_use` claim = \"id\" or \"access\")  **Best practices:** - Use `sub` claim for user identification (stable, unique) - Don't use `email` or `username` as DynamoDB keys (can change) - Consider using Cognito groups for role-based access control - Set appropriate token expiration (ID token: 1 hour, Access token: 1 hour, Refresh token: 30 days)  **When to use each architecture:**  | Scenario | Architecture | |----------|--------------| | API only (no direct AWS service access) | User Pool + API Gateway authorizer | | API + direct S3/DynamoDB access | User Pool + Identity Pool + IAM auth | | Third-party IdP (Auth0, Okta) | Lambda authorizer or Identity Pool with OIDC |", docLink: "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html" },
            { id: 54, question: "Your company's mobile app uses Amazon Cognito User Pools for authentication and API Gateway with Lambda for the backend. The product team wants to implement feature flags that control which features users can access. Feature flags are stored in a DynamoDB table and can be updated in real-time. The API must authorize each request based on the user's identity AND their current feature flag settings. What is the MOST appropriate implementation?", options: ["Use a Cognito User Pool authorizer in API Gateway. In each Lambda function, query DynamoDB for the user's feature flags using the `sub` claim from the Cognito token, check if the requested feature is enabled, and return 403 if disabled.","Use a Lambda authorizer (REQUEST type) in API Gateway. Validate the Cognito token, query DynamoDB for feature flags, generate a cached IAM policy that includes feature-specific resource ARNs with Allow/Deny statements, and set cache TTL to 5 minutes.","Use Cognito User Pool authorizer with Cognito Groups to represent feature flags. Update group memberships when feature flags change. API Gateway authorizer passes groups to Lambda via claims, and Lambda checks if required feature group is present.","Configure API Gateway with no authorizer. Lambda function validates the Cognito token using JWKS, queries feature flags from DynamoDB, caches the combined authorization decision in Lambda execution context, and reuses it for warm container invocations."], correct: 1, explanation: "**Why B is correct:**  This scenario requires **dynamic authorization** based on both identity AND external data (feature flags), making Lambda authorizer the appropriate choice:  **Architecture:**  1. **Lambda Authorizer (REQUEST type):**    - Receives entire request context (headers, path, query strings)    - Validates Cognito token (signature, expiration, issuer)    - Extracts user identity (`sub` claim)    - Queries DynamoDB for user's feature flags    - Generates IAM policy based on enabled features    - Returns policy with caching key  2. **Policy generation based on features:** ```python def lambda_handler(event, context):     # 1. Validate Cognito token     token = event['headers']['Authorization']     claims = validate_jwt(token)  # Verify signature, expiration     user_id = claims['sub']          # 2. Query feature flags     features = get_user_features(user_id)  # DynamoDB query          # 3. Generate IAM policy     policy = {         'principalId': user_id,         'policyDocument': {             'Version': '2012-10-17',             'Statement': []         },         'context': {             'sub': user_id,             'features': json.dumps(features)         }     }          # 4. Add statements based on features     if 'premium-analytics' in features:         policy['policyDocument']['Statement'].append({             'Effect': 'Allow',             'Action': 'execute-api:Invoke',             'Resource': 'arn:aws:execute-api:*:*/*/GET/analytics/*'         })          # 5. Deny other resources     policy['policyDocument']['Statement'].append({         'Effect': 'Deny',         'Action': 'execute-api:Invoke',         'Resource': 'arn:aws:execute-api:*:*/*/GET/analytics/*'     } if 'premium-analytics' not in features else {})          return policy ```  3. **Caching:**    - Set `usageIdentifierKey` in authorizer config (e.g., `$request.header.Authorization`)    - Cache TTL: 5 minutes (balance between performance and freshness)    - Feature flag changes take effect after cache expires    - Can invalidate cache by changing token or using cache versioning  **Why this is better than alternatives:** - Centralized authorization logic (not duplicated in every Lambda) - Built-in caching (faster subsequent requests) - Policy-based access control (API Gateway enforces before Lambda invocation) - Context can be passed to Lambda (e.g., feature flags)  **Why other options are incorrect:**  **A (Cognito authorizer + query in each Lambda):** Inefficient duplication: - Authorization logic duplicated across all Lambda functions - Every Lambda invocation queries DynamoDB (increased latency and cost) - No caching of authorization decisions - Cognito User Pool authorizer only validates the token‚Äîit doesn't check feature flags - Feature flag queries should be centralized in authorizer, not scattered across business logic - **Anti-pattern:** Mixing business logic with authorization logic  **C (Cognito Groups as feature flags):** Poor fit for dynamic feature flags: - Cognito Groups are designed for role-based access (e.g., \"Admins\", \"Users\"), not dynamic feature flags - Groups are embedded in JWT token at token issuance time - Changing group membership requires user to re-authenticate to get new token (unacceptable for real-time feature flags) - Not suitable for frequently changing flags (A/B tests, gradual rollouts) - **Key limitation:** Token claims are static until token expires; feature flags need to change in real-time  **D (No authorizer + Lambda validation):** Multiple problems: - API Gateway has no authorization‚Äîendpoints are publicly accessible - Authorization logic in Lambda (should be in authorizer layer) - \"Caching in execution context\" is unreliable:   - Lambda containers are recycled unpredictably   - No cache invalidation mechanism   - Inconsistent behavior across concurrent executions - Not using API Gateway caching capabilities - **Security flaw:** Unauthenticated API access  **Key concepts:**  **Lambda Authorizer types:**  | Type | Input | Use Case | |------|-------|----------| | **TOKEN** | Just the token | Simple token validation (OAuth2, JWT) | | **REQUEST** | Full request context | Complex authorization with request parameters |  **For this scenario:** REQUEST type is needed because authorization depends on the API path/method being accessed.  **Caching strategy:**  ```json {   \"authorizerUri\": \"arn:aws:apigateway:us-east-1:lambda:path/...\",   \"authorizerResultTtlInSeconds\": 300,   \"identitySource\": \"method.request.header.Authorization\",   \"type\": \"REQUEST\" } ```  **Cache key considerations:** - Include user identity (sub) in cache key - Consider including API path if authorization is path-specific - Shorter TTL = more real-time, but more Lambda invocations - Longer TTL = better performance, but stale authorization decisions  **Feature flag architecture best practices:** - Store flags in DynamoDB with TTL for auto-expiration - Use DynamoDB Streams to invalidate authorization cache - Consider using AWS AppConfig for feature flag management - Implement gradual rollouts (percentage-based flags)  **When to use Lambda authorizer:** - Third-party identity providers without native API Gateway support - Custom authorization logic (e.g., feature flags, rate limiting, IP allowlisting) - Need to query external data sources for authorization decisions - Complex multi-factor authorization", docLink: "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html" },
            { id: 55, question: "You're building a mobile photo-sharing app. Users authenticate with Amazon Cognito User Pools (email/password and Google social login). After authentication, users need to directly upload photos to S3 and query their photo metadata from DynamoDB. Each user should only access their own S3 folder and DynamoDB items. The solution must follow AWS security best practices for mobile applications. Which architecture BEST meets these requirements?", options: ["After User Pool authentication, mobile app sends the Cognito ID token to a Lambda function via API Gateway. Lambda assumes an IAM role using STS AssumeRole, generates S3 presigned URLs and DynamoDB credentials, and returns them to the mobile app. Mobile app uses these credentials for direct AWS service access.","Configure a Cognito Identity Pool with User Pool as the authentication provider. After User Pool authentication, mobile app exchanges the ID token with Identity Pool for temporary AWS credentials mapped to an IAM role. Use IAM policy variables (`${cognito-identity.amazonaws.com:sub}`) to restrict S3 and DynamoDB access to user-specific resources.","Create an IAM user for each app user with programmatic access. Store IAM credentials encrypted in DynamoDB with the Cognito User Pool `sub` as the key. After Cognito authentication, mobile app retrieves its IAM credentials from DynamoDB via API Gateway/Lambda and uses them for S3/DynamoDB access.","Use Cognito User Pool authorizer in API Gateway with Lambda proxy integration. All S3 and DynamoDB operations go through Lambda functions that validate the user identity from Cognito claims and perform operations on behalf of the user using Lambda's execution role."], correct: 1, explanation: "**Why B is correct:**  This is the **classic Cognito User Pool + Identity Pool pattern** for mobile app access to AWS resources:  **Architecture components:**  1. **User Pool:** Handles authentication    - Email/password sign-up/sign-in    - Google social login (federated identity)    - Returns JWT tokens (ID token, access token, refresh token)  2. **Identity Pool:** Handles AWS credential vending    - Exchanges User Pool ID token for temporary AWS credentials    - Maps authenticated users to IAM role(s)    - Calls STS AssumeRoleWithWebIdentity internally    - Returns credentials with configurable duration  3. **IAM role with policy variables:**  ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Action\": [         \"s3:PutObject\",         \"s3:GetObject\",         \"s3:DeleteObject\"       ],       \"Resource\": \"arn:aws:s3:::my-photos-bucket/users/${cognito-identity.amazonaws.com:sub}/*\"     },     {       \"Effect\": \"Allow\",       \"Action\": [         \"dynamodb:GetItem\",         \"dynamodb:PutItem\",         \"dynamodb:UpdateItem\",         \"dynamodb:DeleteItem\",         \"dynamodb:Query\"       ],       \"Resource\": \"arn:aws:dynamodb:us-east-1:123456789012:table/PhotoMetadata\",       \"Condition\": {         \"ForAllValues:StringEquals\": {           \"dynamodb:LeadingKeys\": [\"${cognito-identity.amazonaws.com:sub}\"]         }       }     }   ] } ```  **Flow:** 1. User signs in via User Pool (email/password or Google) 2. User Pool returns ID token 3. Mobile app calls Identity Pool with ID token 4. Identity Pool validates token and maps to IAM role 5. Returns temporary credentials (access key, secret key, session token) 6. Mobile app uses AWS SDK to directly access S3/DynamoDB 7. IAM policy variables ensure user only accesses their own data  **Key security features:** - **Policy variables:** `${cognito-identity.amazonaws.com:sub}` dynamically restricts access - **Temporary credentials:** Short-lived (default 1 hour, max 12 hours for authenticated) - **Least privilege:** Each user gets credentials scoped to only their resources - **No credential storage:** Mobile app doesn't store long-term credentials  **DynamoDB pattern:** - Use Cognito Identity ID (`cognito-identity.amazonaws.com:sub`) as partition key - Policy condition `dynamodb:LeadingKeys` restricts query/scan to user's items  **S3 pattern:** - Folder structure: `s3://bucket/users/{cognito-identity-id}/photo.jpg` - Policy `Resource` uses `${cognito-identity.amazonaws.com:sub}` variable  **Why other options are incorrect:**  **A (Lambda generates presigned URLs):** Overcomplicated and less secure: - Every S3 operation requires round-trip to Lambda (increased latency) - Lambda must assume role for each request (unnecessary STS calls) - Presigned URLs have longer expiration (typically hours) vs direct credentials - Mobile app could share presigned URLs with unauthorized users - Doesn't solve DynamoDB access (presigned URLs don't work for DynamoDB) - Lambda becomes a bottleneck - **Anti-pattern:** Using API Gateway/Lambda proxy when direct AWS SDK access is possible  **C (IAM users per app user):** Severe security and operational anti-pattern: - Creating IAM users for each app user doesn't scale (millions of users = millions of IAM users) - Hits IAM service limits (5,000 IAM users per account by default) - Long-term credentials (access keys) stored in mobile app or DynamoDB (security risk) - No automatic credential revocation when user deletes account - Credential rotation is operationally complex - Violates AWS best practice of avoiding long-term credentials - **Critical flaw:** This is explicitly called out as an anti-pattern in AWS documentation  **D (All operations through Lambda proxy):** Defeats the purpose of Cognito Identity Pool: - Mobile app cannot directly access S3/DynamoDB (must go through Lambda) - Increased latency (every operation requires API call to Lambda) - Increased cost (Lambda invocations + API Gateway requests) - Lambda becomes single point of failure - Doesn't leverage AWS SDK capabilities (multipart upload, retry logic, etc.) - **Use Lambda proxy when:** Operations require business logic/validation - **Use direct SDK access when:** Simple CRUD operations with IAM authorization (this scenario)  **Key concepts:**  **Cognito Identity Pool features:**  | Feature | Description | |---------|-------------| | **Authenticated access** | Users authenticated via User Pool, social providers, or SAML | | **Unauthenticated access** | Guest access with restricted permissions (optional) | | **Enhanced auth flow** | Simplified API (recommended) | | **Basic auth flow** | Legacy API | | **Role mapping** | Map to different IAM roles based on provider, token claims, or rules |  **Policy variable mapping:**  | Variable | Description | Example | |----------|-------------|---------| | `cognito-identity.amazonaws.com:sub` | Cognito Identity ID (unique per user) | `us-east-1:12345678-1234-1234-1234-123456789012` | | `cognito-identity.amazonaws.com:aud` | Identity Pool ID | `us-east-1:abcdefgh-abcd-abcd-abcd-abcdefghijkl` | | `cognito-identity.amazonaws.com:amr` | Authentication method reference | `authenticated`, `unauthenticated` |  **Mobile app SDK usage (simplified):** ```javascript // 1. User Pool authentication const userPool = new CognitoUserPool({ UserPoolId, ClientId }); const authResponse = await user.authenticateUser(credentials); const idToken = authResponse.getIdToken().getJwtToken();  // 2. Identity Pool credential exchange AWS.config.credentials = new AWS.CognitoIdentityCredentials({   IdentityPoolId: 'us-east-1:12345678-1234-1234-1234-123456789012',   Logins: {     'cognito-idp.us-east-1.amazonaws.com/us-east-1_ABC123': idToken   } });  // 3. Direct AWS SDK usage const s3 = new AWS.S3(); await s3.putObject({   Bucket: 'my-photos-bucket',   Key: `users/${AWS.config.credentials.identityId}/photo.jpg`,   Body: photoData }).promise(); ```  **Best practices:** - Use Enhanced (Simplified) AuthFlow for Identity Pool (more secure) - Set appropriate credential expiration (balance security vs user experience) - Use separate IAM roles for authenticated vs unauthenticated access - Monitor `sts:AssumeRoleWithWebIdentity` calls in CloudTrail - Use fine-grained DynamoDB permissions (LeadingKeys condition)", docLink: "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html" },
            { id: 56, question: "Your mobile app uses Google as the sole identity provider (users sign in with \"Sign in with Google\"). After authentication, the app needs temporary AWS credentials to directly access S3 for file storage and invoke Lambda functions for backend processing. You do NOT want to use Amazon Cognito User Pools‚Äîonly Google authentication is required. What is the MOST appropriate architecture?", options: ["Configure Google as an OpenID Connect (OIDC) provider in IAM. Mobile app authenticates with Google, receives an OIDC ID token, then calls STS AssumeRoleWithWebIdentity directly with the Google token to receive temporary AWS credentials.","Create a Cognito Identity Pool configured with Google as an external identity provider (no Cognito User Pool). Mobile app authenticates with Google, exchanges the Google ID token with Cognito Identity Pool for temporary AWS credentials, then directly accesses S3 and Lambda.","Implement API Gateway with a Lambda authorizer that validates Google ID tokens by calling Google's tokeninfo endpoint. Lambda authorizer generates temporary credentials using STS AssumeRole and returns them in the authorization context for use by backend Lambda functions.","Use AWS IAM Identity Center (AWS SSO) with Google Workspace as the identity source. Mobile app redirects users to Google for authentication, receives SAML assertion, exchanges it with IAM Identity Center for temporary credentials, then accesses AWS services."], correct: 1, explanation: "**Why B is correct:**  **Cognito Identity Pool without User Pool** is the AWS-recommended pattern for third-party IdP integration with mobile apps:  **Architecture:** 1. Configure Cognito Identity Pool with Google as external provider 2. No Cognito User Pool required 3. Mobile app uses Google Sign-In SDK for authentication 4. Google returns OIDC ID token 5. Mobile app exchanges Google token with Identity Pool 6. Identity Pool returns AWS credentials  **Configuration:** ```json {   \"IdentityPoolName\": \"MyAppIdentityPool\",   \"AllowUnauthenticatedIdentities\": false,   \"SupportedLoginProviders\": {     \"accounts.google.com\": \"YOUR_GOOGLE_CLIENT_ID\"   } } ```  **Mobile app flow:** ```javascript // 1. Authenticate with Google const googleUser = await GoogleSignIn.signIn(); const googleIdToken = googleUser.authentication.idToken;  // 2. Exchange with Cognito Identity Pool AWS.config.credentials = new AWS.CognitoIdentityCredentials({   IdentityPoolId: 'us-east-1:12345678-1234-1234-1234-123456789012',   Logins: {     'accounts.google.com': googleIdToken   } });  await AWS.config.credentials.refreshPromise();  // 3. Use AWS SDK const s3 = new AWS.S3(); const lambda = new AWS.Lambda(); ```  **Why Identity Pool over direct STS:** - Simplified mobile SDK integration (AWS SDK handles credential refresh) - Built-in token caching and refresh - Support for unauthenticated access if needed in the future - Easier to add additional identity providers later - Enhanced authentication flow (simplified API) - Better handling of credential expiration  **Why other options are incorrect:**  **A (Direct STS AssumeRoleWithWebIdentity):** While technically functional, not the best practice: - More complex mobile app code (must handle STS API directly) - Must manually implement credential refresh logic - No built-in credential caching - Must configure Google as OIDC provider in IAM - Cognito Identity Pool is the AWS-recommended abstraction for this use case - **Key point:** Option A works, but B is \"MOST appropriate\" because it's the AWS best practice  **IAM trust policy for option A would be:** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Effect\": \"Allow\",     \"Principal\": {       \"Federated\": \"accounts.google.com\"     },     \"Action\": \"sts:AssumeRoleWithWebIdentity\",     \"Condition\": {       \"StringEquals\": {         \"accounts.google.com:aud\": \"YOUR_GOOGLE_CLIENT_ID\"       }     }   }] } ```  **C (Lambda authorizer with STS AssumeRole):** Wrong pattern: - Lambda authorizer is for API Gateway authorization, not credential vending - Mobile app needs credentials to directly access S3 and invoke Lambda - Lambda authorizer cannot return AWS credentials to the client (only returns IAM policy or context) - AssumeRole requires the Lambda function to have permission to assume another role (unnecessary complexity) - Lambda authorizer is better suited for validating tokens for API access, not for credential distribution  **D (IAM Identity Center with Google Workspace):** Wrong service: - IAM Identity Center is for **workforce identity**, not customer/end-user identity - Designed for employee SSO to AWS Console and CLI, not mobile app authentication - Requires Google Workspace (enterprise directory), not consumer Google accounts - SAML-based (browser redirect flow), not suitable for native mobile apps - Poor user experience for mobile apps  **Key concepts:**  **Cognito Identity Pool supported providers:** - Cognito User Pools - Amazon, Facebook, Google, Apple (Sign in with Apple) - Any OpenID Connect (OIDC) provider - SAML identity providers - Developer authenticated identities (custom)  **When to use Identity Pool without User Pool:** - Third-party IdP is sufficient (no need for Cognito-managed user directory) - Want to leverage existing social login (Google, Facebook, Apple) - Need AWS credentials for mobile/web app, not custom API authentication  **When to use both User Pool AND Identity Pool:** - Need Cognito-managed user directory (email/password sign-up) - Want Cognito's built-in UI, MFA, password policies - Need to support multiple authentication methods (Cognito + social) - AND need AWS credentials for direct service access  **IAM role for Identity Pool:** ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [{     \"Effect\": \"Allow\",     \"Principal\": {       \"Federated\": \"cognito-identity.amazonaws.com\"     },     \"Action\": \"sts:AssumeRoleWithWebIdentity\",     \"Condition\": {       \"StringEquals\": {         \"cognito-identity.amazonaws.com:aud\": \"us-east-1:12345678-1234-1234-1234-123456789012\"       },       \"ForAnyValue:StringLike\": {         \"cognito-identity.amazonaws.com:amr\": \"authenticated\"       }     }   }] } ```  **Best practices:** - Validate `aud` claim in IAM trust policy (prevents token reuse across apps) - Use separate IAM roles for authenticated vs unauthenticated - Implement least-privilege permissions - Monitor credential requests in CloudTrail (`sts:AssumeRoleWithWebIdentity`)", docLink: "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html" },
            { id: 57, question: "Your company uses AWS Organizations with multiple AWS accounts for different environments. You want to implement attribute-based access control (ABAC) that allows developers to manage only EC2 instances they personally created. Each developer has a unique IAM user with a username matching their corporate ID. When a developer launches an EC2 instance, it should be automatically tagged with `Owner: <username>`. Developers should be able to start, stop, and terminate only instances where the `Owner` tag matches their username. Which IAM policy conditions MUST be included to enforce this? (Select TWO)", options: ["Use `aws:RequestTag/Owner` condition with `StringEquals: \"${aws:username}\"` on the `ec2:RunInstances` action to require that launched instances are tagged with the creator's username.","Use `aws:ResourceTag/Owner` condition with `StringEquals: \"${aws:username}\"` on `ec2:StartInstances`, `ec2:StopInstances`, and `ec2:TerminateInstances` actions to restrict operations to instances matching the user's username.","Use `aws:TagKeys` condition with `ForAllValues:StringEquals: [\"Owner\"]` on all EC2 actions to ensure only the Owner tag can be used for access control decisions.","Use `ec2:ResourceTag/Owner` condition with `StringEquals: \"${aws:username}\"` on all EC2 instance operations to restrict access based on the instance tag."], correct: 0, explanation: "**Why A is correct:**  This condition enforces **tagging at resource creation time** using `aws:RequestTag`:  ```json {   \"Effect\": \"Allow\",   \"Action\": \"ec2:RunInstances\",   \"Resource\": \"arn:aws:ec2:*:*:instance/*\",   \"Condition\": {     \"StringEquals\": {       \"aws:RequestTag/Owner\": \"${aws:username}\"     }   } } ```  **How it works:** - `aws:RequestTag/Owner` checks the tag **being set during the API call** - `${aws:username}` is a policy variable that resolves to the IAM user's name - Developer can only launch instances if they tag them with `Owner: <their-username>` - Prevents developers from tagging instances with other users' names - **Critical for ABAC:** Without this, developers could tag instances with any Owner value and bypass access controls  **Example API call:** ```bash aws ec2 run-instances \\   --image-id ami-12345678 \\   --tag-specifications 'ResourceType=instance,Tags=[{Key=Owner,Value=john.doe}]' ```  If the IAM user is `john.doe`, this works. If user is `jane.smith`, the call is denied.  **Why B is correct:**  This condition enforces **operations on existing resources** using `aws:ResourceTag`:  ```json {   \"Effect\": \"Allow\",   \"Action\": [     \"ec2:StartInstances\",     \"ec2:StopInstances\",     \"ec2:TerminateInstances\"   ],   \"Resource\": \"arn:aws:ec2:*:*:instance/*\",   \"Condition\": {     \"StringEquals\": {       \"aws:ResourceTag/Owner\": \"${aws:username}\"     }   } } ```  **How it works:** - `aws:ResourceTag/Owner` checks the tag **already attached to the resource** - When developer tries to stop instance `i-1234567890abcdef0`, IAM checks if the instance has tag `Owner: john.doe` - If tag matches username, operation is allowed - If tag doesn't match or doesn't exist, operation is denied - **Purpose:** Restricts operations to resources the user owns  **Complete IAM policy example:**  ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"RequireOwnerTagOnCreate\",       \"Effect\": \"Allow\",       \"Action\": \"ec2:RunInstances\",       \"Resource\": \"arn:aws:ec2:*:*:instance/*\",       \"Condition\": {         \"StringEquals\": {           \"aws:RequestTag/Owner\": \"${aws:username}\"         }       }     },     {       \"Sid\": \"AllowOtherRunInstancesResources\",       \"Effect\": \"Allow\",       \"Action\": \"ec2:RunInstances\",       \"Resource\": [         \"arn:aws:ec2:*:*:subnet/*\",         \"arn:aws:ec2:*:*:network-interface/*\",         \"arn:aws:ec2:*:*:volume/*\",         \"arn:aws:ec2:*:*:key-pair/*\",         \"arn:aws:ec2:*:*:security-group/*\",         \"arn:aws:ec2:*::image/ami-*\"       ]     },     {       \"Sid\": \"AllowActionsOnOwnedInstances\",       \"Effect\": \"Allow\",       \"Action\": [         \"ec2:StartInstances\",         \"ec2:StopInstances\",         \"ec2:TerminateInstances\",         \"ec2:RebootInstances\",         \"ec2:ModifyInstanceAttribute\"       ],       \"Resource\": \"arn:aws:ec2:*:*:instance/*\",       \"Condition\": {         \"StringEquals\": {           \"aws:ResourceTag/Owner\": \"${aws:username}\"         }       }     },     {       \"Sid\": \"AllowDescribeActions\",       \"Effect\": \"Allow\",       \"Action\": \"ec2:DescribeInstances\",       \"Resource\": \"*\"     },     {       \"Sid\": \"EnforceTagImmutability\",       \"Effect\": \"Allow\",       \"Action\": \"ec2:CreateTags\",       \"Resource\": \"arn:aws:ec2:*:*:instance/*\",       \"Condition\": {         \"StringEquals\": {           \"aws:ResourceTag/Owner\": \"${aws:username}\",           \"aws:RequestTag/Owner\": \"${aws:username}\"         },         \"ForAllValues:StringEquals\": {           \"aws:TagKeys\": [\"Owner\", \"Environment\", \"Project\"]         }       }     }   ] } ```  **Why other options are incorrect:**  **C (`aws:TagKeys` with ForAllValues):** Wrong purpose: - `aws:TagKeys` checks which tag **keys** are present in a request - `ForAllValues:StringEquals` means ALL tags in the request must be from the specified list - This would restrict which tags can be added, but doesn't enforce the Owner value - Useful for tag governance (allowing only specific tag keys), not for ABAC authorization - Doesn't connect tag value to user identity  **D (`ec2:ResourceTag/Owner`):** Wrong condition key: - `ec2:ResourceTag` is service-specific and doesn't exist for most services - The correct key is `aws:ResourceTag` (universal across all services) - While some older AWS documentation might show service-specific keys, modern IAM uses `aws:ResourceTag` - This option is incorrect due to wrong syntax  **E (`aws:PrincipalTag/Owner`):** Wrong use of principal tags: - `aws:PrincipalTag` checks tags **attached to the IAM principal** (user or role), not the resource - Would require tagging IAM users with `Owner` tag - Doesn't check resource tags at all - Useful for scenarios like: \"Allow users with Department=Engineering tag to access resources with Department=Engineering tag\" - Not applicable to this scenario where we're checking resource ownership  **Key concepts:**  **ABAC condition keys:**  | Condition Key | Checks | Use Case | |---------------|--------|----------| | `aws:RequestTag/<key>` | Tag being set in the current request | Enforce tagging at creation | | `aws:ResourceTag/<key>` | Tag already on the resource | Check existing resource tags | | `aws:PrincipalTag/<key>` | Tag on the IAM principal (user/role) | Map user attributes to permissions | | `aws:TagKeys` | Which tag keys are present | Tag governance |  **Policy variables:**  | Variable | Resolves To | Example | |----------|-------------|---------| | `${aws:username}` | IAM user name | `john.doe` | | `${aws:userid}` | Unique IAM user ID | `AIDAI23HXN2EX4EXAMPLE` | | `${aws:PrincipalTag/Department}` | Principal's Department tag value | `Engineering` | | `${aws:CurrentTime}` | Current date/time | `2026-02-20T10:30:00Z` |  **Set operators for multi-value conditions:**  | Operator | Meaning | |----------|---------| | `ForAllValues:StringEquals` | ALL values in request must be in policy list | | `ForAnyValue:StringEquals` | AT LEAST ONE value in request must be in policy list |  **Common ABAC patterns:**  1. **User owns resources:** `aws:ResourceTag/Owner = ${aws:username}` 2. **Department access:** `aws:ResourceTag/Department = ${aws:PrincipalTag/Department}` 3. **Project-based:** `aws:ResourceTag/Project = ${aws:PrincipalTag/Project}` 4. **Environment separation:** `aws:ResourceTag/Environment = ${aws:PrincipalTag/AllowedEnvironment}`  **Benefits of ABAC:** - Scales better than role-based access control (RBAC) - Fewer IAM policies to manage - Automatic permissions for new resources based on tags - Easier to audit (tags are visible in AWS Console/APIs)  **Common pitfalls:** - Forgetting to allow `ec2:DescribeInstances` on `Resource: \"*\"` (doesn't support resource-level permissions) - Not considering RunInstances requires permissions for multiple resource types (subnet, volume, etc.) - Tag immutability: Users could change Owner tag if not restricted with CreateTags policy", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html" },
            { id: 58, question: "Your security team wants to enforce that all Secrets Manager secrets created by IAM users must have two specific tags: `environment` (with values either \"preprod\" or \"production\") and `cost-center`. Users should NOT be able to create secrets with any other tags. The policy should prevent secrets from being created without these exact tags or with additional tags beyond these two. Which IAM policy statement BEST enforces this requirement?", options: ["(Invalid option)","(Invalid option)","(Invalid option)","(Invalid option)"], correct: 0, explanation: "**Why B is correct:**  This policy enforces BOTH the specific tag values AND restricts which tags can be used:  **Breaking down the conditions:**  1. **`\"aws:RequestTag/environment\": [\"preprod\", \"production\"]`**    - Requires `environment` tag to be present    - Value must be either \"preprod\" OR \"production\"    - Any other value (e.g., \"dev\", \"test\") is denied  2. **`\"Null\": {\"aws:TagKeys\": \"false\"}`**    - Ensures that at least one tag key is present in the request    - `Null: false` means \"the value is NOT null\" (i.e., tags exist)    - Prevents creating secrets with no tags at all    - This condition is technically redundant given the other conditions, but explicit is better  3. **`\"ForAllValues:StringEquals\": {\"aws:TagKeys\": [\"environment\", \"cost-center\"]}`**    - **Critical enforcement:** ALL tag keys in the request must be from this list    - If request includes tags `[environment, cost-center, owner]`, it's DENIED (owner not in list)    - If request includes only `[environment, cost-center]`, it's ALLOWED    - `ForAllValues` = \"Every tag key you're trying to add must be in my allowed list\"  **How it works in practice:**  ‚úÖ **Allowed requests:** ```bash # Valid: Both required tags with valid values aws secretsmanager create-secret \\   --name my-secret \\   --secret-string \"value\" \\   --tags Key=environment,Value=production Key=cost-center,Value=12345  # Valid: preprod environment aws secretsmanager create-secret \\   --name my-secret2 \\   --tags Key=environment,Value=preprod Key=cost-center,Value=67890 ```  ‚ùå **Denied requests:** ```bash # DENIED: Invalid environment value aws secretsmanager create-secret \\   --tags Key=environment,Value=development Key=cost-center,Value=12345  # DENIED: Missing cost-center tag aws secretsmanager create-secret \\   --tags Key=environment,Value=production  # DENIED: Extra tag (owner) not in allowed list aws secretsmanager create-secret \\   --tags Key=environment,Value=production Key=cost-center,Value=12345 Key=owner,Value=john  # DENIED: No tags at all aws secretsmanager create-secret --name my-secret --secret-string \"value\" ```  **Why other options are incorrect:**  **A (Only checks specific tags, not tag keys):** - Checks `environment` value and `cost-center` presence - Does NOT restrict additional tags - User could add any extra tags (owner, project, team, etc.) - **Flaw:** Missing `ForAllValues:StringEquals` on `aws:TagKeys`  Example that would be incorrectly ALLOWED: ```bash aws secretsmanager create-secret \\   --tags Key=environment,Value=production Key=cost-center,Value=123 Key=random-tag,Value=anything ```  **C (Wrong operators and conditions):** - Uses `ForAnyValue:StringEquals` on environment (wrong‚Äîshould be `StringEquals`)   - `ForAnyValue` is for set operations, not single-value checks   - Would allow ANY value as long as at least one matches (logic error) - Uses `StringLike` instead of `ForAllValues:StringEquals` for TagKeys   - `StringLike` supports wildcards (* and ?) and doesn't restrict to ONLY those keys   - Doesn't enforce \"all tags must be from this list\" - Missing `Null` check for tag presence  **D (Uses ResourceTag instead of RequestTag for cost-center):** - `aws:ResourceTag/cost-center` checks tags ALREADY ON the resource - During CreateSecret, resource doesn't exist yet (no existing tags) - Would never match for creation - `aws:ResourceTag` is for checking existing resource tags, not tags being set - **Critical error:** Wrong condition key type for creation operation  **Key concepts:**  **Understanding `ForAllValues` vs `ForAnyValue`:**  | Operator | Logic | Use Case | |----------|-------|----------| | `ForAllValues` | ALL values in request must be in policy list | **Tag governance:** \"Only these tag keys allowed\" | | `ForAnyValue` | AT LEAST ONE value in request must be in policy list | **Minimum requirement:** \"Must have at least one of these\" |  **Example scenarios:**  ```json // Scenario 1: User creates resource with tags [environment, cost-center] \"ForAllValues:StringEquals\": {\"aws:TagKeys\": [\"environment\", \"cost-center\", \"owner\"]} // Result: ALLOW (all tags in request are in the allowed list)  // Scenario 2: User creates resource with tags [environment, cost-center, project] \"ForAllValues:StringEquals\": {\"aws:TagKeys\": [\"environment\", \"cost-center\"]} // Result: DENY (project is NOT in the allowed list)  // Scenario 3: User creates resource with tags [environment] \"ForAnyValue:StringEquals\": {\"aws:TagKeys\": [\"environment\", \"cost-center\"]} // Result: ALLOW (at least one tag from the list is present) ```  **Combining RequestTag and TagKeys:**  | Check | Condition Key | Purpose | |-------|---------------|---------| | **Tag value** | `aws:RequestTag/<key>` | Enforce specific values for specific tags | | **Tag keys** | `aws:TagKeys` | Restrict which tag keys can be used | | **Tag presence** | `Null: false` on `aws:RequestTag/<key>` | Ensure tag is present |  **Complete policy example for production use:**  ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"EnforceTaggingOnCreation\",       \"Effect\": \"Allow\",       \"Action\": [         \"secretsmanager:CreateSecret\"       ],       \"Resource\": \"*\",       \"Condition\": {         \"StringEquals\": {           \"aws:RequestTag/environment\": [\"preprod\", \"production\"]         },         \"Null\": {           \"aws:RequestTag/cost-center\": \"false\",           \"aws:TagKeys\": \"false\"         },         \"ForAllValues:StringEquals\": {           \"aws:TagKeys\": [\"environment\", \"cost-center\"]         }       }     },     {       \"Sid\": \"EnforceTaggingOnTagResource\",       \"Effect\": \"Allow\",       \"Action\": [\"secretsmanager:TagResource\"],       \"Resource\": \"*\",       \"Condition\": {         \"StringEquals\": {           \"aws:RequestTag/environment\": [\"preprod\", \"production\"]         },         \"Null\": {           \"aws:RequestTag/cost-center\": \"false\"         },         \"ForAllValues:StringEquals\": {           \"aws:TagKeys\": [\"environment\", \"cost-center\"]         }       }     },     {       \"Sid\": \"PreventTagRemoval\",       \"Effect\": \"Deny\",       \"Action\": [\"secretsmanager:UntagResource\"],       \"Resource\": \"*\",       \"Condition\": {         \"ForAnyValue:StringEquals\": {           \"aws:TagKeys\": [\"environment\", \"cost-center\"]         }       }     },     {       \"Sid\": \"AllowReadOperations\",       \"Effect\": \"Allow\",       \"Action\": [         \"secretsmanager:DescribeSecret\",         \"secretsmanager:GetSecretValue\",         \"secretsmanager:ListSecrets\"       ],       \"Resource\": \"*\"     }   ] } ```  **Best practices:** - Always combine `aws:RequestTag` (value check) with `aws:TagKeys` (key restriction) - Use `ForAllValues` for \"only these tags\" governance - Use `ForAnyValue` for \"at least one of these tags\" requirements - Prevent tag removal with explicit Deny on UntagResource for required tags - Document required tag values and enforcement policies  **Common mistakes:** - Using `ResourceTag` instead of `RequestTag` for creation - Using `ForAnyValue` instead of `ForAllValues` for tag key restriction - Forgetting to check tag presence with `Null: false` - Not preventing tag removal (users could remove required tags after creation)", docLink: "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html" }
        ];


        let currentMode = 'learn', currentQuestion = 0, userAnswers = [], answersRevealed = [], shuffledQuestions = [], timerInterval = null, timeRemaining = 0, inReviewMode = false, reviewedQuestions = [];
        function shuffleArray(a) { const b = [...a]; for (let i = b.length - 1; i > 0; i--) { const j = Math.floor(Math.random() * (i + 1)); [b[i], b[j]] = [b[j], b[i]]; } return b; }
        function shuffleQuestionOptions(q) { const idx = shuffleArray([0,1,2,3]); return { ...q, options: idx.map(i => q.options[i]), correct: idx.indexOf(q.correct) }; }
        function startQuiz(mode) { currentMode = mode; currentQuestion = 0; userAnswers = new Array(questions.length).fill(null); answersRevealed = new Array(questions.length).fill(false); shuffledQuestions = shuffleArray(questions).map(shuffleQuestionOptions); inReviewMode = false; reviewedQuestions = []; document.getElementById('modeSelector').classList.add('hidden'); document.getElementById('quizContent').classList.remove('hidden'); document.getElementById('resultsCard').classList.remove('show'); document.getElementById('totalQ').textContent = shuffledQuestions.length; document.getElementById('backToReviewBtn').style.display = 'none'; if (mode === 'exam') { timeRemaining = shuffledQuestions.length * 150; startTimer(); document.getElementById('shuffleBtn').classList.add('hidden'); } else { document.getElementById('timer').textContent = 'Learn Mode'; document.getElementById('shuffleBtn').classList.remove('hidden'); } renderQuestion(); }
        function startTimer() { updateTimerDisplay(); timerInterval = setInterval(() => { timeRemaining--; updateTimerDisplay(); if (timeRemaining <= 0) { clearInterval(timerInterval); showResults(); } }, 1000); }
        function updateTimerDisplay() { const m = Math.floor(timeRemaining / 60), s = timeRemaining % 60, el = document.getElementById('timer'); el.textContent = `${m}:${s.toString().padStart(2, '0')}`; el.classList.remove('warning', 'danger'); if (timeRemaining <= 60) el.classList.add('danger'); else if (timeRemaining <= 300) el.classList.add('warning'); }
        function renderQuestion() { const q = shuffledQuestions[currentQuestion], letters = ['A','B','C','D'], ans = userAnswers[currentQuestion], revealed = answersRevealed[currentQuestion]; let opts = q.options.map((o, i) => { let c = 'option'; if (currentMode === 'learn' && revealed) { c += ' disabled'; if (i === q.correct) c += ' correct'; else if (i === ans && ans !== q.correct) c += ' incorrect'; } else if (ans === i) c += ' selected'; return `<div class="${c}" onclick="selectOption(${i})"><span class="option-marker">${letters[i]}</span><span class="option-text">${o}</span></div>`; }).join(''); let exp = currentMode === 'learn' ? `<div class="explanation ${revealed ? 'show' : ''}"><div class="explanation-title">üìñ Explanation</div><div class="answer-header">Correct Answer: ${letters[q.correct]}</div><p class="explanation-text">${q.explanation}</p><a href="${q.docLink}" target="_blank" class="doc-link">üìÑ View AWS Documentation ‚Üí</a></div>` : ''; document.getElementById('questionsContainer').innerHTML = `<div class="question-card"><div class="question-header"><span class="question-number">Question ${currentQuestion + 1}</span><span class="question-domain">SCS-C03</span></div><p class="question-text">${q.question}</p><div class="options">${opts}</div>${currentMode === 'learn' && !revealed ? '<button class="btn btn-primary" style="margin-top:1rem;width:100%;" onclick="revealAnswer()">Show Answer</button>' : ''}${exp}</div>`; document.getElementById('currentQ').textContent = currentQuestion + 1; document.getElementById('progressFill').style.width = `${((currentQuestion + 1) / shuffledQuestions.length) * 100}%`; document.getElementById('prevBtn').disabled = currentQuestion === 0; document.getElementById('nextBtn').textContent = currentQuestion === shuffledQuestions.length - 1 ? (currentMode === 'exam' ? 'Submit Quiz' : 'See Results') : 'Next ‚Üí'; }
        function selectOption(i) { if (currentMode === 'learn' && answersRevealed[currentQuestion]) return; userAnswers[currentQuestion] = i; renderQuestion(); }
        function revealAnswer() { if (userAnswers[currentQuestion] === null) userAnswers[currentQuestion] = -1; answersRevealed[currentQuestion] = true; renderQuestion(); }
        function navigate(d) { if (d === 1 && currentQuestion === shuffledQuestions.length - 1) { showResults(); return; } currentQuestion = Math.max(0, Math.min(currentQuestion + d, shuffledQuestions.length - 1)); renderQuestion(); }
        function shuffleQuiz() { if (currentMode !== 'learn') return; currentQuestion = 0; userAnswers = new Array(questions.length).fill(null); answersRevealed = new Array(questions.length).fill(false); shuffledQuestions = shuffleArray(questions).map(shuffleQuestionOptions); renderQuestion(); }
        function showResults() { if (timerInterval) clearInterval(timerInterval); let correct = shuffledQuestions.reduce((c, q, i) => c + (userAnswers[i] === q.correct ? 1 : 0), 0); const total = shuffledQuestions.length, pct = Math.round((correct / total) * 100), pass = pct >= 80; document.getElementById('quizContent').classList.add('hidden'); document.getElementById('resultsCard').classList.add('show'); document.getElementById('reviewSection').classList.remove('show'); document.getElementById('scorePercent').textContent = pct + '%'; document.getElementById('scorePercent').className = 'results-score ' + (pass ? 'pass' : 'fail'); document.getElementById('resultStatus').textContent = pass ? 'üéâ PASSED!' : 'üìö Keep Studying'; document.getElementById('resultStatus').className = 'results-status ' + (pass ? 'pass' : 'fail'); document.getElementById('resultDetails').textContent = `You answered ${correct} out of ${total} questions correctly.`; document.getElementById('correctCount').textContent = correct; document.getElementById('incorrectCount').textContent = total - correct; }
        function showReview() { const letters = ['A','B','C','D']; const tbody = document.getElementById('reviewTableBody'); tbody.innerHTML = shuffledQuestions.map((q, i) => { const userAns = userAnswers[i]; const isCorrect = userAns === q.correct; const userLetter = userAns !== null && userAns >= 0 ? letters[userAns] : 'No Answer'; const correctLetter = letters[q.correct]; const statusIcon = isCorrect ? '‚úì' : '‚úó'; const statusClass = isCorrect ? 'correct' : 'incorrect'; const qText = q.question.length > 120 ? q.question.substring(0, 120) + '...' : q.question; const reviewedClass = reviewedQuestions.includes(i) ? 'reviewed' : ''; return `<tr class="${reviewedClass}" onclick="reviewQuestion(${i})"><td class="review-q-num">${i + 1}</td><td class="review-q-text">${qText}</td><td class="review-answer ${userAns === q.correct ? 'correct' : 'incorrect'}">${userLetter}</td><td class="review-answer correct">${correctLetter}</td><td class="review-status ${statusClass}">${statusIcon}</td></tr>`; }).join(''); document.getElementById('resultsCard').classList.remove('show'); document.getElementById('reviewSection').classList.add('show'); inReviewMode = true; }
        function reviewQuestion(qIndex) { if (!reviewedQuestions.includes(qIndex)) { reviewedQuestions.push(qIndex); } currentMode = 'learn'; currentQuestion = qIndex; answersRevealed[qIndex] = true; document.getElementById('reviewSection').classList.remove('show'); document.getElementById('quizContent').classList.remove('hidden'); document.getElementById('timer').textContent = 'Learn Mode'; document.getElementById('shuffleBtn').style.display = 'none'; document.getElementById('backToReviewBtn').style.display = 'inline-block'; if (timerInterval) clearInterval(timerInterval); renderQuestion(); }
        function backToReview() { document.getElementById('quizContent').classList.add('hidden'); showReview(); }
        function backToResults() { document.getElementById('reviewSection').classList.remove('show'); document.getElementById('resultsCard').classList.add('show'); inReviewMode = false; }
        function restartQuiz() { document.getElementById('resultsCard').classList.remove('show'); document.getElementById('reviewSection').classList.remove('show'); document.getElementById('modeSelector').classList.remove('hidden'); document.getElementById('quizContent').classList.add('hidden'); document.getElementById('backToReviewBtn').style.display = 'none'; inReviewMode = false; reviewedQuestions = []; if (timerInterval) clearInterval(timerInterval); }
    </script>
</body>
</html>
